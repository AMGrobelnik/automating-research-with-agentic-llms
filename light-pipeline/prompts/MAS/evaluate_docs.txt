================
CODE SNIPPETS
================
TITLE: Install Libraries
DESCRIPTION: Installs necessary libraries for running ðŸ¤— Transformers examples, including 'datasets', 'transformers', 'torch', 'evaluate', 'nltk', and 'rouge_score'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/transformers_integrations.mdx#_snippet_0

LANGUAGE: bash
CODE:
```
pip install datasets transformers torch evaluate nltk rouge_score
```

--------------------------------

TITLE: Install Doc-Builder Tool
DESCRIPTION: Installs the specialized tool required for building the documentation from a Git repository.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/doc-builder
```

--------------------------------

TITLE: Install Documentation Dependencies
DESCRIPTION: Installs the necessary packages to build the documentation, including the evaluate package with its documentation extras.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install -e ".[docs]"
```

--------------------------------

TITLE: Install Hugging Face Evaluate with Pip (Bash)
DESCRIPTION: This command installs the Hugging Face Evaluate library using pip, the standard Python package installer. It's the simplest method for adding the library to your project's environment.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/installation.mdx#_snippet_2

LANGUAGE: Bash
CODE:
```
pip install evaluate
```

--------------------------------

TITLE: Verify Hugging Face Evaluate Installation (Python)
DESCRIPTION: This Python command verifies the installation of the Hugging Face Evaluate library by loading the 'exact_match' metric and computing its value. It's used to confirm that the library is functional.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/installation.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
python -c "import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))"
```

--------------------------------

TITLE: Clone Hugging Face Evaluate Repository and Install from Source (Bash)
DESCRIPTION: This sequence of commands clones the Hugging Face Evaluate repository from GitHub and installs it in editable mode using pip. This method is suitable for developers who want to contribute to or modify the library's code.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/installation.mdx#_snippet_4

LANGUAGE: Bash
CODE:
```
git clone https://github.com/huggingface/evaluate.git
```

LANGUAGE: Bash
CODE:
```
cd evaluate
```

LANGUAGE: Bash
CODE:
```
pip install -e .
```

--------------------------------

TITLE: Create and Navigate Project Directory (Bash)
DESCRIPTION: This snippet demonstrates how to create a new project directory and navigate into it using bash commands. This is a foundational step for setting up a new project, often before creating a virtual environment.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/installation.mdx#_snippet_0

LANGUAGE: Bash
CODE:
```
mkdir ~/my-project
cd ~/my-project
```

--------------------------------

TITLE: Install Documentation Dependencies
DESCRIPTION: Installs the project's documentation build dependencies.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_9

LANGUAGE: bash
CODE:
```
pip install ".[docs]"
```

--------------------------------

TITLE: Creating Multi-line Code Blocks for Examples
DESCRIPTION: This example shows the standard Markdown syntax for creating multi-line code blocks, enclosed by triple backticks. These blocks are used for displaying code examples and are compatible with doctest.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_10

LANGUAGE: Markdown
CODE:
```
```
# first line of code
# second line
# etc
```
```

--------------------------------

TITLE: Install Documentation Builder
DESCRIPTION: Installs the necessary tools for building the project's documentation from a GitHub repository.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_8

LANGUAGE: bash
CODE:
```
pip install git+https://github.com/huggingface/doc-builder
```

--------------------------------

TITLE: Create and Manage Python Virtual Environment (Bash)
DESCRIPTION: This section details the process of creating, activating, and deactivating a Python virtual environment using the `venv` module. Virtual environments are crucial for isolating project dependencies.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/installation.mdx#_snippet_1

LANGUAGE: Bash
CODE:
```
python -m venv .env
```

LANGUAGE: Bash
CODE:
```
# Activate the virtual environment
source .env/bin/activate
```

LANGUAGE: Bash
CODE:
```
# Deactivate the virtual environment
source .env/bin/deactivate
```

--------------------------------

TITLE: Install Hugging Face Evaluate
DESCRIPTION: Installs the Hugging Face Evaluate library using pip. It is recommended to install this within a virtual environment.

SOURCE: https://github.com/huggingface/evaluate/blob/main/README.md#_snippet_0

LANGUAGE: Bash
CODE:
```
pip install evaluate
```

--------------------------------

TITLE: Spacy: Install and Setup
DESCRIPTION: Installs the `spacytextblob` library, downloads necessary TextBlob corpora, and downloads a Spacy English language model.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_4

LANGUAGE: Bash
CODE:
```
pip install spacytextblob
python -m textblob.download_corpora
python -m spacy download en_core_web_sm
```

--------------------------------

TITLE: Defining Method Arguments with Type and Description
DESCRIPTION: This example demonstrates the correct Markdown syntax for defining method arguments, including their type, shape (if applicable), and a detailed description. It follows the 'Args:' prefix and indentation rules.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_7

LANGUAGE: Markdown
CODE:
```
    Args:
        n_layers (`int`): The number of layers of the model.
```

--------------------------------

TITLE: Documenting a Tokenizer Class with Specific Methods
DESCRIPTION: This example shows how to document a tokenizer class, specifying which public methods should be included in the documentation. This allows for selective documentation of class members.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_5

LANGUAGE: Markdown
CODE:
```
## XXXTokenizer

[[autodoc]] XXXTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary
```

--------------------------------

TITLE: Install Development Dependencies
DESCRIPTION: Installs the project in editable mode with development dependencies, including testing and formatting tools.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_2

LANGUAGE: bash
CODE:
```
pip install -e ".[dev]"
```

--------------------------------

TITLE: Install evaluate library with template support
DESCRIPTION: Installs the Hugging Face evaluate library with the necessary dependencies for creating evaluation templates. This is a prerequisite for developing new evaluation modules.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_0

LANGUAGE: bash
CODE:
```
pip install evaluate[template]
```

--------------------------------

TITLE: Documenting Optional Arguments with Defaults
DESCRIPTION: This example illustrates the documentation format for optional arguments with default values. It specifies the type, indicates if the argument is optional, and includes the default value when it's not None.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_9

LANGUAGE: Markdown
CODE:
```
    Args:
        x (`str`, *optional*):
            This argument controls ...
        a (`float`, *optional*, defaults to 1):
            This argument is used to ...
```

--------------------------------

TITLE: Install Dependencies for Creating New Metrics
DESCRIPTION: Installs the necessary dependencies required to create and manage new evaluation modules within the Hugging Face Evaluate ecosystem.

SOURCE: https://github.com/huggingface/evaluate/blob/main/README.md#_snippet_3

LANGUAGE: Bash
CODE:
```
pip install evaluate[template]
```

--------------------------------

TITLE: Build Documentation
DESCRIPTION: Executes the doc-builder command to generate the documentation. It takes the source directory and a build directory as arguments.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
doc-builder build transformers docs/source/ --build_dir ~/tmp/test-build
```

--------------------------------

TITLE: Python Docstring: Tuple Return
DESCRIPTION: Example of a Python docstring showing the formatting for a tuple return type. It details the components of the tuple, including their types, shapes, and descriptions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_12

LANGUAGE: python
CODE:
```
    Returns:
        `tuple(torch.FloatTensor)` comprising various elements depending on the configuration ([`BertConfig`]) and inputs:
        - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.FloatTensor` of shape `(1,)` --
          Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.
        - **prediction_scores** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
          Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
```

--------------------------------

TITLE: Python Docstring: Single Value Return
DESCRIPTION: Example of a Python docstring demonstrating how to format a return block for a single value. It specifies the return type and provides a description.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_11

LANGUAGE: python
CODE:
```
    Returns:
        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.
```

--------------------------------

TITLE: Install Scikit-Learn
DESCRIPTION: Installs the scikit-learn library, which is required for integrating Evaluate metrics with Scikit-Learn estimators and pipelines.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/sklearn_integrations.mdx#_snippet_0

LANGUAGE: bash
CODE:
```
pip install -U scikit-learn
```

--------------------------------

TITLE: SARI Score - Partial Match Example
DESCRIPTION: Shows an example of a partial match between the prediction and reference sentences, demonstrating a typical SARI score calculation for non-ideal outputs. This example uses the same inputs as the initial usage example.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/sari/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
sari = load("sari")
sources=["About 95 species are currently accepted ."]
predictions=["About 95 you now get in ."]
references=[["About 95 species are currently known .","About 95 species are now accepted .","95 species are now accepted ."]]
sari_score = sari.compute(sources=sources, predictions=predictions, references=references)
print(sari_score)
```

--------------------------------

TITLE: Trainer: Accuracy Metric for Classification
DESCRIPTION: Demonstrates integrating the 'accuracy' metric with the ðŸ¤— Transformers 'Trainer' for a sequence classification task. It includes dataset preparation, tokenization, model setup, and defining a 'compute_metrics' function to calculate accuracy.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/transformers_integrations.mdx#_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate

# Prepare and tokenize dataset
dataset = load_dataset("yelp_review_full")
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(200))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(200))

# Setup evaluation 
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Load pretrained model and evaluate model after each epoch
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
```

--------------------------------

TITLE: GLUE Metric Examples for MRPC Subset
DESCRIPTION: Shows how to load the GLUE metric for the 'mrpc' subset and compute results, demonstrating the 'accuracy' and 'f1' output values.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/glue/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
glue_metric = load('glue', 'mrpc')  # 'mrpc' or 'qqp'
references = [0, 1]
predictions = [0, 1]
results = glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Load and Run EvaluationSuite
DESCRIPTION: This example demonstrates how to load a pre-defined EvaluationSuite from the Hugging Face Hub and run it against a specified model. The `run()` method takes the model identifier as input and returns the evaluation results.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_18

LANGUAGE: Python
CODE:
```
from evaluate import EvaluationSuite
suite = EvaluationSuite.load('mathemakitten/sentiment-evaluation-suite')
results = suite.run("huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli")
```

--------------------------------

TITLE: WikiSplit Metric Output Example
DESCRIPTION: Shows an example of the output dictionary from the WikiSplit metric computation, including the 'sari', 'sacrebleu', and 'exact' scores for a given set of inputs.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wiki_split/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> print(results)
{'sari': 21.805555555555557, 'sacrebleu': 14.535768424205482, 'exact': 0.0}
```

--------------------------------

TITLE: Documenting a Configuration Class
DESCRIPTION: This snippet demonstrates how to document a configuration class using the autodoc syntax. It includes all public methods of the specified configuration class in the documentation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_4

LANGUAGE: Markdown
CODE:
```
## XXXConfig

[[autodoc]] XXXConfig
```

--------------------------------

TITLE: Perplexity Example: Custom Predictions
DESCRIPTION: Shows an example of calculating perplexity using custom input texts and the 'gpt2' model, with an option to disable adding the start token. It prints the keys of the results dictionary and rounded values for mean perplexity and the first perplexity score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/perplexity/README.md#_snippet_1

LANGUAGE: python
CODE:
```
perplexity = evaluate.load("perplexity", module_type="metric")
input_texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]
results = perplexity.compute(model_id='gpt2',
                             add_start_token=False,
                             predictions=input_texts)
print(list(results.keys()))
>>>['perplexities', 'mean_perplexity']
print(round(results["mean_perplexity"], 2))
>>>646.75
print(round(results["perplexities"][0], 2))
>>>32.25
```

--------------------------------

TITLE: Simple Accuracy Example
DESCRIPTION: A basic example demonstrating the computation of accuracy with a given set of references and predictions, showing the resulting accuracy score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/accuracy/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> accuracy_metric = evaluate.load("accuracy")
>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
>>> print(results)
{'accuracy': 0.5}
```

--------------------------------

TITLE: FrugalScore Partial Value Example
DESCRIPTION: Illustrates an example of FrugalScore calculation with partial matches between predictions and references, resulting in a lower score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/frugalscore/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> frugalscore = evaluate.load("frugalscore")
>>> results = frugalscore.compute(predictions=['hello world'], references=['hugging face'])
>>> print(results)
{'scores': [0.42482382]}
```

--------------------------------

TITLE: GLUE Metric Examples for STSB Subset
DESCRIPTION: Illustrates loading the GLUE metric for the 'stsb' subset and computing results, showcasing the 'pearson' and 'spearmanr' output values.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/glue/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
glue_metric = load('glue', 'stsb')
references = [0., 1., 2., 3., 4., 5.]
predictions = [-10., -11., -12., -13., -14., -15.]
results = glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: FrugalScore Maximal Value Example
DESCRIPTION: Shows an example of FrugalScore calculation when predictions and references are an exact match, resulting in a high score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/frugalscore/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> frugalscore = evaluate.load("frugalscore")
>>> results = frugalscore.compute(predictions=['hello world'], references=['hello world'])
>>> print(results)
{'scores': [0.9891098]}
```

--------------------------------

TITLE: Matthews Correlation Coefficient Basic Example
DESCRIPTION: A fundamental example demonstrating the computation of the Matthews Correlation Coefficient using only predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/matthews_correlation/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> matthews_metric = evaluate.load("matthews_correlation")
>>> results = matthews_metric.compute(references=[1, 3, 2, 0, 3, 2],
...                                     predictions=[1, 2, 2, 0, 3, 3])
>>> print(results)
{'matthews_correlation': 0.5384615384615384}
```

--------------------------------

TITLE: Load and Run EvaluationSuite
DESCRIPTION: This example demonstrates how to load a pre-defined EvaluationSuite from the Hugging Face Hub using `EvaluationSuite.load()` and then run it against a model, such as 'gpt2'. The `run()` method returns evaluation results, which can be conveniently displayed in a pandas DataFrame.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/evaluation_suite.mdx#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import EvaluationSuite
suite = EvaluationSuite.load('mathemakitten/glue-evaluation-suite')
results = suite.run("gpt2")
```

--------------------------------

TITLE: FrugalScore Output Example
DESCRIPTION: Demonstrates the expected output format of the FrugalScore computation, which is a dictionary containing a list of scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/frugalscore/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> print(results)
{'scores': [0.6307541, 0.6449357]}
```

--------------------------------

TITLE: WikiSplit Metric - Partial Match Example
DESCRIPTION: Presents an example of a partial match between the prediction and reference, showing how the metric calculates intermediate scores for SARI, SacreBLEU, and Exact Match.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wiki_split/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> wiki_split = evaluate.load("wiki_split")
>>> sources = ["About 95 species are currently accepted ."]
>>> predictions = ["About 95 you now get in ."]
>>> references= [["About 95 species are currently known ."]]
>>> results = wiki_split.compute(sources=sources, predictions=predictions, references=references)
>>> print(results)
{'sari': 21.805555555555557, 'sacrebleu': 14.535768424205482, 'exact': 0.0}
```

--------------------------------

TITLE: Word Length with Single String Example
DESCRIPTION: Provides an example of computing the average word length for a single input string. It illustrates the expected output format.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/word_length/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> data = ["hello sun and goodbye moon"]
>>> wordlength = evaluate.load("word_length", module_type="measurement")
>>> results = wordlength.compute(data=data)
>>> print(results)
{'average_word_length': 5}
```

--------------------------------

TITLE: XNLI Metric Example: Partial Accuracy
DESCRIPTION: Provides an example of partial accuracy on the XNLI dataset, where some predictions are correct and others are not. This demonstrates a typical output scenario.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xnli/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from evaluate import load
xnli_metric = load("xnli")
predictions = [1, 0, 1]
references = [1, 0, 0]
results = xnli_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: TREC Eval Example with Sample Data
DESCRIPTION: Provides a minimal working example for the TREC Eval metric. It defines sample 'qrel' and 'run' dictionaries and shows how to compute and access a specific metric ('P@5').

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/trec_eval/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
qrel = {
    "query": [0],
    "q0": ["q0"],
    "docid": ["doc_1"],
    "rel": [2]
}
run = {
    "query": [0, 0],
    "q0": ["q0", "q0"],
    "docid": ["doc_2", "doc_1"],
    "rank": [0, 1],
    "score": [1.5, 1.2],
    "system": ["test", "test"]
}

trec_eval = evaluate.load("trec_eval")
results = trec_eval.compute(references=[qrel], predictions=[run])
results["P@5"]
```

--------------------------------

TITLE: Preserve Links for Renamed/Moved Sections
DESCRIPTION: Example of how to maintain backward compatibility for links when renaming section headers or moving sections to different files. It involves adding a map at the end of the document with original anchors.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_3

LANGUAGE: html
CODE:
```
[ <a href="#section-b">Section A</a><a id="section-a"></a> ]
```

LANGUAGE: html
CODE:
```
[ <a href="../new-file#section-b">Section A</a><a id="section-a"></a> ]
```

--------------------------------

TITLE: Example: Binary Label Distribution
DESCRIPTION: Demonstrates calculating the label distribution for a dataset with binary labels (0 and 1). It shows the expected output format, including labels and their fractions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/label_distribution/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> data = [1, 0, 1, 1, 0, 1, 0]
>>> distribution = evaluate.load("label_distribution")
>>> results = distribution.compute(data=data)
>>> print(results)
{'label_distribution': {'labels': [1, 0], 'fractions': [0.5714285714285714, 0.42857142857142855]}
```

--------------------------------

TITLE: Exact Match Example - Sentences
DESCRIPTION: Provides an example of using the Exact Match metric with longer text inputs, such as sentences, to demonstrate its handling of more complex strings.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/README.md#_snippet_5

LANGUAGE: python
CODE:
```
exact_match = evaluate.load("exact_match")
refs = ["The cat sat on the mat.", "Theaters are great.", "It's like comparing oranges and apples."] preds = ["The cat sat on the mat?", "Theaters are great.", "It's like comparing apples and oranges."] results = exact_match.compute(references=refs, predictions=preds)
print(round(results["exact_match"], 2))
```

--------------------------------

TITLE: Shell Command: Styling Docstrings
DESCRIPTION: Demonstrates the command used to automatically format Python docstrings and code according to the project's styling guidelines, ensuring adherence to line width and using the 'black' formatter.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_13

LANGUAGE: shell
CODE:
```
make style
```

--------------------------------

TITLE: Documenting Input IDs with Detailed Description and Links
DESCRIPTION: This snippet shows a detailed documentation for the `input_ids` argument, including its type, shape, a multi-line description, and internal links to related classes and glossary terms.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_8

LANGUAGE: Markdown
CODE:
```
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary.

            Indices can be obtained using [`AlbertTokenizer`]. See [`~PreTrainedTokenizer.encode`] and
            [`~PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
```

--------------------------------

TITLE: GLUE Metric Examples for COLA Subset
DESCRIPTION: Demonstrates loading the GLUE metric for the 'cola' subset and computing results, highlighting the 'matthews_correlation' output value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/glue/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
glue_metric = load('glue', 'cola')
references = [0, 1]
predictions = [1, 1]
results = glue_metric.compute(predictions=predictions, references=references)
results
```

--------------------------------

TITLE: Word Length with Multiple Strings Example
DESCRIPTION: Shows how to compute the average word length for multiple input strings. This example highlights the calculation across a list of strings.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/word_length/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> data = ["hello sun and goodbye moon", "foo bar foo bar"]
>>> wordlength = evaluate.load("word_length", module_type="measurement")
>>> results = wordlength.compute(data=text)
{'average_word_length': 4.5}
```

--------------------------------

TITLE: Documenting a Tokenizer Class with All Methods and __call__
DESCRIPTION: This snippet illustrates how to document a tokenizer class, including all its public methods and specifically the `__call__` method, even if it's a magic method not documented by default.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/README.md#_snippet_6

LANGUAGE: Markdown
CODE:
```
## XXXTokenizer

[[autodoc]] XXXTokenizer
    - all
    - __call__
```

--------------------------------

TITLE: Seqeval: Partial Match Example
DESCRIPTION: Demonstrates the seqeval metric's behavior with partial matches between predictions and references. This example shows how the metric calculates precision, recall, and F1 scores when there is some overlap but not a complete match.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/seqeval/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> seqeval = evaluate.load('seqeval')
>>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> results = seqeval.compute(predictions=predictions, references=references)
>>> print(results)
{'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 0.5, 'overall_recall': 0.5, 'overall_f1': 0.5, 'overall_accuracy': 0.8}
```

--------------------------------

TITLE: XNLI Metric Example: Minimal Accuracy
DESCRIPTION: Shows an example of minimal accuracy (0.0) on the XNLI dataset, where all predictions are incorrect. This helps in understanding the lower bound of the metric's output.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xnli/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
from evaluate import load
xnli_metric = load("xnli")
predictions = [1, 0]
references = [0, 1]
results = xnli_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Example WER Output
DESCRIPTION: This snippet shows an example of the output format for the computed word error rate, which is a float value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_1

LANGUAGE: python
CODE:
```
print(wer_score)
0.5
```

--------------------------------

TITLE: Download and extract model checkpoint for BLEURT
DESCRIPTION: Demonstrates how to download and extract external resources, such as model checkpoints, required by an evaluation module. This example specifically shows the process for the BLEURT metric using a download manager.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_3

LANGUAGE: python
CODE:
```
def _download_and_prepare(self, dl_manager):
    model_path = dl_manager.download_and_extract(CHECKPOINT_URLS[self.config_name])
    self.scorer = score.BleurtScorer(os.path.join(model_path, self.config_name))
```

--------------------------------

TITLE: Perplexity Example: Dataset Predictions
DESCRIPTION: Illustrates calculating perplexity on a larger set of predictions loaded from the 'wikitext' dataset. It filters out empty strings and then computes and prints the results, similar to the custom prediction example.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/perplexity/README.md#_snippet_2

LANGUAGE: python
CODE:
```
perplexity = evaluate.load("perplexity", module_type="metric")
input_texts = datasets.load_dataset("wikitext",
                                    "wikitext-2-raw-v1",
                                    split="test")["text"][:50]
input_texts = [s for s in input_texts if s!='']
results = perplexity.compute(model_id='gpt2',
                             predictions=input_texts)
print(list(results.keys()))
>>>['perplexities', 'mean_perplexity']
print(round(results["mean_perplexity"], 2))
>>>576.76
print(round(results["perplexities"][0], 2))
>>>889.28
```

--------------------------------

TITLE: XNLI Metric Example: Maximal Accuracy
DESCRIPTION: Illustrates a scenario where the model achieves maximal accuracy (1.0) on the XNLI dataset. This example shows the expected output when all predictions match the references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xnli/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
from evaluate import load
xnli_metric = load("xnli")
predictions = [0, 1]
references = [0, 1]
results = xnli_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Seq2SeqTrainer: ROUGE Metric for Summarization
DESCRIPTION: Illustrates using the ðŸ¤— Transformers 'Seq2SeqTrainer' for summarization tasks with the 'rouge' metric. It covers dataset loading, preprocessing, tokenization, and a 'compute_metrics' function that decodes predictions and labels for ROUGE evaluation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/transformers_integrations.mdx#_snippet_2

LANGUAGE: python
CODE:
```
import nltk
from datasets import load_dataset
import evaluate
import numpy as np
from transformers import AutoTokenizer, DataCollatorForSeq2Seq
from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Prepare and tokenize dataset
billsum = load_dataset("billsum", split="ca_test").shuffle(seed=42).select(range(200))
billsum = billsum.train_test_split(test_size=0.2)
tokenizer = AutoTokenizer.from_pretrained("t5-small")
prefix = "summarize: "

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["text"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

    labels = tokenizer(text_target=examples["summary"], max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_billsum = billsum.map(preprocess_function, batched=True)

# Setup evaluation
nltk.download("punkt_tab", quiet=True)
metric = evaluate.load("rouge")

def compute_metrics(eval_preds):
    preds, labels = eval_preds

    # decode preds and labels
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # rougeLSum expects newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    return result
```

--------------------------------

TITLE: Code Eval Example: Partial Match at k=1, Full Match at k=2
DESCRIPTION: Shows an example where one of two candidates passes the test at k=1, and both candidates pass at k=2. The output reflects these pass rates.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from evaluate import load
code_eval = load("code_eval")
test_cases = ["assert add(2,3)==5"]
candidates = [["def add(a, b): return a+b", "def add(a,b): return a*b"]]
pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1, 2])
print(pass_at_k)
{'pass@1': 0.5, 'pass@2': 1.0}
```

--------------------------------

TITLE: Exact Match Example - Basic
DESCRIPTION: Demonstrates the basic usage of the Exact Match metric without any special ignore options. It calculates the score for a given set of references and predictions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/README.md#_snippet_2

LANGUAGE: python
CODE:
```
exact_match = evaluate.load("exact_match")
refs = ["the cat", "theater", "YELLING", "agent007"] preds = ["cat?", "theater", "yelling", "agent"] results = exact_match.compute(references=refs, predictions=preds)
print(round(results["exact_match"], 2))
```

--------------------------------

TITLE: Seqeval: Minimal Values (No Match) Example
DESCRIPTION: Illustrates the computation of seqeval with minimal values, where predictions and references have no matches. This example shows how the metric handles cases with no overlap between predicted and reference sequences.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/seqeval/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> seqeval = evaluate.load('seqeval')
>>> predictions = [['O', 'B-MISC', 'I-MISC'], ['B-PER', 'I-PER', 'O']]
>>> references = [['B-MISC', 'O', 'O'], ['I-PER', '0', 'I-PER']]
>>> results = seqeval.compute(predictions=predictions, references=references)
>>> print(results)
{'MISC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2}, '_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.0}
```

--------------------------------

TITLE: Example: IMDb Dataset Label Distribution
DESCRIPTION: Calculates the label distribution for the test subset of the IMDb dataset. This example highlights loading data from the `datasets` library and computing the distribution, showing a perfectly balanced dataset.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/label_distribution/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> from datasets import load_dataset
>>> imdb = load_dataset('imdb', split = 'test')
>>> distribution = evaluate.load("label_distribution")
>>> results = distribution.compute(data=imdb['label'])
>>> print(results)
{'label_distribution': {'labels': [0, 1], 'fractions': [0.5, 0.5]}, 'label_skew': 0.0}
```

--------------------------------

TITLE: Seqeval: Maximal Values (Full Match) Example
DESCRIPTION: Demonstrates how to compute seqeval with maximal values, where predictions and references have a full match. This example loads the 'seqeval' metric and computes the results for given predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/seqeval/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> seqeval = evaluate.load('seqeval')
>>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> references = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> results = seqeval.compute(predictions=predictions, references=references)
>>> print(results)
{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 1.0, 'overall_recall': 1.0, 'overall_f1': 1.0, 'overall_accuracy': 1.0}
```

--------------------------------

TITLE: CharacTER Metric Output Example
DESCRIPTION: Provides an example of the output structure returned by the CharacTER metric's compute function, including counts, mean, median, standard deviation, min, max scores, and individual cer_scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/character/README.md#_snippet_1

LANGUAGE: json
CODE:
```
{
    'count': 2,
    'mean': 0.3127282211789254,
    'median': 0.3127282211789254,
    'std': 0.07561653111280243,
    'min': 0.25925925925925924,
    'max': 0.36619718309859156,
    'cer_scores': [0.36619718309859156, 0.25925925925925924]
}
```

--------------------------------

TITLE: Train and Evaluate Seq2Seq Model with Hugging Face Trainer
DESCRIPTION: This snippet shows how to load a pretrained Seq2Seq model (T5-small), set up data collator, define training arguments for evaluation per epoch, and initialize the Seq2SeqTrainer. It then initiates the training process. The `evaluate` method can be used instead of `train` for evaluating an already trained model.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/transformers_integrations.mdx#_snippet_3

LANGUAGE: python
CODE:
```
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Load pretrained model and evaluate model after each epoch
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=2,
    fp16=True,
    predict_with_generate=True
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_billsum["train"],
    eval_dataset=tokenized_billsum["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()

# To evaluate an existing model instead of training:
# trainer.evaluate()
```

--------------------------------

TITLE: Build Documentation Locally
DESCRIPTION: Builds the project's documentation locally using the doc-builder tool, outputting to a specified directory.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_10

LANGUAGE: bash
CODE:
```
doc-builder build evaluate docs/source/ --build_dir ~/tmp/test-build
```

--------------------------------

TITLE: Benchmark Token Classification Models
DESCRIPTION: This example showcases how to benchmark multiple token classification models using the `evaluate` library. It defines a list of models, loads a dataset, initializes the token classification evaluator, and then iterates through the models to compute metrics like 'seqeval'. The results are aggregated into a pandas DataFrame for easy comparison.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/base_evaluator.mdx#_snippet_2

LANGUAGE: python
CODE:
```
import pandas as pd
from datasets import load_dataset
from evaluate import evaluator
from transformers import pipeline

models = [
    "xlm-roberta-large-finetuned-conll03-english",
    "dbmdz/bert-large-cased-finetuned-conll03-english",
    "elastic/distilbert-base-uncased-finetuned-conll03-english",
    "dbmdz/electra-large-discriminator-finetuned-conll03-english",
    "gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner",
    "philschmid/distilroberta-base-ner-conll2003",
    "Jorgeutd/albert-base-v2-finetuned-ner",
]

data = load_dataset("conll2003", split="validation").shuffle().select(range(1000))
task_evaluator = evaluator("token-classification")

results = []
for model in models:
    results.append(
        task_evaluator.compute(
            model_or_pipeline=model, data=data, metric="seqeval"
            )
        )

df = pd.DataFrame(results, index=models)
df[["overall_f1", "overall_accuracy", "total_time_in_seconds", "samples_per_second", "latency_in_seconds"]]
```

--------------------------------

TITLE: Load and Compute F1 Score
DESCRIPTION: Demonstrates how to load the F1 metric and compute the score using predictions and references. This is a basic usage example.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/f1/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> f1_metric = evaluate.load("f1")
>>> results = f1_metric.compute(predictions=[0, 1], references=[0, 1])
>>> print(results)
["{'f1': 1.0}"]
```

--------------------------------

TITLE: Load and Compute CharacTER Metric
DESCRIPTION: Demonstrates how to load the CharacTER metric from the Hugging Face evaluate library and compute scores for single predictions/references and corpus examples.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/character/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import evaluate
character = evaluate.load("character")

# Single hyp/ref 
preds = ["this week the saudis denied information published in the new york times"]
refs = ["saudi arabia denied this week information published in the american new york times"]
results = character.compute(references=refs, predictions=preds)

# Corpus example
preds = ["this week the saudis denied information published in the new york times",
         "this is in fact an estimate"]
refs = ["saudi arabia denied this week information published in the american new york times",
        "this is actually an estimate"]
results = character.compute(references=refs, predictions=preds)
```

--------------------------------

TITLE: CER Example: Partial Match
DESCRIPTION: Shows a partial match scenario with differences between predictions and references, calculating the resulting CER score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cer/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
cer = load("cer")
predictions = ["this is the prediction", "there is an other sample"]
references = ["this is the reference", "there is another one"]
cer_score = cer.compute(predictions=predictions, references=references)
print(cer_score)
0.34146341463414637
```

--------------------------------

TITLE: Exact Match Example - Ignore Regex, Case, Punctuation, Numbers
DESCRIPTION: Illustrates computing the Exact Match score with options to ignore regex patterns, case, punctuation, and numbers. This provides a more lenient comparison.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/README.md#_snippet_4

LANGUAGE: python
CODE:
```
exact_match = evaluate.load("exact_match")
refs = ["the cat", "theater", "YELLING", "agent007"] preds = ["cat?", "theater", "yelling", "agent"] results = exact_match.compute(references=refs, predictions=preds, regexes_to_ignore=["the ", "yell", "YELL"], ignore_case=True, ignore_punctuation=True, ignore_numbers=True)
print(round(results["exact_match"], 2))
```

--------------------------------

TITLE: Batch Processing with add_batch() and compute()
DESCRIPTION: Provides an example of integrating batch processing with the Hugging Face Evaluate library, where predictions obtained from a model in batches are added to the metric using add_batch() before the final score is computed.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_10

LANGUAGE: Python
CODE:
```
>>> for model_inputs, gold_standards in evaluation_dataset:
>>>     predictions = model(model_inputs)
>>>     metric.add_batch(references=gold_standards, predictions=predictions)
>>> metric.compute()
```

--------------------------------

TITLE: SARI Score - Perfect Match Example
DESCRIPTION: Illustrates a scenario where the prediction perfectly matches the reference sentence, resulting in a SARI score of 100.0. This example highlights the metric's behavior with ideal output.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/sari/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
sari = load("sari")
sources=["About 95 species are currently accepted ."]
predictions=["About 95 species are currently accepted ."]
references=[["About 95 species are currently accepted ."]]
sari_score = sari.compute(sources=sources, predictions=predictions, references=references)
print(sari_score)
```

--------------------------------

TITLE: Load and Compute IndicGLUE Metric (Wiki-NER subset)
DESCRIPTION: This example shows how to load the IndicGLUE metric for the 'wiki-ner' subset, which outputs both accuracy and F1 scores. It includes sample predictions and references to calculate these metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/indic_glue/README.md#_snippet_1

LANGUAGE: python
CODE:
```
indic_glue_metric = evaluate.load('indic_glue', 'wiki-ner')
references = [0, 1]
predictions = [1,0]
results = indic_glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Evaluate Text Classification Models
DESCRIPTION: Demonstrates how to use the `TextClassificationEvaluator` to evaluate models on the IMDb dataset. It shows three methods for passing the model: by name/path on the Hub, as an instantiated `transformers` model, or as an instantiated `pipeline`. The example includes specifying label mapping and printing the evaluation results, which include accuracy and performance metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/base_evaluator.mdx#_snippet_0

LANGUAGE: python
CODE:
```
from datasets import load_dataset
from evaluate import evaluator
from transformers import AutoModelForSequenceClassification, pipeline

data = load_dataset("imdb", split="test").shuffle(seed=42).select(range(1000))
task_evaluator = evaluator("text-classification")

# 1. Pass a model name or path
eval_results = task_evaluator.compute(
    model_or_pipeline="lvwerra/distilbert-imdb",
    data=data,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)

# 2. Pass an instantiated model
model = AutoModelForSequenceClassification.from_pretrained("lvwerra/distilbert-imdb")

eval_results = task_evaluator.compute(
    model_or_pipeline=model,
    data=data,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)

# 3. Pass an instantiated pipeline
pipe = pipeline("text-classification", model="lvwerra/distilbert-imdb")

eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)
print(eval_results)
```

--------------------------------

TITLE: WikiSplit Metric - No Match Example
DESCRIPTION: Demonstrates a case where there is no match between the prediction and reference, resulting in low or zero scores for SARI, SacreBLEU, and Exact Match.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wiki_split/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> wiki_split = evaluate.load("wiki_split")
>>> sources = ["About 95 species are currently accepted ."]
>>> predictions = ["Hello world ."]
>>> references= [["About 95 species are currently known ."]]
>>> results = wiki_split.compute(sources=sources, predictions=predictions, references=references)
>>> print(results)
{'sari': 14.047619047619046, 'sacrebleu': 0.0, 'exact': 0.0}
```

--------------------------------

TITLE: CER Example: No Match
DESCRIPTION: Demonstrates a scenario with no overlap between predictions and references, resulting in a CER score of 1.0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cer/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
cer = load("cer")
predictions = ["hello"]
references = ["gracias"]
cer_score = cer.compute(predictions=predictions, references=references)
print(cer_score)
1.0
```

--------------------------------

TITLE: Load and Compute Precision Metric
DESCRIPTION: Demonstrates how to load the 'precision' metric from the Hugging Face evaluate library and compute the precision score using predicted and reference labels. This is a basic usage example for binary classification.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/precision/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> precision_metric = evaluate.load("precision")
>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'precision': 1.0}
```

--------------------------------

TITLE: Compute F1 Score for Simple Binary Classification
DESCRIPTION: A straightforward example of computing the F1 score for a binary classification problem, showing the basic input format for predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/f1/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> f1_metric = evaluate.load("f1")
>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])
>>> print(results)
{'f1': 0.5}
```

--------------------------------

TITLE: Compute Exact Match metric
DESCRIPTION: Provides an example implementation of the `_compute` method for an exact match metric. This method calculates the exact match score between references and predictions and returns it as a dictionary.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_5

LANGUAGE: python
CODE:
```
def _compute(self, references, predictions):
    em = sum([r==p for r, p in zip(references, predictions)])/len(references)
    return {"exact_match": em}
```

--------------------------------

TITLE: Exact Match Example - Ignore Regex, Case, Punctuation
DESCRIPTION: Shows how to compute the Exact Match score while ignoring specific regex patterns, case differences, and punctuation. Note that regex ignoring happens before case normalization.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/README.md#_snippet_3

LANGUAGE: python
CODE:
```
exact_match = evaluate.load("exact_match")
refs = ["the cat", "theater", "YELLING", "agent007"] preds = ["cat?", "theater", "yelling", "agent"] results = exact_match.compute(references=refs, predictions=preds, regexes_to_ignore=["the ", "yell"], ignore_case=True, ignore_punctuation=True)
print(round(results["exact_match"], 2))
```

--------------------------------

TITLE: Compute Text Duplicates - No Duplicates Example
DESCRIPTION: Demonstrates the usage of the text_duplicates measurement when there are no duplicate strings in the input list.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/text_duplicates/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> data = ["foo", "bar", "foobar"]
>>> duplicates = evaluate.load("text_duplicates")
>>> results = duplicates.compute(data=data)
>>> print(results)
{'duplicate_fraction': 0.0}
```

--------------------------------

TITLE: List Available Evaluation Modules
DESCRIPTION: Lists all available metrics, comparisons, and measurements within the Hugging Face Evaluate library.

SOURCE: https://github.com/huggingface/evaluate/blob/main/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
import evaluate

evaluate.list_evaluation_modules()
```

--------------------------------

TITLE: Load and Compute seqeval Metric
DESCRIPTION: Demonstrates how to load the seqeval metric from the Hugging Face evaluate library and compute scores using example predictions and references. This is a common workflow for evaluating sequence labeling tasks.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/seqeval/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> seqeval = evaluate.load('seqeval')
>>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
>>> results = seqeval.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Compute MAPE with Uniform Average
DESCRIPTION: Shows an example of computing the MAPE metric with the `uniform_average` configuration, which averages errors across multiple outputs with uniform weight. This is useful for multi-output regression problems.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mape/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> mape_metric = evaluate.load("mape")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> results = mape_metric.compute(predictions=predictions, references=references)
>>> print(results)
{'mape': 0.3273...}
```

--------------------------------

TITLE: Clone Repository and Set Up Remotes
DESCRIPTION: Clones the Hugging Face Evaluate repository and adds the upstream remote for syncing changes.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_0

LANGUAGE: bash
CODE:
```
git clone git@github.com:<your Github handle>/evaluate.git
cd evaluate 
git remote add upstream https://github.com/huggingface/evaluate.git
```

--------------------------------

TITLE: Evaluate Image Classification Models on Large Datasets
DESCRIPTION: Evaluate image classification models, including on large datasets like ImageNet-1k. This example demonstrates loading a dataset, setting up an image classification pipeline, and using the evaluator with a label mapping. It highlights memory-efficient data handling using memory mappings.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/base_evaluator.mdx#_snippet_5

LANGUAGE: Python
CODE:
```
from datasets import load_dataset
from evaluate import evaluator
from transformers import pipeline

data = load_dataset("imagenet-1k", split="validation", token=True)

pipe = pipeline(
    task="image-classification",
    model="facebook/deit-small-distilled-patch16-224"
)

task_evaluator = evaluator("image-classification")
eval_results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="accuracy",
    label_mapping=pipe.model.config.label2id
)
```

--------------------------------

TITLE: CER Example: Perfect Match
DESCRIPTION: Illustrates a perfect match scenario where predictions and references are identical, resulting in a CER score of 0.0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cer/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
cer = load("cer")
predictions = ["hello world", "good night moon"]
references = ["hello world", "good night moon"]
cer_score = cer.compute(predictions=predictions, references=references)
print(cer_score)
0.0
```

--------------------------------

TITLE: Compute SARI Score
DESCRIPTION: Demonstrates how to load the SARI metric from the Hugging Face evaluate library and compute the score using source, prediction, and reference sentences. This is the primary usage example for the SARI metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/sari/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
sari = load("sari")
sources=["About 95 species are currently accepted."]
predictions=["About 95 you now get in."]
references=[["About 95 species are currently known.","About 95 species are now accepted.","95 species are now accepted."]]
sari_score = sari.compute(sources=sources, predictions=predictions, references=references)
print(sari_score)
```

--------------------------------

TITLE: WikiSplit Metric - Perfect Match Example
DESCRIPTION: Illustrates a scenario where the prediction perfectly matches the reference, resulting in maximum scores (100.0) for SARI, SacreBLEU, and Exact Match.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wiki_split/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> wiki_split = evaluate.load("wiki_split")
>>> sources = ["About 95 species are currently accepted ."]
>>> predictions = ["About 95 species are currently accepted ."]
>>> references= [["About 95 species are currently accepted ."]]
>>> results = wiki_split.compute(sources=sources, predictions=predictions, references=references)
>>> print(results)
{'sari': 100.0, 'sacrebleu': 100.00000000000004, 'exact': 100.0}
```

--------------------------------

TITLE: Code Eval Example: No Match at k=1
DESCRIPTION: Illustrates a case where the generated code does not satisfy the test case at k=1, resulting in a pass@1 score of 0.0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
code_eval = load("code_eval")
test_cases = ["assert add(2,3)==5"]
candidates = [["def add(a,b): return a*b"]]
pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1])
print(pass_at_k)
{'pass@1': 0.0}
```

--------------------------------

TITLE: Exact Match Metric Example
DESCRIPTION: Demonstrates the Exact Match metric, which provides a simple binary score (1 if strings are identical, 0 otherwise) for evaluating text similarity. It's highly interpretable due to its clear range and definition.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/considerations.mdx#_snippet_0

LANGUAGE: Python
CODE:
```
from evaluate import load

metric = load("exact_match")
references = ["the cat sat on the mat", "the dog chased the ball"]
predictions = ["the cat sat on the mat", "the dog chased the ball"]
results = metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Launch Gradio widget for evaluation module
DESCRIPTION: Loads an evaluation module (e.g., 'element_count') and launches a Gradio widget for interactive use. This is typically done in the `app.py` file for Spaces.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_6

LANGUAGE: python
CODE:
```
import evaluate
from evaluate.utils import launch_gradio_widget


module = evaluate.load("lvwerra/element_count")
launch_gradio_widget(module)
```

--------------------------------

TITLE: Code Eval Example: Full Match at k=1
DESCRIPTION: Demonstrates a scenario where the generated code provides a full match for the test case at k=1. The output shows a pass@1 score of 1.0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
code_eval = load("code_eval")
test_cases = ["assert add(2,3)==5"]
candidates = [["def add(a, b): return a+b"]]
pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1])
print(pass_at_k)
{'pass@1': 1.0}
```

--------------------------------

TITLE: Compute Mean IoU with Hugging Face Evaluate
DESCRIPTION: This example demonstrates how to load and use the Mean IoU metric from the Hugging Face evaluate library. It shows the basic computation with sample predicted and ground truth segmentation maps.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mean_iou/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import numpy as np
import evaluate

mean_iou = evaluate.load("mean_iou")
predicted = np.array([[2, 2, 3], [8, 2, 4], [3, 255, 2]])
ground_truth = np.array([[1, 2, 2], [8, 2, 1], [3, 255, 1]])
results = mean_iou.compute(predictions=[predicted], references=[ground_truth], num_labels=10, ignore_index=255)
```

--------------------------------

TITLE: ROUGE with Custom Tokenizer
DESCRIPTION: Illustrates how to use a custom tokenizer with the ROUGE metric, which is particularly useful for non-Latin languages. The example uses a simple split-by-space tokenizer.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> results = rouge.compute(predictions=predictions,
...                         references=references,
                            tokenizer=lambda x: x.split())
>>> print(results)
{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}
```

--------------------------------

TITLE: Run Full Test Suite
DESCRIPTION: Executes all tests in the project's test suite to ensure overall code stability.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_5

LANGUAGE: bash
CODE:
```
python -m pytest ./tests/ 
```

--------------------------------

TITLE: Compute SQuAD Metric (Perfect Match)
DESCRIPTION: Computes the SQuAD metric with perfect match predictions and references. This example demonstrates achieving maximum Exact Match and F1 scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad")
predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
results = squad_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Initialize Evaluator and Compute Accuracy
DESCRIPTION: Demonstrates loading a transformers pipeline, a dataset, and a metric, then initializing the evaluator and computing the accuracy score. It handles mapping model outputs to dataset labels.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_14

LANGUAGE: Python
CODE:
```
from transformers import pipeline
from datasets import load_dataset
from evaluate import evaluator
import evaluate

pipe = pipeline("text-classification", model="lvwerra/distilbert-imdb", device=0)
data = load_dataset("imdb", split="test").shuffle().select(range(1000))
metric = evaluate.load("accuracy")

task_evaluator = evaluator("text-classification")

results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={"NEGATIVE": 0, "POSITIVE": 1})

print(results)
```

--------------------------------

TITLE: Compute SQuAD Metric (No Match)
DESCRIPTION: Computes the SQuAD metric with predictions that do not match the references. This example demonstrates achieving zero Exact Match and F1 scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad")
predictions = [{'prediction_text': '1999', 'id': '56e10a3be3433e1400422b22'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
results = squad_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Calculate Recall (Multiclass Classification)
DESCRIPTION: Provides examples for multiclass classification, demonstrating how to compute recall using different averaging strategies ('macro', 'micro', 'weighted') and without averaging (returning per-class recall).

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/recall/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> recall_metric = evaluate.load('recall')
>>> predictions = [0, 2, 1, 0, 0, 1]
>>> references = [0, 1, 2, 0, 1, 2]
>>> results = recall_metric.compute(predictions=predictions, references=references, average='macro')
>>> print(results)
{'recall': 0.3333333333333333}
```

LANGUAGE: python
CODE:
```
>>> results = recall_metric.compute(predictions=predictions, references=references, average='micro')
>>> print(results)
{'recall': 0.3333333333333333}
```

LANGUAGE: python
CODE:
```
>>> results = recall_metric.compute(predictions=predictions, references=references, average='weighted')
>>> print(results)
{'recall': 0.3333333333333333}
```

LANGUAGE: python
CODE:
```
>>> results = recall_metric.compute(predictions=predictions, references=references, average=None)
>>> print(results)
{'recall': array([1., 0., 0.])}
```

--------------------------------

TITLE: Compute Google BLEU with Multiple References
DESCRIPTION: This example shows how to compute the Google BLEU score when the first prediction has multiple references. It illustrates the flexibility of the metric in handling varying numbers of references for each sample.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/google_bleu/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> predictions = ['It is a guide to action which ensures that the rubber duck always disobeys the commands of the cat', 'he read the book because he was interested in world history']
>>> references  = [['It is the guiding principle which guarantees the rubber duck forces never being under the command of the cat', 'It is a guide to action that ensures that the rubber duck will never heed the cat commands', 'It is the practical guide for the rubber duck army never to heed the directions of the cat'], ['he was interested in world history because he read the book']]
>>> google_bleu = evaluate.load("google_bleu")
>>> results = google_bleu.compute(predictions=predictions, references=references)
>>> print(round(results["google_bleu"], 2))
0.61
```

--------------------------------

TITLE: Compute Regard Scores (Comparison Mode)
DESCRIPTION: This example demonstrates how to use the 'regard' metric in comparison mode. It takes two lists of text inputs (data and references) and computes the difference in regard scores between them.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_8

LANGUAGE: python
CODE:
```
>>> regard = evaluate.load("regard", "compare")
>>> group1 = ['xyz are described as mean', 'xyz are thought of as being too ambitious']
>>> group2 = ['xyz are known for making too much noise', 'xyz are described as often violent']
>>> results = regard.compute(data = group1, references = group2)
>>> print({k: round(v, 2) for k, v in results['regard_difference'].items()})
{'neutral': 0.46, 'positive': 0.01, 'negative': -0.46, 'other': -0.01}
```

--------------------------------

TITLE: Download NLTK resources
DESCRIPTION: Shows how to download specific NLTK resources, like 'punkt_tab', which might be required by an evaluation module. This is handled within the `_download_and_prepare` method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_4

LANGUAGE: python
CODE:
```
def _download_and_prepare(self, dl_manager):
    import nltk
    nltk.download("punkt_tab")
```

--------------------------------

TITLE: CER Example: Insertion Errors (CER > 1)
DESCRIPTION: Explains and demonstrates how insertion errors can lead to CER values greater than 1.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cer/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from evaluate import load
cer = load("cer")
predictions = ["hello world"]
references = ["hello"]
cer_score = cer.compute(predictions=predictions, references=references)
print(cer_score)
1.2
```

--------------------------------

TITLE: Create a new evaluation module using CLI
DESCRIPTION: Uses the evaluate CLI to create a new evaluation module. This command initializes a new Space on the Hugging Face Hub, clones it locally, and populates it with a template for a 'metric' type evaluation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_2

LANGUAGE: bash
CODE:
```
evaluate-cli create "My Metric" --module_type "metric"
```

--------------------------------

TITLE: List Evaluation Modules
DESCRIPTION: Lists available evaluation modules on the Hugging Face Hub, with options to filter by module type (e.g., 'comparison'), exclude community modules, and include detailed information like likes.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
>>> evaluate.list_evaluation_modules(
...   module_type="comparison",
...   include_community=False,
...   with_details=True)

[{'name': 'mcnemar', 'type': 'comparison', 'community': False, 'likes': 1},
 {'name': 'exact_match', 'type': 'comparison', 'community': False, 'likes': 0}]
```

--------------------------------

TITLE: Load and Compute IndicGLUE Metric (CVIT-MKBC-CLS subset)
DESCRIPTION: This snippet illustrates loading the IndicGLUE metric for the 'cvit-mkb-clsr' subset, which evaluates using 'precision@10'. It provides example references and predictions, where both are lists of floats.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/indic_glue/README.md#_snippet_2

LANGUAGE: python
CODE:
```
indic_glue_metric = evaluate.load('indic_glue', 'cvit-mkb-clsr')
references = [[0.5, 0.5, 0.5], [0.1, 0.2, 0.3]]
predictions = [[0.5, 0.5, 0.5], [0.1, 0.2, 0.3]]
results = indic_glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Compute Google BLEU with One Reference per Sample
DESCRIPTION: This example demonstrates how to compute the Google BLEU score using the Hugging Face evaluate library when each prediction has one reference. It loads the 'google_bleu' metric and computes the score for given predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/google_bleu/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> predictions = ['It is a guide to action which ensures that the rubber duck always disobeys the commands of the cat', 'he read the book because he was interested in world history']
>>> references = [['It is the guiding principle which guarantees the rubber duck forces never being under the command of the cat'], ['he was interested in world history because he read the book']]
>>> google_bleu = evaluate.load("google_bleu")
>>> results = google_bleu.compute(predictions=predictions, references=references)
>>> print(round(results["google_bleu"], 2))
0.44
```

--------------------------------

TITLE: Compute SQuAD v2 Metric - Minimal Match
DESCRIPTION: Calculates the exact match and F1 scores for SQuAD v2 when predictions do not match the references. This example shows a scenario where the predicted answer is incorrect.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad_v2/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad_v2")
predictions = [{'prediction_text': '1999', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
results = squad_v2_metric.compute(predictions=predictions, references=references)
results
```

--------------------------------

TITLE: Compute WER: No Match (Python)
DESCRIPTION: Calculates the Word Error Rate (WER) using the Hugging Face evaluate library when there is no match between predictions and references. This example illustrates a scenario with significant differences.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_4

LANGUAGE: python
CODE:
```
from evaluate import load
wer = load("wer")
predictions = ["hello world", "good night moon"]
references = ["hi everyone", "have a great day"]
wer_score = wer.compute(predictions=predictions, references=references)
print(wer_score)
1.0
```

--------------------------------

TITLE: Compute SQuAD Metric (Partial Match)
DESCRIPTION: Computes the SQuAD metric with partially matching predictions and references. This example shows a scenario with a partial match, resulting in scores less than 100%.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad")
predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}, {'prediction_text': 'Beyonce', 'id': '56d2051ce7d4791d0090260b'},  {'prediction_text': 'climate change', 'id': '5733b5344776f419006610e1'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}, {'answers': {'answer_start': [233], 'text': ['BeyoncÃ© and Bruno Mars']}, 'id': '56d2051ce7d4791d0090260b'}, {'answers': {'answer_start': [891], 'text': ['climate change']}, 'id': '5733b5344776f419006610e1'}]
results = squad_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Push evaluation module to Hugging Face Hub
DESCRIPTION: Demonstrates the Git commands required to add, commit, and push changes for a local evaluation module to the Hugging Face Hub. This makes the module publicly available.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_7

LANGUAGE: bash
CODE:
```
cd PATH_TO_MODULE
git add .
git commit -m "Add my new, shiny module."
git push
```

--------------------------------

TITLE: Login to Hugging Face Hub
DESCRIPTION: Authenticates your local environment with the Hugging Face Hub using your API token. This step is necessary for pushing and pulling resources from the Hub, including new evaluation modules.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/creating_and_sharing.mdx#_snippet_1

LANGUAGE: bash
CODE:
```
huggingface-cli login
```

--------------------------------

TITLE: Compute F1 Score with Binary Classification (pos_label=0)
DESCRIPTION: Shows how to compute the F1 score for a binary classification task when the positive label is 0. This example highlights the effect of the `pos_label` parameter.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/f1/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> f1_metric = evaluate.load("f1")
>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)
>>> print(round(results['f1'], 2))
0.67
```

--------------------------------

TITLE: Compute HONEST Score (No Groups)
DESCRIPTION: Computes the HONEST score using provided completions. This example demonstrates the basic usage without specifying any identity groups.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/honest/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> honest = evaluate.load('honest', 'en')
>>> completions = [['CEO', 'businessman', 'politician'], ['good', 'excellent', 'smart'], ['secretary', 'prostitute', 'maid'], ['beautiful', 'smart', 'tall']]
>>> result = honest.compute(predictions=completions)
>>> round(result['honest_score'],3)
0.167
```

--------------------------------

TITLE: Compute Competition MATH Accuracy
DESCRIPTION: Loads the competition_math metric and computes accuracy by comparing predictions to references. Canonicalizes inputs like '1/2' to '\frac{1}{2}'. Requires 'math_equivalence' installation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/competition_math/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
math = load("competition_math")
references = ["\\frac{1}{2}"]
predictions = ["1/2"]
results = math.compute(references=references, predictions=predictions)
```

LANGUAGE: python
CODE:
```
from evaluate import load
math = load("competition_math")
references = ["\\frac{1}{2}"]
predictions = ["1/2"]
results = math.compute(references=references, predictions=predictions)
print(results)
```

LANGUAGE: python
CODE:
```
from evaluate import load
math = load("competition_math")
references = ["\\frac{1}{2}"]
predictions = ["3/4"]
results = math.compute(references=references, predictions=predictions)
print(results)
```

LANGUAGE: python
CODE:
```
from evaluate import load
math = load("competition_math")
references = ["\\frac{1}{2}","\\frac{3}{4}"]
predictions = ["1/5", "3/4"]
results = math.compute(references=references, predictions=predictions)
print(results)
```

--------------------------------

TITLE: Load and Compute METEOR Score
DESCRIPTION: This snippet demonstrates how to load the METEOR metric using the Hugging Face evaluate library and compute the score for a given prediction and reference. It requires the 'evaluate' library to be installed.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/meteor/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> meteor = evaluate.load('meteor')
>>> predictions = ["It is a guide to action which ensures that the military always obeys the commands of the party"]
>>> references = ["It is a guide to action that ensures that the military will forever heed Party commands"]
>>> results = meteor.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Compute SQuAD v2 Metric - Maximal Match
DESCRIPTION: Calculates the exact match and F1 scores for SQuAD v2 when predictions perfectly match the references. This example demonstrates a scenario with a single correct answer.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad_v2/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
squad_v2_metric = load("squad_v2")
predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
results = squad_v2_metric.compute(predictions=predictions, references=references)
results
```

--------------------------------

TITLE: Compute Coval Metric
DESCRIPTION: Loads the Coval metric and computes it using provided predictions and references in CoNLL format. This example shows the basic usage for calculating coreference resolution metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/coval/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
from evaluate import load
coval = load('coval')
words = ['bc/cctv/00/cctv_0005   0   0       Thank   VBP  (TOP(S(VP*    thank  01   1    Xu_li  *           (V*)        *       -',
... 'bc/cctv/00/cctv_0005   0   1         you   PRP        (NP*)      -    -   -    Xu_li  *        (ARG1*)   (ARG0*)   (116)',
... 'bc/cctv/00/cctv_0005   0   2    everyone    NN        (NP*)      -    -   -    Xu_li  *    (ARGM-DIS*)        *    (116)',
... 'bc/cctv/00/cctv_0005   0   3         for    IN        (PP*       -    -   -    Xu_li  *        (ARG2*         *       -',
... 'bc/cctv/00/cctv_0005   0   4    watching   VBG   (S(VP*))))   watch  01   1    Xu_li  *             *)      (V*)      -',
... 'bc/cctv/00/cctv_0005   0   5           .     .          *))      -    -   -    Xu_li  *             *         *       -']
references = [words]
predictions = [words]
results = coval.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Load ROC AUC Metric for Multilabel
DESCRIPTION: Loads the ROC AUC metric for multilabel classification problems. This is used when each example can have multiple labels from a set of more than two possible classes.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> roc_auc_score = evaluate.load("roc_auc", "multilabel")
```

--------------------------------

TITLE: BLEU Metric Example
DESCRIPTION: Illustrates the BLEU (Bilingual Evaluation Understudy) metric, commonly used for machine translation. While it ranges from 0 to 1, its interpretation can be complex due to sensitivity to tokenization, normalization, and parameter choices.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/considerations.mdx#_snippet_1

LANGUAGE: Python
CODE:
```
from evaluate import load

metric = load("bleu")
references = ["the cat sat on the mat", "the dog chased the ball"]
predictions = ["the cat sat on the mat", "the dog chased the ball"]
results = metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Apply Code Formatting and Quality Checks
DESCRIPTION: Runs automated style corrections and code verifications using tools like black and isort, optimized for modified files.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_6

LANGUAGE: bash
CODE:
```
make quality
```

--------------------------------

TITLE: Compute SQuAD v2 Metric - Partial Match
DESCRIPTION: Calculates the exact match and F1 scores for SQuAD v2 with multiple predictions and references, demonstrating a partial match scenario where some answers are correct. This example includes three prediction-reference pairs.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad_v2/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad_v2")
predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22', 'no_answer_probability': 0.}, {'prediction_text': 'Beyonce', 'id': '56d2051ce7d4791d0090260b', 'no_answer_probability': 0.},  {'prediction_text': 'climate change', 'id': '5733b5344776f419006610e1', 'no_answer_probability': 0.}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}, {'answers': {'answer_start': [233], 'text': ['BeyoncÃ© and Bruno Mars']}, 'id': '56d2051ce7d4791d0090260b'}, {'answers': {'answer_start': [891], 'text': ['climate change']}, 'id': '5733b5344776f419006610e1'}]
results = squad_v2_metric.compute(predictions=predictions, references=references)
results
```

--------------------------------

TITLE: Coval Metric Maximal Values Example
DESCRIPTION: Demonstrates the Coval metric calculation resulting in maximal scores, indicating perfect performance. The output includes detailed recall, precision, and F1 scores for various metrics, as well as the overall CoNLL score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/coval/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
from evaluate import load
coval = load('coval')
words = ['bc/cctv/00/cctv_0005   0   0       Thank   VBP  (TOP(S(VP*    thank  01   1    Xu_li  *           (V*)        *       -',
... 'bc/cctv/00/cctv_0005   0   1         you   PRP        (NP*)      -    -   -    Xu_li  *        (ARG1*)   (ARG0*)   (116)',
... 'bc/cctv/00/cctv_0005   0   2    everyone    NN        (NP*)      -    -   -    Xu_li  *    (ARGM-DIS*)        *    (116)',
... 'bc/cctv/00/cctv_0005   0   3         for    IN        (PP*       -    -   -    Xu_li  *        (ARG2*         *       -',
... 'bc/cctv/00/cctv_0005   0   4    watching   VBG   (S(VP*))))   watch  01   1    Xu_li  *             *)      (V*)      -',
... 'bc/cctv/00/cctv_0005   0   5           .     .          *))      -    -   -    Xu_li  *             *         *       -']
references = [words]
predictions = [words]
results = coval.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: ROUGE Calculation with Aggregation (Python)
DESCRIPTION: This example shows how to compute ROUGE scores with aggregation enabled. It loads the ROUGE metric, provides prediction and reference texts, and computes the aggregated scores, displaying the keys and the aggregated rouge1 score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> rouge = evaluate.load('rouge')
>>> predictions = ["hello goodbye", "ankh morpork"]
>>> references = ["goodbye", "general kenobi"]
>>> results = rouge.compute(predictions=predictions,
...                         references=references,
...                         use_aggregator=True)
>>> print(list(results.keys()))
['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
>>> print(results["rouge1"])
0.25
```

--------------------------------

TITLE: Load ROC AUC Metric for Multiclass
DESCRIPTION: Loads the ROC AUC metric specifically for multiclass classification problems. This is used when each example has only one label, but there are more than two possible classes.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> roc_auc_score = evaluate.load("roc_auc", "multiclass")
```

--------------------------------

TITLE: Load and Compute GLUE Metric (sst2)
DESCRIPTION: Demonstrates loading the GLUE metric for the 'sst2' subset and computing the results using predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/glue/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
glue_metric = load('glue', 'sst2')
references = [0, 1]
predictions = [0, 1]
results = glue_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Load and Compute with an Evaluation Module
DESCRIPTION: Demonstrates how to load a specific evaluation module (e.g., 'accuracy') and then use it to compute results with provided arguments.

SOURCE: https://github.com/huggingface/evaluate/blob/main/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
accuracy = evaluate.load("accuracy")
results = accuracy.compute(references=[0, 1, 2], predictions=[0, 2, 1])
```

--------------------------------

TITLE: Compute WER: Perfect Match (Python)
DESCRIPTION: Calculates the Word Error Rate (WER) using the Hugging Face evaluate library when predictions and references have a perfect match. This example demonstrates loading the 'wer' metric and computing the score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
wer = load("wer")
predictions = ["hello world", "good night moon"]
references = ["hello world", "good night moon"]
wer_score = wer.compute(predictions=predictions, references=references)
print(wer_score)
0.0
```

--------------------------------

TITLE: Evaluate Machine Translation with COMET (No Match)
DESCRIPTION: This Python example shows the COMET metric's behavior when there is no match between the hypothesis and reference translations. It loads the metric and computes scores using distinct source, hypothesis, and reference sentences.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
comet_metric = load('comet') 
source = ["Dem Feuer konnte Einhalt geboten werden", "Schulen und KindergÃ¤rten wurden erÃ¶ffnet."]
hypothesis = ["The girl went for a walk", "The boy was sleeping"]
reference = ["They were able to control the fire", "Schools and kindergartens opened"]
results = comet_metric.compute(predictions=hypothesis, references=reference, sources=source)
print([round(v, 2) for v in results["scores"]])
```

--------------------------------

TITLE: Compute Regard Scores (Single Input)
DESCRIPTION: This example shows how to load the 'regard' metric and compute scores for a list of text inputs. It iterates through the results to print the rounded scores for each category (neutral, positive, negative, other).

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> regard = evaluate.load("regard")
>>> group1 = ['xyz are described as mean', 'xyz are thought of as being too ambitious']
>>> results = regard.compute(data = group1)
>>> for d in results['regard']:
...     print({l['label']: round(l['score'],2) for l in d})
{'neutral': 0.95, 'positive': 0.02, 'negative': 0.02, 'other': 0.01}
{'negative': 0.97, 'other': 0.02, 'neutral': 0.01, 'positive': 0.0}
```

--------------------------------

TITLE: Compute WER: Partial Match (Python)
DESCRIPTION: Calculates the Word Error Rate (WER) using the Hugging Face evaluate library for cases with partial matches between predictions and references. This example shows how the metric handles minor differences.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
wer = load("wer")
predictions = ["this is the prediction", "there is an other sample"]
references = ["this is the reference", "there is another one"]
wer_score = wer.compute(predictions=predictions, references=references)
print(wer_score)
0.5
```

--------------------------------

TITLE: Load and Compute RL Reliability Metrics (Python)
DESCRIPTION: Demonstrates how to load the 'rl_reliability' metric for both 'online' and 'offline' configurations and compute metrics using sample timesteps and rewards.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rl_reliability/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import evaluate
import numpy as np

rl_reliability = evaluate.load("rl_reliability", "online")
results = rl_reliability.compute(
    timesteps=[np.linspace(0, 2000000, 1000)],
    rewards=[np.linspace(0, 100, 1000)]
    )

rl_reliability = evaluate.load("rl_reliability", "offline")
results = rl_reliability.compute(
    timesteps=[np.linspace(0, 2000000, 1000)],
    rewards=[np.linspace(0, 100, 1000)]
    )
```

LANGUAGE: python
CODE:
```
import pandas as pd

dfs = [pd.read_csv(f"./csv_data/sac_humanoid_{i}_train.csv") for i in range(1, 4)]

rl_reliability = evaluate.load("rl_reliability", "online")
rl_reliability.compute(timesteps=[df["Metrics/EnvironmentSteps"] for df in dfs],
                       rewards=[df["Metrics/AverageReturn"] for df in dfs])
```

--------------------------------

TITLE: Create a New Evaluation Module
DESCRIPTION: Uses the `evaluate-cli` tool to create a new evaluation module with a specified name (e.g., 'Awesome Metric'). This command sets up the basic structure for a new metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/README.md#_snippet_4

LANGUAGE: Bash
CODE:
```
evaluate-cli create "Awesome Metric"
```

--------------------------------

TITLE: Compute chrF Score - Python
DESCRIPTION: Demonstrates how to compute the chrF score using the Hugging Face evaluate library. It requires a list of predictions and a list of lists of references. The example shows loading the metric and printing the results, which include the score and n-gram orders.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/chrf/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> prediction = ["The relationship between cats and dogs is not exactly friendly.", "a good bookshop is just a genteel black hole that knows how to read."]
>>> reference = [["The relationship between dogs and cats is not exactly friendly.", ], ["A good bookshop is just a genteel Black Hole that knows how to read." ]]
>>> chrf = evaluate.load("chrf")
>>> results = chrf.compute(predictions=prediction, references=reference)
>>> print(results)
{'score': 84.64214891738334, 'char_order': 6, 'word_order': 0, 'beta': 2}
```

--------------------------------

TITLE: Compute MSE with Multi-dimensional Lists and Raw Values
DESCRIPTION: Illustrates the computation of MSE for multi-dimensional lists using the Hugging Face Evaluate library. It shows how to use the 'multioutput' parameter set to 'raw_values' to get individual errors for each output.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mse/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> mse_metric = evaluate.load("mse", "multilist")
>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]
>>> references = [[0, 2], [-1, 2], [8, -5]]
>>> results = mse_metric.compute(predictions=predictions, references=references, multioutput='raw_values')
>>> print(results) 
{'mse': array([0.41666667, 1. ])}
```

--------------------------------

TITLE: ROUGE Calculation without Aggregation (Python)
DESCRIPTION: This example demonstrates how to compute ROUGE scores without aggregation. It loads the ROUGE metric, provides prediction and reference texts, and computes the scores, showing the keys and values for rouge1.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> rouge = evaluate.load('rouge')
>>> predictions = ["hello goodbye", "ankh morpork"]
>>> references = ["goodbye", "general kenobi"]
>>> results = rouge.compute(predictions=predictions,
...                         references=references,
...                         use_aggregator=False)
>>> print(list(results.keys()))
['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
>>> print(results["rouge1"])
[0.5, 0.0]
```

--------------------------------

TITLE: Compute Average Regard Scores (Comparison Mode)
DESCRIPTION: This example demonstrates how to compute the average regard scores for both the data and reference groups when using the 'regard' metric in comparison mode. The 'aggregation' parameter is set to 'average'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_10

LANGUAGE: python
CODE:
```
>>> regard = evaluate.load("regard", "compare")
>>> group1 = ['xyz are described as mean', 'xyz are thought of as being too ambitious']
>>> group2 = ['xyz are known for making too much noise', 'xyz are described as often violent']
>>> results = regard.compute(data = group1, references = group2, aggregation = "average")
>>> print({k: round(v, 2) for k, v in results['average_data_regard'].items()})
{'neutral': 0.48, 'positive': 0.01, 'negative': 0.5, 'other': 0.01}
>>> print({k: round(v, 2) for k, v in results['average_references_regard'].items()})
{'negative': 0.96, 'other': 0.02, 'neutral': 0.02, 'positive': 0.0}
```

--------------------------------

TITLE: Compute Maximum Regard Scores (Comparison Mode)
DESCRIPTION: This example shows how to compute the maximum regard scores for both the data and reference groups when using the 'regard' metric in comparison mode. The 'aggregation' parameter is set to 'maximum'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_9

LANGUAGE: python
CODE:
```
>>> regard = evaluate.load("regard", "compare")
>>> group1 = ['xyz are described as mean', 'xyz are thought of as being too ambitious']
>>> group2 = ['xyz are known for making too much noise', 'xyz are described as often violent']
>>> results = regard.compute(data = group1, references = group2, aggregation = "maximum")
>>> print({k: round(v, 2) for k, v in results['max_data_regard'].items()})
{'neutral': 0.95, 'positive': 0.02, 'negative': 0.97, 'other': 0.02}
>>> print({k: round(v, 2) for k, v in results['max_references_regard'].items()})
{'negative': 0.98, 'other': 0.04, 'neutral': 0.03, 'positive': 0.0}
```

--------------------------------

TITLE: Load Community Module
DESCRIPTION: Loads a community-contributed evaluation module, 'element_count', by providing its repository ID on the Hugging Face Hub. This allows users to leverage custom or shared evaluation tools.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_2

LANGUAGE: Python
CODE:
```
>>> element_count = evaluate.load("lvwerra/element_count", module_type="measurement")
```

--------------------------------

TITLE: Get Current Logging Verbosity
DESCRIPTION: Provides the Python code to retrieve the current logging verbosity level set within the Hugging Face Evaluate library. This is useful for checking the active logging configuration.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/logging_methods.mdx#_snippet_2

LANGUAGE: Python
CODE:
```
import evaluate
current_verbosity = evaluate.logging.get_verbosity()
```

--------------------------------

TITLE: Spacy: Compute Metrics with Evaluator
DESCRIPTION: Computes the accuracy metric using the Hugging Face evaluator with the Spacy pipeline on the IMDb test set. Note that this process may take longer than the Scikit-Learn example.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_8

LANGUAGE: Python
CODE:
```
eval.compute(pipe, ds["test"], "accuracy")
>>> {'accuracy': 0.6914}
```

--------------------------------

TITLE: Push Changes to Fork
DESCRIPTION: Pushes the local development branch to the remote fork on GitHub.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_13

LANGUAGE: bash
CODE:
```
git push -u origin a-descriptive-name-for-my-changes
```

--------------------------------

TITLE: Mean IoU Calculation with Multiple Segmentation Maps
DESCRIPTION: This example illustrates calculating Mean IoU using multiple predicted and ground truth segmentation maps. It includes optional parameters like `ignore_index` and `reduce_labels` for more nuanced evaluation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mean_iou/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import numpy as np
import evaluate

mean_iou = evaluate.load("mean_iou")
# suppose one has 3 different segmentation maps predicted
predicted_1 = np.array([[1, 2], [3, 4], [5, 255]])
actual_1 = np.array([[0, 3], [5, 4], [6, 255]])
predicted_2 = np.array([[2, 7], [9, 2], [3, 6]])
actual_2 = np.array([[1, 7], [9, 2], [3, 6]])
predicted_3 = np.array([[2, 2, 3], [8, 2, 4], [3, 255, 2]])
actual_3 = np.array([[1, 2, 2], [8, 2, 1], [3, 255, 1]])
predictions = [predicted_1, predicted_2, predicted_3]
references = [actual_1, actual_2, actual_3]
results = mean_iou.compute(predictions=predictions, references=references, num_labels=10, ignore_index=255, reduce_labels=False)
print(results) # doctest: +NORMALIZE_WHITESPACE
```

--------------------------------

TITLE: ROUGE Calculation for Specific Type (rouge_1) with Aggregation (Python)
DESCRIPTION: This example demonstrates calculating a specific ROUGE type, 'rouge_1', with aggregation enabled. It loads the ROUGE metric, provides prediction and reference texts, and computes the aggregated score for 'rouge_1', showing the key and the score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_7

LANGUAGE: python
CODE:
```
>>> rouge = evaluate.load('rouge')
>>> predictions = ["hello goodbye", "ankh morpork"]
>>> references = ["goodbye", "general kenobi"]
>>> results = rouge.compute(predictions=predictions,
...                         references=references,
...                         rouge_types=['rouge_1'],
...                         use_aggregator=True)
>>> print(list(results.keys()))
['rouge1']
>>> print(results["rouge1"])
0.25
```

--------------------------------

TITLE: Load Exact Match Metric
DESCRIPTION: Loads the Exact Match metric from the Hugging Face Evaluate library. This is the initial step before computing any scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
exact_match_metric = load("exact_match")
```

--------------------------------

TITLE: Compute Accuracy Score (All-in-One)
DESCRIPTION: Demonstrates the simplest way to compute an evaluation score by directly calling the compute() method with all necessary inputs (predictions and references) provided as arguments.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_7

LANGUAGE: Python
CODE:
```
>>> accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])
{'accuracy': 0.5}
```

--------------------------------

TITLE: Compute TREC Eval Metrics
DESCRIPTION: Loads the TREC Eval metric and computes various information retrieval metrics using provided predictions (run) and references (qrel). The example demonstrates basic usage with dictionaries representing a single query.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/trec_eval/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from evaluate import load
trec_eval = load("trec_eval")
results = trec_eval.compute(predictions=[run], references=[qrel])
```

--------------------------------

TITLE: Accessing Accuracy Metric Description
DESCRIPTION: Demonstrates how to load the 'accuracy' metric using the evaluate.load() function and access its 'description' attribute to understand its theoretical calculation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_4

LANGUAGE: Python
CODE:
```
>>> accuracy = evaluate.load("accuracy")
>>> accuracy.description
Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: True positive
TN: True negative
FP: False positive
FN: False negative
```

--------------------------------

TITLE: Compute Google BLEU with Adjusted min_len and max_len
DESCRIPTION: This example shows how to compute the Google BLEU score with both `min_len` set to 2 and `max_len` set to 6. Adjusting these parameters influences the range of n-grams considered in the score calculation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/google_bleu/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> predictions = ['It is a guide to action which ensures that the rubber duck always disobeys the commands of the cat', 'he read the book because he was interested in world history']
>>> references = [['It is the guiding principle which guarantees the rubber duck forces never being under the command of the cat', 'It is a guide to action that ensures that the rubber duck will never heed the cat commands', 'It is the practical guide for the rubber duck army never to heed the directions of the cat'], ['he was interested in world history because he read the book']]
>>> google_bleu = evaluate.load("google_bleu")
>>> results = google_bleu.compute(predictions=predictions,references=references, min_len=2, max_len=6)
>>> print(round(results["google_bleu"], 2))
0.4
```

--------------------------------

TITLE: Load and Compute MAPE Metric
DESCRIPTION: Demonstrates how to load the MAPE metric from the Hugging Face Evaluate library and compute it using sample predictions and references. This is the basic usage for a single output.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mape/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> mape_metric = evaluate.load("mape")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> results = mape_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Run Specific Tests
DESCRIPTION: Executes specific tests within the test suite to verify changes. This command targets a particular test file and a specific test case.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_3

LANGUAGE: bash
CODE:
```
pytest tests/<TEST_TO_RUN>.py
```

--------------------------------

TITLE: Compute Precision Metric
DESCRIPTION: Demonstrates how to load and compute the precision metric using the Hugging Face evaluate library. This is a generic metric applicable to various tasks.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx#_snippet_0

LANGUAGE: Python
CODE:
```
>>> precision_metric = evaluate.load("precision")
>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'precision': 1.0}
```

--------------------------------

TITLE: Create a New Development Branch
DESCRIPTION: Creates a new branch for development work, ensuring that the main branch is not modified directly.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_1

LANGUAGE: bash
CODE:
```
git checkout -b a-descriptive-name-for-my-changes
```

--------------------------------

TITLE: Compute Google BLEU with Adjusted min_len
DESCRIPTION: This example demonstrates computing the Google BLEU score with an adjusted `min_len` parameter set to 2. This means the metric will consider n-grams of length 2 and above, affecting the score compared to the default `min_len` of 1.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/google_bleu/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> predictions = ['It is a guide to action which ensures that the rubber duck always disobeys the commands of the cat', 'he read the book because he was interested in world history']
>>> references = [['It is the guiding principle which guarantees the rubber duck forces never being under the command of the cat', 'It is a guide to action that ensures that the rubber duck will never heed the cat commands', 'It is the practical guide for the rubber duck army never to heed the directions of the cat'], ['he was interested in world history because he read the book']]
>>> google_bleu = evaluate.load("google_bleu")
>>> results = google_bleu.compute(predictions=predictions, references=references, min_len=2)
>>> print(round(results["google_bleu"], 2))
0.53
```

--------------------------------

TITLE: Load Accuracy Metric
DESCRIPTION: Loads the 'accuracy' metric using the `evaluate.load` function. This is the primary entry point for accessing evaluation tools within the library.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_0

LANGUAGE: Python
CODE:
```
>>> import evaluate
>>> accuracy = evaluate.load("accuracy")
```

--------------------------------

TITLE: Evaluate Question Answering Models with Confidence Intervals
DESCRIPTION: Evaluate Question Answering models and compute confidence intervals using bootstrapping. This involves loading a dataset, initializing the question-answering evaluator, and calling the compute method with specified parameters like the model, data, metric, and bootstrapping strategy.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/base_evaluator.mdx#_snippet_4

LANGUAGE: Python
CODE:
```
from datasets import load_dataset
from evaluate import evaluator

task_evaluator = evaluator("question-answering")

data = load_dataset("squad", split="validation[:1000]")
eval_results = task_evaluator.compute(
    model_or_pipeline="distilbert-base-uncased-distilled-squad",
    data=data,
    metric="squad",
    strategy="bootstrap",
    n_resamples=30
)
```

--------------------------------

TITLE: Load and Compute Pearson Correlation Coefficient
DESCRIPTION: Demonstrates how to load the Pearson correlation coefficient metric and compute it with predictions and references. It also shows how to round the result.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/pearsonr/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> pearsonr_metric = evaluate.load("pearsonr")
>>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])
>>> print(round(results['pearsonr'], 2))
-0.74
```

--------------------------------

TITLE: Load and Compute Accuracy
DESCRIPTION: Demonstrates how to load the accuracy metric from the Hugging Face evaluate library and compute it using references and predictions. It also shows how to print the results.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/accuracy/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> accuracy_metric = evaluate.load("accuracy")
>>> results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'accuracy': 1.0}
```

--------------------------------

TITLE: Syncing Forked Repository with Upstream
DESCRIPTION: This Git command sequence allows a user to sync their forked repository's main branch with the upstream repository's main branch. It creates a new branch, pulls changes from upstream using squash, commits them with a custom message, and pushes the new branch to the origin.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_15

LANGUAGE: Shell
CODE:
```
git checkout -b your-branch-for-syncing
git pull --squash --no-commit upstream main
git commit -m '<your message without GitHub references>'
git push --set-upstream origin your-branch-for-syncing
```

--------------------------------

TITLE: Download and Extract Sample Data (Bash)
DESCRIPTION: Provides bash commands to download and extract sample data for testing the RL Reliability metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rl_reliability/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
wget https://storage.googleapis.com/rl-reliability-metrics/data/tf_agents_example_csv_dataset.tgz
tar -xvzf tf_agents_example_csv_dataset.tgz
```

--------------------------------

TITLE: Stage and Commit Changes
DESCRIPTION: Stages modified files and creates a commit to record local changes, emphasizing the importance of good commit messages.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_11

LANGUAGE: bash
CODE:
```
git add modified_file.py
git commit
```

--------------------------------

TITLE: Sync Local Branch with Upstream
DESCRIPTION: Fetches changes from the upstream repository and rebases the current branch onto the main branch to incorporate updates.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_12

LANGUAGE: bash
CODE:
```
git fetch upstream
git rebase upstream/main
```

--------------------------------

TITLE: Evaluate Models with LightEval Library
DESCRIPTION: LightEval is a library designed for comprehensive and customizable evaluation of Large Language Models (LLMs). It is actively maintained and recommended for recent evaluation approaches on the Hugging Face Hub. Visit the repository for more information.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/index.mdx#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load

# Example of loading a metric (e.g., accuracy)
accuracy = load("accuracy")

# Example of computing a metric
results = accuracy.compute(references=[0, 1, 2], predictions=[0, 2, 1])
print(results)
```

--------------------------------

TITLE: Load and Compute Poseval Metric
DESCRIPTION: Demonstrates how to load the 'poseval' metric from the evaluate library and compute scores using provided predictions and references. It also shows how to access the results, including overall accuracy and per-label recall.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/poseval/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> predictions = [['INTJ', 'ADP', 'PROPN', 'NOUN', 'PUNCT', 'INTJ', 'ADP', 'PROPN', 'VERB', 'SYM']]
>>> references = [['INTJ', 'ADP', 'PROPN', 'PROPN', 'PUNCT', 'INTJ', 'ADP', 'PROPN', 'PROPN', 'SYM']]
>>> poseval = evaluate.load("poseval")
>>> results = poseval.compute(predictions=predictions, references=references)
>>> print(list(results.keys()))
['ADP', 'INTJ', 'NOUN', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'accuracy', 'macro avg', 'weighted avg']
>>> print(results["accuracy"])
0.8
>>> print(results["PROPN"]["recall"])
0.5
```

--------------------------------

TITLE: Evaluate: Main Evaluator Entry Point
DESCRIPTION: This snippet represents the main entry point for using the evaluator classes in the Hugging Face evaluate library. It allows users to access and utilize various evaluation functionalities.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_0

LANGUAGE: python
CODE:
```
import evaluate

# Example usage (conceptual, actual usage depends on specific evaluator)
evaluator = evaluate.evaluator
```

--------------------------------

TITLE: Load SQuAD Metric
DESCRIPTION: Loads the SQuAD metric from the Hugging Face evaluate library. This is the initial step before computing scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad")
```

--------------------------------

TITLE: Load and Compute Confusion Matrix
DESCRIPTION: Demonstrates how to load the 'confusion_matrix' metric from the Hugging Face Evaluate library and compute it using sample predictions and references. The output is a dictionary containing the confusion matrix.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/confusion_matrix/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> confusion_metric = evaluate.load("confusion_matrix")
>>> results = confusion_metric.compute(references=[0, 1, 1, 2, 0, 2, 2], predictions=[0, 2, 1, 1, 0, 2, 0])
>>> print(results)
{'confusion_matrix': [[2, 0, 0], [0, 1, 1], [1, 1, 1]]}
```

--------------------------------

TITLE: Load Toxicity Measurement with Custom Model
DESCRIPTION: Demonstrates how to load the toxicity measurement, specifying a custom hate speech detection model. The loaded model should be compatible with Hugging Face's AutoModelForSequenceClassification.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/toxicity/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
toxicity = evaluate.load("toxicity", 'DaNLP/da-electra-hatespeech-detection', module_type="measurement")
```

--------------------------------

TITLE: Generate Radar Plot for Model Comparison
DESCRIPTION: Illustrates how to use the radar_plot function from the evaluate.visualization module to compare multiple models based on various metrics and performance characteristics like latency. It takes a list of result dictionaries and corresponding model names.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_16

LANGUAGE: Python
CODE:
```
import evaluate
from evaluate.visualization import radar_plot

data = [
   {"accuracy": 0.99, "precision": 0.8, "f1": 0.95, "latency_in_seconds": 33.6},
   {"accuracy": 0.98, "precision": 0.87, "f1": 0.91, "latency_in_seconds": 11.2},
   {"accuracy": 0.98, "precision": 0.78, "f1": 0.88, "latency_in_seconds": 87.6}, 
   {"accuracy": 0.88, "precision": 0.78, "f1": 0.81, "latency_in_seconds": 101.6}
   ]
model_names = ["Model 1", "Model 2", "Model 3", "Model 4"]
plot = radar_plot(data=data, model_names=model_names)
plot.show()
```

--------------------------------

TITLE: Load and Compute MAE Metric
DESCRIPTION: Demonstrates how to load the MAE metric from the Hugging Face Evaluate library and compute it using sample predictions and references. This is the basic usage for calculating MAE.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mae/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> mae_metric = evaluate.load("mae")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> results = mae_metric.compute(predictions=predictions, references=references)
>>> print(results)
{'mae': 0.5}
```

--------------------------------

TITLE: Apply Code Style Corrections
DESCRIPTION: Applies automatic style corrections to the code using configured formatting tools.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_7

LANGUAGE: bash
CODE:
```
make style
```

--------------------------------

TITLE: Run a Specific Test Case
DESCRIPTION: Runs a single, specific test case within a test file, identified by its class and method name.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_4

LANGUAGE: python
CODE:
```
python -m pytest ./tests/test_evaluator.py::TestQuestionAnsweringEvaluator::test_model_init
```

--------------------------------

TITLE: RL Reliability Metrics Citation (BibTeX)
DESCRIPTION: The BibTeX entry for citing the 'Measuring the Reliability of Reinforcement Learning Algorithms' paper.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rl_reliability/README.md#_snippet_2

LANGUAGE: bibtex
CODE:
```
@conference{rl_reliability_metrics,
  title = {Measuring the Reliability of Reinforcement Learning Algorithms},
  author = {Stephanie CY Chan, Sam Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama},
  booktitle = {International Conference on Learning Representations, Addis Ababa, Ethiopia},
  year = 2020,
}
```

--------------------------------

TITLE: Load and Compute Recall Metric
DESCRIPTION: Demonstrates how to load the 'recall' metric from the Hugging Face Evaluate library and compute it using sample predictions and references. It shows the basic usage for calculating the recall score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/recall/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> recall_metric = evaluate.load('recall')
>>> results = recall_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
["{'recall': 1.0}"]
```

--------------------------------

TITLE: Push Evaluation Results to Hugging Face Hub
DESCRIPTION: Uploads evaluation results to a model's repository on the Hugging Face Hub. This function requires parameters such as `model_id`, `metric_value`, `metric_type`, `dataset_type`, `task_type`, and their respective pretty names.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_13

LANGUAGE: Python
CODE:
```
evaluate.push_to_hub(
  model_id="huggingface/gpt2-wikitext2",  # model repository on hub
  metric_value=0.5,                       # metric value
  metric_type="bleu",                     # metric name, e.g. accuracy.name
  metric_name="BLEU",                     # pretty name which is displayed
  dataset_type="wikitext",                # dataset name on the hub
  dataset_name="WikiText",                # pretty name
  dataset_split="test",                   # dataset split used
  task_type="text-generation",            # task id, see https://github.com/huggingface/evaluate/blob/main/src/evaluate/config.py#L154-L192
  task_name="Text Generation"             # pretty name for task
)
```

--------------------------------

TITLE: Load and Compute ROUGE Metric
DESCRIPTION: Demonstrates how to load the ROUGE metric from the Hugging Face Evaluate library and compute scores using predictions and references. It shows the basic usage and the expected output format.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> rouge = evaluate.load('rouge')
>>> predictions = ["hello there", "general kenobi"]
>>> references = ["hello there", "general kenobi"]
>>> results = rouge.compute(predictions=predictions,
...                         references=references)
>>> print(results)
{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}
```

--------------------------------

TITLE: Configure Git for Windows Line Endings
DESCRIPTION: This command configures Git on Windows to automatically convert Windows-style line endings (CRLF) to Unix-style line endings (LF) when committing files. This ensures consistency across different operating systems.

SOURCE: https://github.com/huggingface/evaluate/blob/main/CONTRIBUTING.md#_snippet_14

LANGUAGE: Shell
CODE:
```
git config core.autocrlf input
```

--------------------------------

TITLE: Load and Compute sMAPE Metric
DESCRIPTION: Demonstrates how to load the sMAPE metric from the evaluate library and compute it using predictions and references. It covers basic usage with single and multi-dimensional lists, including different multioutput configurations.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/smape/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> smape_metric = evaluate.load("smape")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> results = smape_metric.compute(predictions=predictions, references=references)
>>> print(results)
{'smape': 0.5787...}
```

LANGUAGE: python
CODE:
```
>>> smape_metric = evaluate.load("smape", "multilist")
>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]
>>> references = [[0.1, 2], [-1, 2], [8, -5]]
>>> results = smape_metric.compute(predictions=predictions, references=references)
>>> print(results)
{'smape': 0.8874...}
```

LANGUAGE: python
CODE:
```
>>> results = smape_metric.compute(predictions=predictions, references=references, multioutput='raw_values')
>>> print(results)
{'smape': array([1.3749..., 0.4])}
```

--------------------------------

TITLE: Accessing Accuracy Metric Features
DESCRIPTION: Illustrates how to inspect the input format requirements of the 'accuracy' metric by accessing its 'features' attribute, which defines the expected data types for predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_6

LANGUAGE: Python
CODE:
```
>>> accuracy.features
{
    'predictions': Value(dtype='int32', id=None),
    'references': Value(dtype='int32', id=None)
}
```

--------------------------------

TITLE: Evaluate Library Core Classes
DESCRIPTION: Provides information on the base classes and their subclasses within the Hugging Face Evaluate library. This includes EvaluationModuleInfo and EvaluationModule, which serve as foundations for specific evaluation types like metrics, comparisons, and measurements.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/main_classes.mdx#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import EvaluationModuleInfo
from evaluate import MetricInfo
from evaluate import ComparisonInfo
from evaluate import MeasurementInfo

# Example usage (conceptual):
# info = EvaluationModuleInfo(...)
# metric_info = MetricInfo(...)
# comparison_info = ComparisonInfo(...)
# measurement_info = MeasurementInfo(...)
```

LANGUAGE: python
CODE:
```
from evaluate import EvaluationModule
from evaluate import Metric
from evaluate import Comparison
from evaluate import Measurement

# Example usage (conceptual):
# module = EvaluationModule(...)
# metric = Metric(...)
# comparison = Comparison(...)
# measurement = Measurement(...)
```

--------------------------------

TITLE: Calculate Recall (Binary Classification)
DESCRIPTION: Demonstrates the basic usage of the recall metric for binary classification. It shows how to load the metric and compute recall with default parameters.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/recall/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> recall_metric = evaluate.load('recall')
>>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1])
>>> print(results)
{'recall': 0.6666666666666666}
```

--------------------------------

TITLE: Load and Compute Word Length
DESCRIPTION: Demonstrates how to load the 'word_length' measurement and compute the average word length for a list of strings. It shows the basic usage with default tokenization.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/word_length/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> data = ["hello world"]
>>> wordlength = evaluate.load("word_length", module_type="measurement")
>>> results = wordlength.compute(data=data)
```

--------------------------------

TITLE: Load and Compute SuperGLUE Metric (Python)
DESCRIPTION: Demonstrates how to load a specific SuperGLUE metric subset (e.g., 'copa') using the 'evaluate' library and compute the metric by providing model predictions and reference labels. This is a common pattern for evaluating language understanding models.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/super_glue/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
super_glue_metric = load('super_glue', 'copa') 
predictions = [0, 1]
references = [0, 1]
results = super_glue_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Enable Progress Bar
DESCRIPTION: Shows how to re-enable the display of `tqdm` progress bars for Hugging Face Evaluate operations. This is the default behavior and can be explicitly turned back on if previously disabled.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/logging_methods.mdx#_snippet_5

LANGUAGE: Python
CODE:
```
import evaluate
evaluate.logging.enable_progress_bar()
```

--------------------------------

TITLE: Compute Accuracy with Bootstrapping
DESCRIPTION: Shows how to compute accuracy using bootstrapping to obtain confidence intervals and standard error, providing a measure of score stability. This requires specifying the 'bootstrap' strategy and the number of resamples.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_15

LANGUAGE: Python
CODE:
```
results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={"NEGATIVE": 0, "POSITIVE": 1},
                       strategy="bootstrap", n_resamples=200)

print(results)
```

--------------------------------

TITLE: Load and Compute Wilcoxon Test
DESCRIPTION: Demonstrates how to load the Wilcoxon evaluation metric and compute the comparison between two sets of predictions. It takes two lists of predictions as input and outputs the Wilcoxon statistic and p-value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/comparisons/wilcoxon/README.md#_snippet_0

LANGUAGE: python
CODE:
```
wilcoxon = evaluate.load("wilcoxon")
results = wilcoxon.compute(predictions1=[-7, 123.45, 43, 4.91, 5], predictions2=[1337.12, -9.74, 1, 2, 3.21])
print(results)
```

--------------------------------

TITLE: Load and Compute XNLI Metric
DESCRIPTION: Demonstrates how to load the XNLI metric from the 'evaluate' library and compute its accuracy using provided predictions and references. This is the primary method for evaluating models on the XNLI dataset.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xnli/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from evaluate import load
xnli_metric = load("xnli")
predictions = [0, 1]
references = [0, 1]
results = xnli_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Citation for Text Chunking
DESCRIPTION: BibTeX entry for the paper 'Text Chunking using Transformation-Based Learning' by Ramshaw and Marcus, relevant to sequence labeling tasks.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/seqeval/README.md#_snippet_5

LANGUAGE: BibTeX
CODE:
```
@inproceedings{ramshaw-marcus-1995-text,
    title = "Text Chunking using Transformation-Based Learning",
    author = "Ramshaw, Lance  and
      Marcus, Mitch",
    booktitle = "Third Workshop on Very Large Corpora",
    year = "1995",
    url = "https://www.aclweb.org/anthology/W95-0107",
}
```

--------------------------------

TITLE: Calculate R^2 using Hugging Face Evaluate
DESCRIPTION: Demonstrates how to load and use the R^2 metric from the Hugging Face evaluate library to compute the R^2 value given predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/r_squared/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
>>> r2_metric = evaluate.load("r_squared")
>>> r_squared = r2_metric.compute(predictions=[1, 2, 3, 4], references=[0.9, 2.1, 3.2, 3.8])
>>> print(r_squared)
0.98
```

LANGUAGE: python
CODE:
```
from evaluate import load
>>> r2_metric = evaluate.load("r_squared")
>>> r_squared = r2_metric.compute(predictions=[1, 2, 3, 4], references=[1, 2, 3, 4])
>>> print(r_squared)
1.0
```

--------------------------------

TITLE: Calculate Perplexity with Evaluate Library
DESCRIPTION: Demonstrates how to load the perplexity metric from the Hugging Face evaluate library and compute perplexity scores for a list of predictions using a specified model ID.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/perplexity/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
perplexity = load("perplexity", module_type="metric")
results = perplexity.compute(predictions=predictions, model_id='gpt2')
```

--------------------------------

TITLE: Compute COMET Scores
DESCRIPTION: Demonstrates how to load the COMET metric and compute scores for a given set of source sentences, hypothesis translations, and reference translations. It requires the 'evaluate' library and COMET models.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from evaluate import load
comet_metric = load('comet')
source = ["Dem Feuer konnte Einhalt geboten werden", "Schulen und KindergÃ¤rten wurden erÃ¶ffnet."]
hypothesis = ["The fire could be stopped", "Schools and kindergartens were open"]
reference = ["They were able to control the fire.", "Schools and kindergartens opened"]
comet_score = comet_metric.compute(predictions=hypothesis, references=reference, sources=source)
```

--------------------------------

TITLE: Accessing Accuracy Metric Citation
DESCRIPTION: Shows how to retrieve the BibTeX citation information for the 'accuracy' metric using its 'citation' attribute, useful for academic referencing.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_5

LANGUAGE: Python
CODE:
```
>>> accuracy.citation
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
```

--------------------------------

TITLE: Load and Compute Matthews Correlation Coefficient
DESCRIPTION: Demonstrates how to load the Matthews Correlation Coefficient metric and compute it using predictions and references. This is the minimum requirement for using the metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/matthews_correlation/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> matthews_metric = evaluate.load("matthews_correlation")
>>> results = matthews_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'matthews_correlation': 1.0}
```

--------------------------------

TITLE: Compute Accuracy Score (Incremental with add())
DESCRIPTION: Shows the incremental approach to computing a score, where individual predictions and references are added using the add() method within a loop, followed by a final compute() call.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_8

LANGUAGE: Python
CODE:
```
>>> for ref, pred in zip([0,1,0,1], [1,0,0,1]):
>>>     accuracy.add(references=ref, predictions=pred)
>>> accuracy.compute()
{'accuracy': 0.5}
```

--------------------------------

TITLE: Compute SQuAD Metric
DESCRIPTION: Shows how to load and compute the SQuAD metric for question answering tasks. This metric requires specific prediction and reference formats, including 'prediction_text' and 'answers'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx#_snippet_1

LANGUAGE: Python
CODE:
```
>>> from evaluate import load
>>> squad_metric = load("squad")
>>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
>>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
>>> results = squad_metric.compute(predictions=predictions, references=references)
>>> results
{'exact_match': 100.0, 'f1': 100.0}
```

--------------------------------

TITLE: Load and Compute WikiSplit Metric
DESCRIPTION: Demonstrates how to load the WikiSplit metric from the Hugging Face evaluate library and compute scores using source, prediction, and reference sentences. This is the primary method for using the metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wiki_split/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> wiki_split = evaluate.load("wiki_split")
>>> sources = ["About 95 species are currently accepted ."]
>>> predictions = ["About 95 you now get in ."]
>>> references= [["About 95 species are currently known ."]]
>>> results = wiki_split.compute(sources=sources, predictions=predictions, references=references)
```

--------------------------------

TITLE: Load and Use CharCut Metric
DESCRIPTION: Demonstrates how to load the CharCut metric from the Hugging Face evaluate library and compute scores by comparing predictions against references. The metric calculates a 'charcut_mt' score, where lower values indicate better performance.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/charcut_mt/README.md#_snippet_0

LANGUAGE: python
CODE:
```
import evaluate
charcut = evaluate.load("charcut")
preds = ["this week the saudis denied information published in the new york times",
                "this is in fact an estimate"]
refs = ["saudi arabia denied this week information published in the american new york times",
                "this is actually an estimate"]
results = charcut.compute(references=refs, predictions=preds)
print(results)
```

--------------------------------

TITLE: COMET: A Neural Framework for MT Evaluation (BibTeX)
DESCRIPTION: BibTeX entry for the foundational COMET paper, introducing it as a neural framework for Machine Translation evaluation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_6

LANGUAGE: bibtex
CODE:
```
@inproceedings{rei-etal-2020-comet,
   title = "{COMET}: A Neural Framework for {MT} Evaluation",
   author = "Rei, Ricardo  and
      Stewart, Craig  and
      Farinha, Ana C  and
      Lavie, Alon",
   booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
   month = nov,
   year = "2020",
   address = "Online",
   publisher = "Association for Computational Linguistics",
   url = "https://www.aclweb.org/anthology/2020.emnlp-main.213",
   pages = "2685--2702",
}

```

--------------------------------

TITLE: Load HONEST Measurement (English)
DESCRIPTION: Loads the HONEST measurement for English language prompts and completions. This is the initial step before computing the HONEST score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/honest/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> honest = evaluate.load('honest', 'en')
```

--------------------------------

TITLE: Combining Evaluation Modules
DESCRIPTION: Demonstrates how to combine multiple EvaluationModule instances into a single CombinedEvaluations object using the `combine` function. This is useful for managing and running multiple evaluations together.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/main_classes.mdx#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import combine, EvaluationModule

# Example usage (conceptual):
# module1 = EvaluationModule(...)
# module2 = EvaluationModule(...)
# combined_eval = combine(module1, module2)
# print(combined_eval)
```

LANGUAGE: python
CODE:
```
from evaluate import CombinedEvaluations

# Example usage (conceptual):
# Assuming 'combined_eval' is an instance of CombinedEvaluations
# print(combined_eval)
```

--------------------------------

TITLE: Load and Compute Brier Score
DESCRIPTION: Demonstrates how to load the Brier Score metric and compute it using sample predictions and references. It requires the 'evaluate' library and 'numpy'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/brier_score/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> brier_score = evaluate.load("brier_score")
>>> predictions = np.array([0, 0, 1, 1])
>>> references = np.array([0.1, 0.9, 0.8, 0.3])
>>> results = brier_score.compute(predictions=predictions, references=references)
>>> print(results)
{'brier_score': 0.3375}
```

--------------------------------

TITLE: Cite FrugalScore
DESCRIPTION: Provides the BibTeX citation for the FrugalScore paper, useful for academic referencing.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/frugalscore/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@article{eddine2021frugalscore,
  title={FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation},
  author={Eddine, Moussa Kamal and Shang, Guokan and Tixier, Antoine J-P and Vazirgiannis, Michalis},
  journal={arXiv preprint arXiv:2110.08559},
  year={2021}
}
```

--------------------------------

TITLE: Competition MATH Dataset Citation
DESCRIPTION: BibTeX entry for the paper introducing the MATH dataset and evaluation methodology.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/competition_math/README.md#_snippet_1

LANGUAGE: bibtex
CODE:
```
@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks
    and Collin Burns
    and Saurav Kadavath
    and Akul Arora
    and Steven Basart
    and Eric Tang
    and Dawn Song
    and Jacob Steinhardt},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
```

--------------------------------

TITLE: Compute Pearson Correlation Coefficient with P-value
DESCRIPTION: Shows how to compute the Pearson correlation coefficient along with its p-value by setting the `return_pvalue` argument to `True`. It also demonstrates how to print both results.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/pearsonr/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> pearsonr_metric = evaluate.load("pearsonr")
>>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5], return_pvalue=True)
>>> print(sorted(list(results.keys())))
['p-value', 'pearsonr']
>>> print(round(results['pearsonr'], 2))
-0.74
>>> print(round(results['p-value'], 2))
0.15
```

--------------------------------

TITLE: Scikit-Learn Data Loading and Preprocessing Pipeline
DESCRIPTION: Loads the Titanic dataset from OpenML and sets up preprocessing pipelines for numeric and categorical features using Scikit-Learn's ColumnTransformer and Pipeline. This prepares the data for a machine learning model.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/sklearn_integrations.mdx#_snippet_1

LANGUAGE: python
CODE:
```
import numpy as np
np.random.seed(0)
import evaluate
from sklearn.compose import ColumnTransformer
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X, y = fetch_openml("titanic", version=1, as_frame=True, return_X_y=True)

# Alternatively X and y can be obtained directly from the frame attribute:
# X = titanic.frame.drop('survived', axis=1)
# y = titanic.frame['survived']

numeric_features = ["age", "fare"]
numeric_transformer = Pipeline(
    steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())]
)

categorical_features = ["embarked", "sex", "pclass"]
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ]
)

clf = Pipeline(
    steps=[("preprocessor", preprocessor), ("classifier", LogisticRegression())]
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```

--------------------------------

TITLE: Compute BLEU Score with Hugging Face Evaluate
DESCRIPTION: Demonstrates how to load and compute the BLEU score using the Hugging Face evaluate library. It takes a list of predictions and a list of reference translations as input.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleu/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> predictions = ["hello there general kenobi", "foo bar foobar"]
>>> references = [
...     ["hello there general kenobi", "hello there !"],
...     ["foo bar foobar"]
... ]
>>> bleu = evaluate.load("bleu")
>>> results = bleu.compute(predictions=predictions, references=references)
>>> print(results)
{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}
```

--------------------------------

TITLE: Citation: Morris et al. (2004)
DESCRIPTION: BibTeX entry for the paper 'From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.' by Andrew Morris, Viktoria Maier, and Phil Green, published in 2004.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_6

LANGUAGE: bibtex
CODE:
```
@inproceedings{morris2004,
author = {Morris, Andrew and Maier, Viktoria and Green, Phil},
year = {2004},
month = {01},
pages = {},
title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}
}
```

--------------------------------

TITLE: Save Evaluation Results Locally
DESCRIPTION: Saves evaluation results to a local file or directory. The function can take a specific filename or a directory path. Any key-value pairs passed as arguments are stored in a JSON file, along with system information for reproducibility.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_12

LANGUAGE: Python
CODE:
```
>>> result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])

>>> hyperparams = {"model": "bert-base-uncased"}
>>> evaluate.save("./results/", experiment="run 42", **result, **hyperparams)
PosixPath('results/result-2022_05_30-22_09_11.json')
```

--------------------------------

TITLE: BibTeX: Citation for Scikit-learn
DESCRIPTION: This BibTeX entry provides citation details for the scikit-learn library, which is often used in conjunction with machine learning metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/precision/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@article{scikit-learn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
    and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. 
    and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
    Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}
```

--------------------------------

TITLE: Measuring Harmful Sentence Completion for LGBTQIA+ Individuals
DESCRIPTION: BibTeX entry for a paper that measures harmful sentence completion in language models specifically for LGBTQIA+ individuals. It details the authors, publication year, and the workshop it was presented at.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/honest/README.md#_snippet_5

LANGUAGE: BibTeX
CODE:
```
@inproceedings{nozza-etal-2022-measuring,
    title = {Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals},
    author = "Nozza, Debora and Bianchi, Federico and Lauscher, Anne and Hovy, Dirk",
    booktitle = "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion",
    publisher = "Association for Computational Linguistics",
    year={2022}
}
```

--------------------------------

TITLE: Load and Compute SQuAD v2 Metric
DESCRIPTION: This snippet demonstrates how to load the SQuAD v2 metric using the 'evaluate' library and compute scores by providing model predictions and references. The predictions should include 'id', 'prediction_text', and 'no_answer_probability', while references need 'id', 'answers' (a list of dictionaries with 'text'), and 'no_answer_threshold'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad_v2/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
squad_metric = load("squad_v2")
results = squad_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Load and Compute Spearman Correlation
DESCRIPTION: Demonstrates how to load the Spearman correlation metric and compute the correlation coefficient between references and predictions. This is the basic usage of the metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/spearmanr/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> spearmanr_metric = evaluate.load("spearmanr")
>>> results = spearmanr_metric.compute(references=[1, 2, 3, 4, 5], predictions=[10, 9, 2.5, 6, 4])
>>> print(results)
{'spearmanr': -0.7}
```

--------------------------------

TITLE: Load and Compute CUAD Metric
DESCRIPTION: Demonstrates how to load the CUAD metric from the Hugging Face evaluate library and compute results using sample predictions and references. The metric calculates various scores based on the provided data.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cuad/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
cuad_metric = load("cuad")
predictions = [{'prediction_text': ['The seller:', 'The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd.'], 'id': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement__Parties'}]
references = [{'answers': {'answer_start': [143, 49], 'text': ['The seller:', 'The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd.']}, 'id': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement__Parties'}]
results = cuad_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: SuperGLUE Benchmark Citation
DESCRIPTION: BibTeX citation for the SuperGLUE paper.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/super_glue/README.md#_snippet_4

LANGUAGE: bibtex
CODE:
```
@article{wang2019superglue,
  title={Super{GLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1905.00537},
  year={2019}
}
```

--------------------------------

TITLE: Load FrugalScore Metric
DESCRIPTION: Loads the FrugalScore metric with a specified model. The default model is 'moussaKam/frugalscore_tiny_bert-base_bert-score'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/frugalscore/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> frugalscore = evaluate.load("frugalscore", "moussaKam/frugalscore_medium_bert-base_mover-score")
```

--------------------------------

TITLE: Load and Compute Mahalanobis Distance
DESCRIPTION: Demonstrates how to load the Mahalanobis metric from the Hugging Face Evaluate library and compute the distance between a reference distribution and a data point. It requires the `evaluate` library and takes `reference_distribution` and `X` as inputs.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mahalanobis/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> mahalanobis_metric = evaluate.load("mahalanobis")
>>> results = mahalanobis_metric.compute(reference_distribution=[[0, 1], [1, 0]], X=[[0, 1]])
```

LANGUAGE: python
CODE:
```
>>> print(results)
{'mahalanobis': array([0.5])}
```

--------------------------------

TITLE: Compute Accuracy Score (Incremental with add_batch())
DESCRIPTION: Illustrates using the add_batch() method for the incremental computation of scores, allowing lists of predictions and references to be added at once before calling compute(). This is useful for processing data in batches.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_9

LANGUAGE: Python
CODE:
```
>>> for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):
>>>     accuracy.add_batch(references=refs, predictions=preds)
>>> accuracy.compute()
{'accuracy': 0.5}
```

--------------------------------

TITLE: Searching for COMETINHO Citation (BibTeX)
DESCRIPTION: BibTeX entry for the paper 'Searching for COMETINHO: The Little Metric That Could', referencing its authors and publication details.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_7

LANGUAGE: bibtex
CODE:
```
@inproceedings{rei-etal-2022-searching,
    title = "Searching for {COMETINHO}: The Little Metric That Could",
    author = "Rei, Ricardo  and
      Farinha, Ana C  and
      de Souza, Jos{'e} G.C.  and
      Ramos, Pedro G.  and
      Martins, Andr{'e} F.T.  and
      Coheur, Luisa  and
      Lavie, Alon",
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.9",
    pages = "61--70",
}

```

--------------------------------

TITLE: Compute CER Score
DESCRIPTION: Demonstrates how to load the 'cer' metric from the Hugging Face evaluate library and compute the CER score by providing lists of predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cer/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
cer = load("cer")
cer_score = cer.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Spacy: Load Pipeline and Add TextBlob
DESCRIPTION: Loads a Spacy English language model and adds the `spacytextblob` component to enable sentiment analysis.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_5

LANGUAGE: Python
CODE:
```
import spacy

nlp = spacy.load('en_core_web_sm')
nlp.add_pipe('spacytextblob')
```

--------------------------------

TITLE: MAUVE Score with Perfect Match
DESCRIPTION: Demonstrates MAUVE calculation when predictions and references are identical, resulting in a perfect score of 1.0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mauve/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
mauve = load('mauve')
predictions = ["hello world", "goodnight moon"]
references = ["hello world",  "goodnight moon"]
mauve_results = mauve.compute(predictions=predictions, references=references)
print(mauve_results.mauve)
1.0
```

--------------------------------

TITLE: Combine Multiple Metrics
DESCRIPTION: Bundles multiple metrics into a single object for sequential computation. Accepts a list of metric names or instantiated metric objects. The `compute` method then calculates each metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_11

LANGUAGE: Python
CODE:
```
>>> clf_metrics = evaluate.combine(["accuracy", "f1", "precision", "recall"])
>>> clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])

{
  'accuracy': 0.667,
  'f1': 0.667,
  'precision': 1.0,
  'recall': 0.5
}
```

--------------------------------

TITLE: Define Custom EvaluationSuite in Python
DESCRIPTION: This Python code defines a custom EvaluationSuite named 'Suite' that inherits from `evaluate.EvaluationSuite`. It includes two SubTasks for text classification using the GLUE dataset (SST2 and RTE). Each SubTask specifies the task type, dataset, split, and arguments for the evaluator, including metric, input columns, and label mapping. A data preprocessor is also defined to convert input text to lowercase.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/evaluation_suite.mdx#_snippet_0

LANGUAGE: python
CODE:
```
import evaluate
from evaluate.evaluation_suite import SubTask

class Suite(evaluate.EvaluationSuite):

    def __init__(self, name):
        super().__init__(name)
        self.preprocessor = lambda x: {"text": x["text"].lower()}
        self.suite = [
            SubTask(
                task_type="text-classification",
                data="glue",
                subset="sst2",
                split="validation[:10]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "sentence",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0.0,
                        "LABEL_1": 1.0
                    }
                }
            ),
            SubTask(
                task_type="text-classification",
                data="glue",
                subset="rte",
                split="validation[:10]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "sentence1",
                    "second_input_column": "sentence2",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0,
                        "LABEL_1": 1
                    }
                }
            )
        ]
```

--------------------------------

TITLE: Load and Compute IndicGLUE Metric (WNLI subset)
DESCRIPTION: This snippet demonstrates how to load the IndicGLUE metric for the 'wnli' subset and compute the results using provided predictions and references. It's used for tasks where accuracy is the primary evaluation metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/indic_glue/README.md#_snippet_0

LANGUAGE: python
CODE:
```
indic_glue_metric = evaluate.load('indic_glue', 'wnli')
references = [0, 1]
predictions = [0, 1]
results = indic_glue_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: HONEST: Measuring Hurtful Sentence Completion
DESCRIPTION: BibTeX entry for the HONEST paper, which focuses on measuring hurtful sentence completion in language models. This entry includes author details, publication venue, and relevant links.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/honest/README.md#_snippet_4

LANGUAGE: BibTeX
CODE:
```
@inproceedings{nozza-etal-2021-honest,
    title = {"{HONEST}: Measuring Hurtful Sentence Completion in Language Models"},
    author = "Nozza, Debora and Bianchi, Federico  and Hovy, Dirk",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.191",
    doi = "10.18653/v1/2021.naacl-main.191",
    pages = "2398--2406",
}
```

--------------------------------

TITLE: Load and Compute Word Count
DESCRIPTION: Demonstrates how to load the 'word_count' measurement and compute word counts for a list of strings. It shows the expected output format for total and unique words.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/word_count/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> data = ["hello world and hello moon"]
>>> wordcount= evaluate.load("word_count")
>>> results = wordcount.compute(data=data)
>>> print(results)
{'total_word_count': 5, 'unique_words': 4}
```

LANGUAGE: python
CODE:
```
>>> data = ["hello sun and goodbye moon"]
>>> wordcount = evaluate.load("word_count")
>>> results = wordcount.compute(data=data)
>>> print(results)
{'total_word_count': 5, 'unique_words': 5}
```

LANGUAGE: python
CODE:
```
>>> data = ["hello sun and goodbye moon", "foo bar foo bar"]
>>> wordcount = evaluate.load("word_count")
>>> results = wordcount.compute(data=data)
>>> print(results)
{'total_word_count': 9, 'unique_words': 7}
```

--------------------------------

TITLE: Scikit-Learn: Load IMDb Dataset
DESCRIPTION: Loads the IMDb dataset using the `datasets` library for text classification tasks.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_0

LANGUAGE: Python
CODE:
```
from datasets import load_dataset

ds = load_dataset("imdb")
```

--------------------------------

TITLE: Define EvaluationSuite with SubTasks
DESCRIPTION: This Python code defines a custom EvaluationSuite by inheriting from `evaluate.EvaluationSuite`. It includes two `SubTask` definitions for text classification on the 'imdb' and 'sst2' datasets, specifying the task type, data, split, and arguments for the accuracy metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_17

LANGUAGE: Python
CODE:
```
import evaluate
from evaluate.evaluation_suite import SubTask

class Suite(evaluate.EvaluationSuite):

    def __init__(self, name):
        super().__init__(name)

        self.suite = [
            SubTask(
                task_type="text-classification",
                data="imdb",
                split="test[:1]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "text",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0.0,
                        "LABEL_1": 1.0
                    }
                }
            ),
            SubTask(
                task_type="text-classification",
                data="sst2",
                split="test[:1]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "sentence",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0.0,
                        "LABEL_1": 1.0
                    }
                }
            )
        ]
```

--------------------------------

TITLE: Compute SacreBLEU Score
DESCRIPTION: Demonstrates how to load the SacreBLEU metric and compute BLEU scores for a given set of predictions and references. It shows the expected output keys and the rounded score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/sacrebleu/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
>>> predictions = ["hello there general kenobi", "foo bar foobar"]
>>> references = [["hello there general kenobi", "hello there !"],
...                 ["foo bar foobar", "foo bar foobar"]]
>>> sacrebleu = evaluate.load("sacrebleu")
>>> results = sacrebleu.compute(predictions=predictions, 
...                             references=references)
>>> print(list(results.keys()))
['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']
>>> print(round(results["score"], 1))
100.0
```

LANGUAGE: Python
CODE:
```
>>> predictions = ["hello there general kenobi", 
...                 "on our way to ankh morpork"]
>>> references = [["hello there general kenobi", "hello there !"],
...                 ["goodbye ankh morpork", "ankh morpork"]]
>>> sacrebleu = evaluate.load("sacrebleu")
>>> results = sacrebleu.compute(predictions=predictions, 
...                             references=references)
>>> print(list(results.keys()))
['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']
>>> print(round(results["score"], 1))
39.8
```

--------------------------------

TITLE: Load XTREME-S Metric
DESCRIPTION: Loads the XTREME-S metric for a specific subset of the benchmark. The subset determines the evaluation tasks and languages.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> xtreme_s_metric = evaluate.load('xtreme_s', 'mls')
```

--------------------------------

TITLE: Load Specific Module Type
DESCRIPTION: Loads a specific evaluation module, 'word_length', by explicitly specifying its type as 'measurement'. This is useful for disambiguation when module names might overlap.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/a_quick_tour.mdx#_snippet_1

LANGUAGE: Python
CODE:
```
>>> word_length = evaluate.load("word_length", module_type="measurement")
```

--------------------------------

TITLE: Unbabel's WMT20 Metrics Shared Task Participation (BibTeX)
DESCRIPTION: BibTeX entry for the paper describing Unbabel's participation in the WMT20 Metrics Shared Task, including authors and publication details.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@inproceedings{rei-EtAl:2020:WMT,
   author    = {Rei, Ricardo  and  Stewart, Craig  and  Farinha, Ana C  and  Lavie, Alon},
   title     = {Unbabel's Participation in the WMT20 Metrics Shared Task},
   booktitle      = {Proceedings of the Fifth Conference on Machine Translation},
   month          = {November},
   year           = {2020},
   address        = {Online},
   publisher      = {Association for Computational Linguistics},
   pages     = {909--918},
}

```

--------------------------------

TITLE: Load and Compute Spearman Correlation with P-value
DESCRIPTION: Shows how to compute the Spearman correlation coefficient along with its p-value by setting the `return_pvalue` argument to `True`. This provides a more comprehensive statistical analysis.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/spearmanr/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> spearmanr_metric = evaluate.load("spearmanr")
>>> results = spearmanr_metric.compute(references=[1, 2, 3, 4, 5], predictions=[10, 9, 2.5, 6, 4], return_pvalue=True)
>>> print(results)
{'spearmanr': -0.7, 'spearmanr_pvalue': 0.1881204043741873}
>>> print(results['spearmanr'])
-0.7
>>> print(results['spearmanr_pvalue'])
0.1881204043741873
```

--------------------------------

TITLE: BibTeX Citations for chrF
DESCRIPTION: Provides BibTeX entries for the papers that introduced and discussed the chrF and chrF++ metrics, as well as a related paper on reporting evaluation scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/chrf/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@inproceedings{popovic-2015-chrf,
    title = "chr{F}: character n-gram {F}-score for automatic {MT} evaluation",
    author = "Popovi{'c}, Maja",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3049",
    doi = "10.18653/v1/W15-3049",
    pages = "392--395",
}
@inproceedings{popovic-2017-chrf,
    title = "chr{F}++: words helping character n-grams",
    author = "Popovi{'c}, Maja",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4770",
    doi = "10.18653/v1/W17-4770",
    pages = "612--618",
}
@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6319",
    pages = "186--191",
}
```

--------------------------------

TITLE: Set Logging Verbosity to INFO in Python
DESCRIPTION: Demonstrates how to set the logging verbosity level to INFO for the Hugging Face Evaluate library using a direct Python setter. This allows for more detailed output during library operations.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/logging_methods.mdx#_snippet_0

LANGUAGE: Python
CODE:
```
import evaluate
evaluate.logging.set_verbosity_info()
```

--------------------------------

TITLE: Compute Google BLEU Score
DESCRIPTION: Demonstrates how to load and compute the Google BLEU score using the Hugging Face evaluate library. It takes a list of predictions and a list of references to calculate the score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/google_bleu/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import evaluate

sentence1 = "the cat sat on the mat"
sentence2 = "the cat ate the mat"
google_bleu = evaluate.load("google_bleu")
result = google_bleu.compute(predictions=[sentence1], references=[[sentence2]])
print(result)
>>> {'google_bleu': 0.3333333333333333}
```

--------------------------------

TITLE: Compute Accuracy for SuperGLUE COPA Subset
DESCRIPTION: Loads the SuperGLUE metric for the COPA subset and computes accuracy by comparing predictions and references. Requires the 'super_glue' library.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/super_glue/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
super_glue_metric = load('super_glue', 'copa')  # any of ["copa", "rte", "wic", "wsc", "wsc.fixed", "boolq", "axg"]
predictions = [0, 1]
references = [0, 1]
results = super_glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Calculate Perplexity with Hugging Face Evaluate
DESCRIPTION: Demonstrates how to load the perplexity measurement and compute perplexity scores for a list of input texts using a specified model ID. It also shows how to access the computed 'perplexities' and 'mean_perplexity' from the results.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/perplexity/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
perplexity = load("perplexity",  module_type= "measurement")
results = perplexity.compute(data=input_texts, model_id='gpt2')
```

LANGUAGE: python
CODE:
```
import evaluate
perplexity = evaluate.load("perplexity", module_type="measurement")
input_texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]
results = perplexity.compute(model_id='gpt2',
                             add_start_token=False,
                             data=input_texts)
print(list(results.keys()))
print(round(results["mean_perplexity"], 2))
print(round(results["perplexities"][0], 2))
```

LANGUAGE: python
CODE:
```
import evaluate
import datasets
perplexity = evaluate.load("perplexity", module_type= "measurement")
input_texts = datasets.load_dataset("wikitext",
                                    "wikitext-2-raw-v1",
                                    split="test")["text"][:50]
input_texts = [s for s in input_texts if s!='']
results = perplexity.compute(model_id='gpt2',
                             data=input_texts)
print(list(results.keys()))
print(round(results["mean_perplexity"], 2))
print(round(results["perplexities"][0], 2))
```

--------------------------------

TITLE: MAUVE Score with Partial Match
DESCRIPTION: Illustrates MAUVE calculation with partial matches between predictions and references, showing a lower score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mauve/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
mauve = load('mauve')
predictions = ["hello world", "goodnight moon"]
references = ["hello there", "general kenobi"]
mauve_results = mauve.compute(predictions=predictions, references=references)
print(mauve_results.mauve)
0.27811372536724027
```

--------------------------------

TITLE: Define Keras Model for Fashion MNIST
DESCRIPTION: Sets up a Convolutional Neural Network (CNN) model using Keras for image classification. It includes data loading, preprocessing, and model architecture definition for the Fashion MNIST dataset.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/keras_integrations.md#_snippet_0

LANGUAGE: python
CODE:
```
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import evaluate

# We pull example code from Keras.io's guide on classifying with MNIST
# Located here: https://keras.io/examples/vision/mnist_convnet/

# Model / data parameters
input_shape = (28, 28, 1)

# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()


# Only select tshirts/tops and trousers, classes 0 and 1
def get_tshirts_tops_and_trouser(x_vals, y_vals):
    mask = np.where((y_vals == 0) | (y_vals == 1))
    return x_vals[mask], y_vals[mask]

x_train, y_train = get_tshirts_tops_and_trouser(x_train, y_train)
x_test, y_test = get_tshirts_tops_and_trouser(x_test, y_test)


# Scale images to the [0, 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)


model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(1, activation="sigmoid"),
    ]
)

```

--------------------------------

TITLE: BibTeX Citation for Evaluating LLMs Trained on Code
DESCRIPTION: This BibTeX entry provides the full citation details for the paper 'Evaluating Large Language Models Trained on Code', which is relevant to the Hugging Face Evaluate project. It includes author information, publication year, arXiv details, and primary classification.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code},
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan \\
      and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards \\
      and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray \\
      and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf \\
      and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray \\
      and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser \\
      and Mohammad Bavarian and Clemens Winter and Philippe Tillet \\
      and Felipe Petroski Such and Dave Cummings and Matthias Plappert \\
      and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss \\
      and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak \\
      and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain \\
      and William Saunders and Christopher Hesse and Andrew N. Carr \\
      and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa \\
      and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati \\
      and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei \\
      and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

--------------------------------

TITLE: Evaluate Machine Translation with COMET (Partial Match)
DESCRIPTION: This Python code illustrates using the COMET metric for machine translation evaluation with partially matching sentences. It loads the metric and computes scores using provided source, hypothesis, and reference texts.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
comet_metric = load('comet') 
source = ["Dem Feuer konnte Einhalt geboten werden", "Schulen und KindergÃ¤rten wurden erÃ¶ffnet."]
hypothesis = ["The fire could be stopped", "Schools and kindergartens were open"]
reference = ["They were able to control the fire", "Schools and kindergartens opened"]
results = comet_metric.compute(predictions=hypothesis, references=reference, sources=source)
print([round(v, 2) for v in results["scores"]])
```

--------------------------------

TITLE: GLUE Benchmark Citation (BibTeX)
DESCRIPTION: Provides the BibTeX citation for the GLUE benchmark paper, which is essential for academic referencing.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/glue/README.md#_snippet_4

LANGUAGE: bibtex
CODE:
```
@inproceedings{wang2019glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  note={In the Proceedings of ICLR.},
  year={2019}
}
```

--------------------------------

TITLE: Compute Exact Match and F1 for SuperGLUE MultiRC Subset
DESCRIPTION: Loads the SuperGLUE metric for the MultiRC subset and computes exact match and F1 scores. The input predictions are structured dictionaries.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/super_glue/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
super_glue_metric = load('super_glue', 'multirc')
predictions = [{'idx': {'answer': 0, 'paragraph': 0, 'question': 0}, 'prediction': 0}, {'idx': {'answer': 1, 'paragraph': 2, 'question': 3}, 'prediction': 1}]
references = [1,0]
results = super_glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Evaluate Multiple Metrics with `evaluate.combine`
DESCRIPTION: This snippet demonstrates how to use the `evaluate.combine` function to bundle multiple metrics (accuracy, recall, precision, f1) into a single object for simultaneous evaluation. It takes a list of metric names and returns a combined metric object that can be used with the `task_evaluator.compute` method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/base_evaluator.mdx#_snippet_1

LANGUAGE: python
CODE:
```
import evaluate

eval_results = task_evaluator.compute(
    model_or_pipeline="lvwerra/distilbert-imdb",
    data=data,
    metric=evaluate.combine(["accuracy", "recall", "precision", "f1"]),
    label_mapping={"NEGATIVE": 0, "POSITIVE": 1}
)
print(eval_results)
```

--------------------------------

TITLE: Evaluate Machine Translation with COMET (Full Match)
DESCRIPTION: This Python code snippet demonstrates how to load the COMET metric and compute scores for machine translation. It uses sample source, hypothesis, and reference sentences to calculate the quality scores, showing a full match scenario.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
comet_metric = load('comet') 
source = ["Dem Feuer konnte Einhalt geboten werden", "Schulen und KindergÃ¤rten wurden erÃ¶ffnet."]
hypothesis = ["They were able to control the fire.", "Schools and kindergartens opened"]
reference = ["They were able to control the fire.", "Schools and kindergartens opened"]
results = comet_metric.compute(predictions=hypothesis, references=reference, sources=source)
print([round(v, 1) for v in results["scores"]])
```

--------------------------------

TITLE: Load and Compute MSE
DESCRIPTION: Loads the MSE metric from the Hugging Face Evaluate library and computes the mean squared error between predictions and references. It demonstrates basic usage with sample data.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mse/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> mse_metric = evaluate.load("mse")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> results = mse_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: MMSegmentation Citation (BibTeX)
DESCRIPTION: This snippet provides the BibTeX citation for the MMSegmentation project, a toolbox and benchmark for semantic segmentation developed by OpenMMLab. It includes author, license, publication details, and a URL to the GitHub repository.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mean_iou/README.md#_snippet_2

LANGUAGE: bibtex
CODE:
```
@software{MMSegmentation_Contributors_OpenMMLab_Semantic_Segmentation_2020,
author = {{MMSegmentation Contributors}},
license = {Apache-2.0},
month = {7},
title = {{OpenMMLab Semantic Segmentation Toolbox and Benchmark}},
url = {https://github.com/open-mmlab/mmsegmentation},
year = {2020}
}
```

--------------------------------

TITLE: Compute MAE with Multi-dimensional Lists and Raw Values
DESCRIPTION: Shows how to compute MAE with multi-dimensional prediction and reference lists, specifically using the `raw_values` option for multioutput. This allows for detailed error analysis across multiple outputs.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mae/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> mae_metric = evaluate.load("mae", "multilist")
>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]
>>> references = [[0, 2], [-1, 2], [8, -5]]
>>> results = mae_metric.compute(predictions=predictions, references=references)
>>> print(results)
{'mae': 0.75}
>>> results = mae_metric.compute(predictions=predictions, references=references, multioutput='raw_values')
>>> print(results)
{'mae': array([0.5, 1. ])}
```

--------------------------------

TITLE: Compute WER using Hugging Face evaluate
DESCRIPTION: This code snippet demonstrates how to load and use the 'wer' metric from the Hugging Face `evaluate` library to compute the word error rate between predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
wer = load("wer")
wer_score = wer.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Calculate Recall with Sample Weight (Binary Classification)
DESCRIPTION: Illustrates the use of `sample_weight` to give different importance to samples during recall calculation. This allows for more nuanced evaluation when some data points are more critical than others.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/recall/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> recall_metric = evaluate.load('recall')
>>> sample_weight = [0.9, 0.2, 0.9, 0.3, 0.8]
>>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1], sample_weight=sample_weight)
>>> print(results)
{'recall': 0.55}
```

--------------------------------

TITLE: Compute Word Count from Hugging Face Datasets
DESCRIPTION: Illustrates how to compute word counts for text data loaded from a Hugging Face Datasets object. It shows the process of loading a dataset and passing its text column to the 'word_count' computation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/word_count/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> imdb = datasets.load_dataset('imdb', split = 'train')
>>> wordcount = evaluate.load("word_count")
>>> results = wordcount.compute(data=imdb['text'])
>>> print(results)
{'total_word_count': 5678573, 'unique_words': 74849}
```

--------------------------------

TITLE: Load and Compute McNemar Statistic
DESCRIPTION: Loads the McNemar comparison metric and computes the statistic and p-value by comparing predictions from two models against reference labels. This is useful for analyzing paired nominal data and assessing classifier performance differences.

SOURCE: https://github.com/huggingface/evaluate/blob/main/comparisons/mcnemar/README.md#_snippet_0

LANGUAGE: python
CODE:
```
mcnemar = evaluate.load("mcnemar")
results = mcnemar.compute(references=[1, 0, 1], predictions1=[1, 1, 1], predictions2=[1, 0, 1])
print(results)
```

--------------------------------

TITLE: Citation for Seqeval
DESCRIPTION: BibTeX entry for the seqeval library, providing citation information for academic and research use.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/seqeval/README.md#_snippet_4

LANGUAGE: BibTeX
CODE:
```
@misc{seqeval,
  title={{seqeval}: A Python framework for sequence labeling evaluation},
  url={https://github.com/chakki-works/seqeval},
  note={Software available from https://github.com/chakki-works/seqeval},
  author={Hiroki Nakayama},
  year={2018},
}
```

--------------------------------

TITLE: Compute Accuracy with Normalize False
DESCRIPTION: Shows how to compute the accuracy metric with the `normalize` parameter set to `False`, which returns the raw count of correctly classified samples instead of the fraction.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/accuracy/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> accuracy_metric = evaluate.load("accuracy")
>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)
>>> print(results)
{'accuracy': 3.0}
```

--------------------------------

TITLE: XTREME-S Citation
DESCRIPTION: BibTeX citation for the XTREME-S paper, 'XTREME-S: Evaluating Cross-lingual Speech Representations'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_6

LANGUAGE: BibTeX
CODE:
```
@article{conneau2022xtreme,
  title={XTREME-S: Evaluating Cross-lingual Speech Representations},
  author={Conneau, Alexis and Bapna, Ankur and Zhang, Yu and Ma, Min and von Platen, Patrick and Lozhkov, Anton and Cherry, Colin and Jia, Ye and Rivera, Clara and Kale, Mihir and others},
  journal={arXiv preprint arXiv:2203.10752},
  year={2022}
}
```

--------------------------------

TITLE: Compute F1 Score with Sample Weights
DESCRIPTION: Illustrates the use of `sample_weight` when computing the F1 score. This allows for differential weighting of individual samples in the calculation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/f1/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> f1_metric = evaluate.load("f1")
>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])
>>> print(round(results['f1'], 2))
0.35
```

--------------------------------

TITLE: MAUVE Metric Citation (BibTeX)
DESCRIPTION: Provides the BibTeX citation for the MAUVE paper, which is essential for academic referencing when using or discussing the MAUVE metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mauve/README.md#_snippet_3

LANGUAGE: bibtex
CODE:
```
@inproceedings{pillutla-etal:mauve:neurips2021,
  title={MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  booktitle = {NeurIPS},
  year  	= {2021}
}
```

--------------------------------

TITLE: Load and Compute Label Distribution
DESCRIPTION: Loads the 'label_distribution' measurement and computes the distribution and skew for a given list of labels. This is a fundamental operation for analyzing dataset balance.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/label_distribution/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> distribution = evaluate.load("label_distribution")
>>> data = [1, 0, 2, 2, 0, 0, 0, 0, 0, 2]
>>> results = distribution.compute(data=data)
```

--------------------------------

TITLE: Scikit-learn Citation (BibTeX)
DESCRIPTION: This BibTeX entry provides the citation details for the scikit-learn library, a popular machine learning toolkit in Python. It includes title, authors, journal, volume, pages, and year of publication.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/f1/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@article{scikit-learn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
           and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
           and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
           Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}
```

--------------------------------

TITLE: Compute BERTScore
DESCRIPTION: Demonstrates how to load and use the BERTScore metric from the Hugging Face evaluate library. It takes predictions, references, and language as input to compute the scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bertscore/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
bertscore = load("bertscore")
predictions = ["hello there", "general kenobi"]
references = ["hello there", "general kenobi"]
results = bertscore.compute(predictions=predictions, references=references, lang="en")
```

--------------------------------

TITLE: SARI Metric Citation (BibTeX)
DESCRIPTION: Provides the BibTeX entry for citing the original paper that introduced the SARI metric for optimizing statistical machine translation for text simplification.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/sari/README.md#_snippet_3

LANGUAGE: bibtex
CODE:
```
@inproceedings{xu-etal-2016-optimizing,
title = {Optimizing Statistical Machine Translation for Text Simplification},
authors={Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},
journal = {Transactions of the Association for Computational Linguistics},
volume = {4},
year={2016},
url = {https://www.aclweb.org/anthology/Q16-1029},
pages = {401--415},
}
```

--------------------------------

TITLE: Compute FrugalScore
DESCRIPTION: Computes the FrugalScore for given predictions and references. Optional parameters include batch_size, max_length, and device.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/frugalscore/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> results = frugalscore.compute(predictions=['hello there', 'huggingface'], references=['hello world', 'hugging face'], batch_size=16, max_length=64, device="gpu")
```

--------------------------------

TITLE: Compute MAPE with Raw Values for Multi-dimensional Lists
DESCRIPTION: Illustrates how to compute MAPE for multi-dimensional lists using the `raw_values` configuration. This returns the MAPE score for each output dimension separately, providing more granular insights.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mape/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> mape_metric = evaluate.load("mape", "multilist")
>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]
>>> references = [[0.1, 2], [-1, 2], [8, -5]]
>>> results = mape_metric.compute(predictions=predictions, references=references, multioutput='raw_values')
>>> print(results)
{'mape': array([1.3749..., 0.4])}
```

--------------------------------

TITLE: Compute Toxicity Scores (Default)
DESCRIPTION: Calculates toxicity scores for a list of input texts using the default toxicity model. The output is a list of toxicity scores, one for each input sentence.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/toxicity/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
toxicity = evaluate.load("toxicity", module_type="measurement")
input_texts = ["she went to the library", "he is a douchebag"]
results = toxicity.compute(predictions=input_texts)
print([round(s, 4) for s in results["toxicity"]])
```

--------------------------------

TITLE: Compute MAUVE Score
DESCRIPTION: Calculates the MAUVE score by comparing prediction and reference text distributions. Requires the 'evaluate' library and takes lists of strings as input.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mauve/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
mauve = load('mauve')
predictions = ["hello world", "goodnight moon"]
references = ["hello world",  "goodnight moon"]
mauve_results = mauve.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Set Logging Verbosity via Environment Variable
DESCRIPTION: Shows how to control the logging verbosity of the Hugging Face Evaluate library by setting the EVALUATE_VERBOSITY environment variable before running a Python script. This method allows overriding the default verbosity without modifying the code.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/logging_methods.mdx#_snippet_1

LANGUAGE: Bash
CODE:
```
EVALUATE_VERBOSITY=error ./myprogram.py
```

--------------------------------

TITLE: R^2 Metric Citation
DESCRIPTION: BibTeX entry for the R^2 model metric, providing citation details for academic reference.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/r_squared/README.md#_snippet_1

LANGUAGE: bibtex
CODE:
```
@article{r_squared_model,
  title={The R^2 Model Metric: A Comprehensive Guide},
  author={John Doe},
  journal={Journal of Model Evaluation},
  volume={10},
  number={2},
  pages={101-112},
  year={2022},
  publisher={Model Evaluation Society}}

```

--------------------------------

TITLE: Compute Accuracy with Sample Weight
DESCRIPTION: Illustrates computing accuracy while applying sample weights to the predictions and references. This allows for differential weighting of samples in the accuracy calculation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/accuracy/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> accuracy_metric = evaluate.load("accuracy")
>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])
>>> print(results)
{'accuracy': 0.8778625954198473}
```

--------------------------------

TITLE: Calculate chrF Score
DESCRIPTION: Demonstrates the basic usage of the 'chrf' metric to calculate chrF scores. It takes prediction and reference lists as input and returns a dictionary containing the score and order details.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/chrf/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> prediction = ["The relationship between cats and dogs is not exactly friendly.", "a good bookshop is just a genteel black hole that knows how to read."]
>>> reference = [["The relationship between dogs and cats is not exactly friendly.", ], ["A good bookshop is just a genteel Black Hole that knows how to read."]]
>>> chrf = evaluate.load("chrf")
>>> results = chrf.compute(predictions=prediction, references=reference)
>>> print(results)
{'score': 84.64214891738334, 'char_order': 6, 'word_order': 0, 'beta': 2}
```

--------------------------------

TITLE: BibTeX Citation for BLEU Score Clarity Paper
DESCRIPTION: This snippet provides the BibTeX entry for the paper 'A Call for Clarity in Reporting BLEU Scores' by Matt Post, which is relevant to the Hugging Face Evaluate library. It includes details such as title, author, publication venue, and URL.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/sacrebleu/README.md#_snippet_1

LANGUAGE: bibtex
CODE:
```
@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6319",
    pages = "186--191",
}
```

--------------------------------

TITLE: Scikit-Learn: Build TF-IDF and Naive Bayes Pipeline
DESCRIPTION: Constructs a Scikit-Learn pipeline for text classification, including TF-IDF vectorization and a Naive Bayes classifier. This pipeline is trained on the IMDb dataset.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_1

LANGUAGE: Python
CODE:
```
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer

text_clf = Pipeline([
        ('vect', CountVectorizer()),
        ('tfidf', TfidfTransformer()),
        ('clf', MultinomialNB()),
])

text_clf.fit(ds["train"]["text"], ds["train"]["label"])
```

--------------------------------

TITLE: Python: Compute Precision (Binary)
DESCRIPTION: Calculates the precision for a binary classification task. It takes references and predictions as input. The `evaluate.load('precision')` function initializes the metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/precision/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> precision_metric = evaluate.load("precision")
>>> results = precision_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])
>>> print(results)
{'precision': 0.5}
```

--------------------------------

TITLE: Load and Compute MASE Metric
DESCRIPTION: Loads the MASE metric from the Hugging Face evaluate library and computes it using provided predictions, references, and training data. This is the basic usage for evaluating forecast accuracy.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mase/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> mase_metric = evaluate.load("mase")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> training = [5, 0.5, 4, 6, 3, 5, 2]
>>> results = mase_metric.compute(predictions=predictions, references=references, training=training)
```

--------------------------------

TITLE: Visualize Model Performance with Radar Plot
DESCRIPTION: Visualize different aspects of model performance using the radar_plot function. This function takes model results and names as input and can invert the range for metrics where lower values are better, such as latency. The resulting plot can be displayed or saved locally.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/base_evaluator.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
import evaluate
from evaluate.visualization import radar_plot

>>> plot = radar_plot(data=results, model_names=models, invert_range=["latency_in_seconds"])
>>> plot.show()
```

--------------------------------

TITLE: Compute F1 Score for Multiclass Classification (various averages)
DESCRIPTION: Demonstrates computing the F1 score for multiclass classification using different averaging methods ('macro', 'micro', 'weighted', and None). This shows how to handle multi-class outputs.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/f1/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> predictions = [0, 2, 1, 0, 0, 1]
>>> references = [0, 1, 2, 0, 1, 2]
>>> results = f1_metric.compute(predictions=predictions, references=references, average="macro")
>>> print(round(results['f1'], 2))
0.27
>>> results = f1_metric.compute(predictions=predictions, references=references, average="micro")
>>> print(round(results['f1'], 2))
0.33
>>> results = f1_metric.compute(predictions=predictions, references=references, average="weighted")
>>> print(round(results['f1'], 2))
0.27
>>> results = f1_metric.compute(predictions=predictions, references=references, average=None)
>>> print(results)
{'f1': array([0.8, 0. , 0. ])}
```

--------------------------------

TITLE: ROUGE Output with Aggregator False
DESCRIPTION: Demonstrates the output format of the ROUGE metric when the `use_aggregator` parameter is set to `False`. In this case, scores are provided for each sentence individually.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_3

LANGUAGE: python
CODE:
```
{'rouge1': [0.6666666666666666, 1.0], 'rouge2': [0.0, 1.0]}
```

--------------------------------

TITLE: chrF Output Format - Python
DESCRIPTION: Illustrates the expected output format for the chrF metric computation. The output is a dictionary containing the chrF score, character n-gram order, word n-gram order, and beta value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/chrf/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
{'score': 61.576379378113785, 'char_order': 6, 'word_order': 0, 'beta': 2}
```

--------------------------------

TITLE: Spacy: Analyze Sentiment Polarity
DESCRIPTION: Processes a list of texts using the Spacy pipeline with `spacytextblob` to extract sentiment polarity. It then prints the text and its corresponding polarity.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_6

LANGUAGE: Python
CODE:
```
texts = ["This movie is horrible", "This movie is awesome"]
results = nlp.pipe(texts)

for txt, res in zip(texts, results):
    print(f"{text} | Polarity: {res._.blob.polarity}")
```

--------------------------------

TITLE: Evaluate Accuracy Metric with Scikit-Learn Predictions
DESCRIPTION: Demonstrates how to compute the accuracy metric using Hugging Face's Evaluate library with predictions generated from a Scikit-Learn pipeline. It converts the predictions and references to lists before computing the accuracy.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/sklearn_integrations.mdx#_snippet_2

LANGUAGE: python
CODE:
```
# Evaluate metrics accept lists as inputs for values of references and predictions

y_test = y_test.tolist()
y_pred = y_pred.tolist()

# Accuracy

accuracy_metric = evaluate.load("accuracy")
accuracy = accuracy_metric.compute(references=y_test, predictions=y_pred)
print("Accuracy:", accuracy)
# Accuracy: 0.79
```

--------------------------------

TITLE: Compute HONEST Score (French)
DESCRIPTION: Computes the HONEST score for French language prompts and completions. This demonstrates the multilingual capability of the HONEST measurement.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/honest/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> honest = evaluate.load('honest', 'fr')
>>> completions = [['PDG', 'avocat', 'pÃ¨re'], ['bon','excellent', 'intelligent'], ['secrÃ©taire', 'prostituÃ©e', 'bonne'], ['belle', 'intelligente', 'grande']]
>>> result = honest.compute(predictions=completions)
>>> round(result['honest_score'],3)
0.083
```

--------------------------------

TITLE: Enable Code Eval Execution
DESCRIPTION: Sets the HF_ALLOW_CODE_EVAL environment variable to '1' to permit the execution of the code evaluation. This is a security measure, and users should ensure they understand the risks and have taken appropriate precautions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import os
os.environ["HF_ALLOW_CODE_EVAL"] = "1"
```

--------------------------------

TITLE: Scikit-learn Citation
DESCRIPTION: BibTeX entry for the scikit-learn library, often cited in machine learning research for its comprehensive tools, including evaluation metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mape/README.md#_snippet_3

LANGUAGE: bibtex
CODE:
```
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
```

--------------------------------

TITLE: ROUGE Output with Aggregator True
DESCRIPTION: Illustrates the output format of the ROUGE metric when the `use_aggregator` parameter is set to `True` (the default). The output provides aggregated scores for the entire set of predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_4

LANGUAGE: python
CODE:
```
{'rouge1': 1.0, 'rouge2': 1.0}
```

--------------------------------

TITLE: Python: Compute Precision (Multiclass with different averages)
DESCRIPTION: Demonstrates computing precision for a multiclass classification problem using various averaging methods: 'macro', 'micro', 'weighted', and 'None'. Each method provides a different perspective on the model's performance across classes.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/precision/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> predictions = [0, 2, 1, 0, 0, 1]
>>> references = [0, 1, 2, 0, 1, 2]
>>> results = precision_metric.compute(predictions=predictions, references=references, average='macro')
>>> print(results)
{'precision': 0.2222222222222222}
>>> results = precision_metric.compute(predictions=predictions, references=references, average='micro')
>>> print(results)
{'precision': 0.3333333333333333}
>>> results = precision_metric.compute(predictions=predictions, references=references, average='weighted')
>>> print(results)
{'precision': 0.2222222222222222}
>>> results = precision_metric.compute(predictions=predictions, references=references, average=None)
>>> print([round(res, 2) for res in results['precision']])
[0.67, 0.0, 0.0]
```

--------------------------------

TITLE: Compute MSE with RMSE Option
DESCRIPTION: Demonstrates how to compute the Mean Squared Error (MSE) using the Hugging Face Evaluate library, specifically showing how to obtain the Root Mean Squared Error (RMSE) by setting the 'squared' parameter to False.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mse/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> mse_metric = evaluate.load("mse")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> rmse_result = mse_metric.compute(predictions=predictions, references=references, squared=False)
>>> print(rmse_result)
{'mse': 0.6123724356957945}
```

--------------------------------

TITLE: Calculate CUAD Metric (Minimal Values)
DESCRIPTION: This Python code snippet illustrates the minimal usage of the CUAD metric, showing how to compute it with minimal predictions and references. It highlights a scenario with no matching predictions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cuad/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
from evaluate import load
cuad_metric = load("cuad")
predictions = [{'prediction_text': ['The Company appoints the Distributor as an exclusive distributor of Products in the Market, subject to the terms and conditions of this Agreement.'], 'id': 'LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGREEMENT__Exclusivity_0'}]
references = [{'answers': {'answer_start': [143], 'text': 'The seller'}, 'id': 'LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGREEMENT__Exclusivity_0'}]
results = cuad_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Disable Progress Bar
DESCRIPTION: Demonstrates how to disable the display of `tqdm` progress bars during Hugging Face Evaluate downloads and processing. This can be useful in environments where progress bars are not desired or cause issues.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/logging_methods.mdx#_snippet_4

LANGUAGE: Python
CODE:
```
import evaluate
evaluate.logging.disable_progress_bar()
```

--------------------------------

TITLE: Compute Toxicity Ratio
DESCRIPTION: Calculates the ratio of toxic sentences within a given list of input texts. This requires specifying the 'ratio' aggregation method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/toxicity/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
toxicity = evaluate.load("toxicity", module_type="measurement")
input_texts = ["she went to the library", "he is a douchebag"]
results = toxicity.compute(predictions=input_texts, aggregation="ratio")
print(results['toxicity_ratio'])
```

--------------------------------

TITLE: Compute Matthews Correlation Coefficient with Sample Weights
DESCRIPTION: Shows how to compute the Matthews Correlation Coefficient while also providing sample weights. This allows for differential weighting of samples in the calculation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/matthews_correlation/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> matthews_metric = evaluate.load("matthews_correlation")
>>> results = matthews_metric.compute(references=[1, 3, 2, 0, 3, 2],
...                                     predictions=[1, 2, 2, 0, 3, 3],
...                                     sample_weight=[0.5, 3, 1, 1, 1, 2])
>>> print(results)
{'matthews_correlation': 0.09782608695652174}
```

--------------------------------

TITLE: BibTeX Citation for Coreference Evaluation
DESCRIPTION: BibTeX entries for academic papers discussing coreference evaluation metrics and algorithms. These are commonly used for citing research in academic writing.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/coval/README.md#_snippet_3

LANGUAGE: BibTeX
CODE:
```
@InProceedings{moosavi2019minimum,
  author = {Nafise Sadat Moosavi, Leo Born, Massimo Poesio & Michael Strube},
  title = {Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection},
  year = {2019},
  booktitle = {Proceedings of the 57th Annual Meeting of
      the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
}
```

LANGUAGE: BibTeX
CODE:
```
@inproceedings{10.3115/1072399.1072405,
  author = {Vilain, Marc and Burger, John and Aberdeen, John and Connolly, Dennis and Hirschman, Lynette},
  title = {A Model-Theoretic Coreference Scoring Scheme},
  year = {1995},
  isbn = {1558604022},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  url = {https://doi.org/10.3115/1072399.1072405},
  doi = {10.3115/1072399.1072405},
  booktitle = {Proceedings of the 6th Conference on Message Understanding},
  pages = {45â€“52},
  numpages = {8},
  location = {Columbia, Maryland},
  series = {MUC6 â€™95}
}
```

LANGUAGE: BibTeX
CODE:
```
@INPROCEEDINGS{Bagga98algorithmsfor,
    author = {Amit Bagga & Breck Baldwin},
    title = {Algorithms for Scoring Coreference Chains},
    booktitle = {In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference},
    year = {1998},
    pages = {563--566}
}
```

LANGUAGE: BibTeX
CODE:
```
@INPROCEEDINGS{Luo05oncoreference,
    author = {Xiaoqiang Luo},
    title = {On coreference resolution performance metrics},
    booktitle = {In Proc. of HLT/EMNLP},
    year = {2005},
    pages = {25--32},
    publisher = {URL}
}
```

LANGUAGE: BibTeX
CODE:
```
@inproceedings{moosavi-strube-2016-coreference,
    title = "Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric",
    author = "Moosavi, Nafise Sadat  and
      Strube, Michael",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1060",
    doi = "10.18653/v1/P16-1060",
    pages = "632--642",
}
```

--------------------------------

TITLE: SQuAD v2 Metric Citation
DESCRIPTION: BibTeX entry for the SQuAD v2 paper, providing citation details for academic referencing.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/squad_v2/README.md#_snippet_4

LANGUAGE: bibtex
CODE:
```
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
```

--------------------------------

TITLE: Visualize Label Distribution with Matplotlib
DESCRIPTION: Shows how to use the computed label distribution results to plot a histogram of label frequencies using Matplotlib. This is useful for visually inspecting dataset balance.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/label_distribution/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> import matplotlib.pyplot as plt
>>> data = [1, 0, 2, 2, 0, 0, 0, 0, 0, 2]
>>> distribution = evaluate.load("label_distribution")
>>> results = distribution.compute(data=data)
>>> plt.bar(results['label_distribution']['labels'], results['label_distribution']['fractions'])
>>> plt.show()
```

--------------------------------

TITLE: TREC Eval with TrecTools Data
DESCRIPTION: Demonstrates a more realistic use case for the TREC Eval metric, loading data from 'trectools' compatible files (robust03_qrels.txt and input.InexpC2). It reads CSV files, converts them to dictionaries, and computes the results.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/trec_eval/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
import pandas as pd

qrel = pd.read_csv("robust03_qrels.txt", sep="\s+", names=["query", "q0", "docid", "rel"])
qrel["q0"] = qrel["q0"].astype(str)
qrel = qrel.to_dict(orient="list")

run = pd.read_csv("input.InexpC2", sep="\s+", names=["query", "q0", "docid", "rank", "score", "system"])
run = run.to_dict(orient="list")

trec_eval = evaluate.load("trec_eval")
result = trec_eval.compute(run=[run], qrel=[qrel])
```

--------------------------------

TITLE: Scikit-Learn: Create Evaluator-Compatible Wrapper
DESCRIPTION: Wraps a Scikit-Learn pipeline to be compatible with the Hugging Face evaluator. The wrapper class provides a `__call__` method that returns predictions in the expected format and includes a `task` attribute.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_2

LANGUAGE: Python
CODE:
```
class ScikitEvalPipeline:
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.task = "text-classification"

    def __call__(self, input_texts, **kwargs):
        return [{"label": p} for p in self.pipeline.predict(input_texts)]

pipe = ScikitEvalPipeline(text_clf)
```

--------------------------------

TITLE: Set Logging Verbosity to a Specific Level
DESCRIPTION: Illustrates how to set the logging verbosity to a chosen level (e.g., WARNING, INFO, DEBUG, ERROR) using the `set_verbosity` function in the Hugging Face Evaluate library. This offers granular control over log output.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/logging_methods.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
import evaluate
# Example: Set verbosity to WARNING
evaluate.logging.set_verbosity(30) 
# Or using the constant: evaluate.logging.set_verbosity(evaluate.logging.WARNING)
```

--------------------------------

TITLE: ROUGE with Multiple References
DESCRIPTION: Shows how to compute ROUGE scores when each prediction has multiple reference texts. This is useful for scenarios where human evaluations might provide several acceptable outputs.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> rouge = evaluate.load('rouge')
>>> predictions = ["hello there", "general kenobi"]
>>> references = [["hello", "there"], ["general kenobi", "general yoda"]]
>>> results = rouge.compute(predictions=predictions,
...                         references=references)
>>> print(results)
{'rouge1': 0.8333, 'rouge2': 0.5, 'rougeL': 0.8333, 'rougeLsum': 0.8333}
```

--------------------------------

TITLE: Average Regard Scores with Single Input
DESCRIPTION: Demonstrates how to obtain the average regard scores for each category ('neutral', 'positive', 'negative', 'other') when using the 'average' aggregation option with a single input list.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_2

LANGUAGE: python
CODE:
```
{'neutral': 0.48, 'positive': 0.01, 'negative': 0.5, 'other': 0.01}
```

--------------------------------

TITLE: Evaluate Coreference using CoVal (Python)
DESCRIPTION: This snippet demonstrates how to use the CoVal tool for coreference evaluation. It is designed to work with datasets in the CoNLL line format and implements various standard evaluation metrics.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/coval/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load

coval = load("evaluate/coval")

# Example usage (assuming you have predictions and references in CoNLL format)
# predictions = [...] 
# references = [...] 
# results = coval.compute(predictions=predictions, references=references)
# print(results)
```

--------------------------------

TITLE: Calculate chrF++ Score
DESCRIPTION: Shows how to compute chrF++ scores by setting the 'word_order' argument to 2. This variation is noted to correlate better with human judgments.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/chrf/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> prediction = ["The relationship between cats and dogs is not exactly friendly.", "a good bookshop is just a genteel black hole that knows how to read."]
>>> reference = [["The relationship between dogs and cats is not exactly friendly.", ], ["A good bookshop is just a genteel Black Hole that knows how to read."]]
>>> chrf = evaluate.load("chrf")
>>> results = chrf.compute(predictions=prediction,
...                         references=reference,
...                         word_order=2)
>>> print(results)
{'score': 82.87263732906315, 'char_order': 6, 'word_order': 2, 'beta': 2}
```

--------------------------------

TITLE: Load and Compute Code Eval Metric
DESCRIPTION: Loads the 'code_eval' metric from the evaluate library and computes the pass@k metric. It takes lists of test cases and code predictions as input, along with a list of k values to evaluate.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/code_eval/README.md#_snippet_0

LANGUAGE: python
CODE:
```
from evaluate import load
code_eval = load("code_eval")
test_cases = ["assert add(2,3)==5"]
candidates = [["def add(a,b): return a*b", "def add(a, b): return a+b"]]
pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1, 2])
```

--------------------------------

TITLE: BibTeX Citation for Hugging Face Evaluate
DESCRIPTION: This snippet provides the BibTeX formatted citation for the Hugging Face Evaluate library, referencing the SciPy 1.0 publication. It is essential for academic referencing when using the library.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/spearmanr/README.md#_snippet_2

LANGUAGE: bibtex
CODE:
```
@book{kokoska2000crc,
  title={CRC standard probability and statistics tables and formulae},
  author={Kokoska, Stephen and Zwillinger, Daniel},
  year={2000},
  publisher={Crc Press}
}
@article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}
```

--------------------------------

TITLE: BibTeX Citation for Hugging Face Evaluate
DESCRIPTION: This BibTeX entry provides the necessary information to cite the Hugging Face Evaluate library in academic work. It includes author names, publication year, and the title of the relevant paper.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cer/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@inproceedings{morris2004,
author = {Morris, Andrew and Maier, Viktoria and Green, Phil},
year = {2004},
month = {01},
pages = {},
title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}
}
```

--------------------------------

TITLE: Compute METEOR with Multiple References (Partial Match)
DESCRIPTION: This snippet illustrates computing the METEOR score with multiple references per prediction, specifically showing a case where the match might be partial. It loads the 'meteor' metric, defines predictions and references, computes the score, and prints the rounded result, which in this case is 0.69.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/meteor/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> meteor = evaluate.load('meteor')
>>> predictions = ["It is a guide to action which ensures that the military always obeys the commands of the party"]
>>> references = [['It is a guide to action that ensures that the military will forever heed Party commands', 'It is the guiding principle which guarantees the military forces always being under the command of the Party', 'It is the practical guide for the army always to heed the directions of the party']]
>>> results = meteor.compute(predictions=predictions, references=references)
>>> print(round(results['meteor'], 2))
0.69
```

--------------------------------

TITLE: CUAD Dataset Citation
DESCRIPTION: This BibTeX entry provides the citation for the CUAD dataset, which is relevant for understanding the context and origin of the CUAD metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cuad/README.md#_snippet_4

LANGUAGE: BibTeX
CODE:
```
@article{hendrycks2021cuad,
      title={CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},
      author={Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},
      journal={arXiv preprint arXiv:2103.06268},
      year={2021}
}
```

--------------------------------

TITLE: Evaluate: AudioClassificationEvaluator
DESCRIPTION: This snippet represents the AudioClassificationEvaluator from the Hugging Face evaluate library, used for evaluating audio classification models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_11

LANGUAGE: python
CODE:
```
from evaluate import AudioClassificationEvaluator

# Example usage (conceptual)
audio_evaluator = AudioClassificationEvaluator()
# results = audio_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Compute BLEURT Score (Default Model)
DESCRIPTION: Calculates the BLEURT score by comparing predicted sentences to reference sentences using the default BLEURT model. It requires the 'evaluate' library and takes lists of strings as input.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleurt/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from evaluate import load

predictions = ["hello there", "general kenobi"]
references = ["hello there", "general kenobi"]
bleurt = load("bleurt", module_type="metric")
results = bleurt.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Compute Text Duplicates with Duplicate List
DESCRIPTION: Calculates the fraction of duplicate strings and provides a list of duplicate strings with their counts. This option can be memory-intensive for large datasets.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/text_duplicates/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> data = ["hello sun", "goodbye moon", "hello sun", "foo bar", "foo bar"]
>>> duplicates = evaluate.load("text_duplicates")
>>> results = duplicates.compute(data=data, list_duplicates=True)
>>> print(results)
{'duplicate_fraction': 0.4, 'duplicates_dict': {'hello sun': 2, 'foo bar': 2}}
```

--------------------------------

TITLE: BLEU Calculation with NLTK Tokenizer
DESCRIPTION: Shows how to use a custom tokenizer, specifically the word_tokenize from NLTK, with the BLEU metric. This allows for more flexible text processing before calculating the BLEU score, enabling comparison with different tokenization strategies.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleu/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> bleu = evaluate.load("bleu")
>>> from nltk.tokenize import word_tokenize
>>> predictions = [
...     ["hello there general kenobi",
...     ["foo bar foobar"]
... ]
>>> references = [
...     [["hello there general kenobi"], ["hello there!"]],
...     [["foo bar foobar"]]
... ]
>>> results = bleu.compute(predictions=predictions, references=references, tokenizer=word_tokenize)
>>> print(results)
{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}
```

--------------------------------

TITLE: Compute Matthews Correlation for SuperGLUE COLA Subset
DESCRIPTION: Loads the SuperGLUE metric for the AXB subset (often used for COLA-like tasks) and computes the Matthews correlation coefficient.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/super_glue/README.md#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import load
super_glue_metric = load('super_glue', 'axb')
references = [0, 1]
predictions = [1,1]
results = super_glue_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Wilcoxon Citation
DESCRIPTION: Provides the citation for the Wilcoxon test, referencing the original work by Frank Wilcoxon.

SOURCE: https://github.com/huggingface/evaluate/blob/main/comparisons/wilcoxon/README.md#_snippet_1

LANGUAGE: bibtex
CODE:
```
@incollection{wilcoxon1992individual,
  title={Individual comparisons by ranking methods},
  author={Wilcoxon, Frank},
  booktitle={Breakthroughs in statistics},
  pages={196--202},
  year={1992},
  publisher={Springer}
}
```

--------------------------------

TITLE: ROUGE Citation (BibTeX)
DESCRIPTION: This entry provides the BibTeX citation for the ROUGE metric paper, which is useful for academic referencing.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/rouge/README.md#_snippet_8

LANGUAGE: bibtex
CODE:
```
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}
```

--------------------------------

TITLE: Calculate CUAD Metric (Maximal Values)
DESCRIPTION: This Python code snippet demonstrates how to load the CUAD metric and compute it with maximal matching predictions and references. It shows the expected output for a perfect match scenario.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cuad/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
from evaluate import load
cuad_metric = load("cuad")
predictions = [{'prediction_text': ['The seller:', 'The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd.'], 'id': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement__Parties'}]
references = [{'answers': {'answer_start': [143, 49], 'text': ['The seller:', 'The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd.']}, 'id': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement__Parties'}]
results = cuad_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: SciPy 1.0 Citation (BibTeX)
DESCRIPTION: This BibTeX entry provides the full citation details for SciPy 1.0, a foundational library for scientific computing in Python. It includes author information, title, journal, publication year, volume, pages, and DOI.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/pearsonr/README.md#_snippet_2

LANGUAGE: BibTeX
CODE:
```
@article{2020SciPy-NMeth,
author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
      Haberland, Matt and Reddy, Tyler and Cournapeau, David and
      Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
      Bright, Jonathan and {van der Walt}, St{'e}fan J. and
      Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
      Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
      Kern, Robert and Larson, Eric and Carey, C J and
      Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
      {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
      Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
      Harris, Charles R. and Archibald, Anne M. and
      Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
      {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
      Computing in Python}},
journal = {Nature Methods},
year    = {2020},
volume  = {17},
pages   = {261--272},
adsurl  = {https://rdcu.be/b08Wh},
doi = {10.1038/s41592-019-0686-2},
}

```

--------------------------------

TITLE: Hugging Face Evaluate: TER with Normalization
DESCRIPTION: Calculates TER with normalization enabled. This preprocesses the text before comparison. Requires the 'evaluate' library. Outputs score, edits, and reference length.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/ter/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> predictions = ["does this sentence match??",
...                     "what about this sentence?"]
>>> references = [["does this sentence match", "does this sentence match!?!"],
...             ["wHaT aBoUt ThIs SeNtEnCe?", "wHaT aBoUt ThIs SeNtEnCe?"]]
>>> ter = evaluate.load("ter")
>>> results = ter.compute(predictions=predictions, 
...                         references=references, 
...                         normalized=True,
...                         case_sensitive=True)
>>> print(results)
{'score': 57.14285714285714, 'num_edits': 6, 'ref_length': 10.5}
```

--------------------------------

TITLE: BLEU Calculation with Multiple References
DESCRIPTION: Demonstrates how to compute BLEU scores when predictions can have multiple references. This is useful for scenarios where different valid translations exist for a given source text. The function handles nested lists for references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleu/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> predictions = [
...     ["hello there general kenobi",
...     ["foo bar foobar"]
... ]
>>> references = [
...     [["hello there general kenobi"], ["hello there!"]],
...     [["foo bar foobar"]]
... ]
>>> bleu = evaluate.load("bleu")
>>> results = bleu.compute(predictions=predictions, references=references)
>>> print(results)
{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}
```

--------------------------------

TITLE: WikiSplit Citation
DESCRIPTION: Provides the BibTeX citation for the paper that introduced the WikiSplit metric, allowing users to reference the original work.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wiki_split/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@article{rothe2020leveraging,
  title={Leveraging pre-trained checkpoints for sequence generation tasks},
  author={Rothe, Sascha and Narayan, Shashi and Severyn, Aliaksei},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={264--280},
  year={2020},
  publisher={MIT Press}
}
```

--------------------------------

TITLE: Calculate Recall with pos_label (Binary Classification)
DESCRIPTION: Shows how to compute recall when the positive label is specified as 0, differing from the default of 1. This is useful for datasets where the minority class is labeled as 0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/recall/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> recall_metric = evaluate.load('recall')
>>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1], pos_label=0)
>>> print(results)
{'recall': 0.5}
```

--------------------------------

TITLE: Hugging Face Evaluate: Basic TER Calculation
DESCRIPTION: Calculates the Translation Edit Rate (TER) using predictions and references. Requires the 'evaluate' library. Outputs a dictionary with score, number of edits, and reference length.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/ter/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> predictions = ["does this sentence match??",
...                     "what about this sentence?"]
>>> references = [["does this sentence match", "does this sentence match!?!"],
...             ["wHaT aBoUt ThIs SeNtEnCe?", "wHaT aBoUt ThIs SeNtEnCe?"]]
>>> ter = evaluate.load("ter")
>>> results = ter.compute(predictions=predictions, 
...                         references=references,
...                         case_sensitive=True)
>>> print(results)
{'score': 62.5, 'num_edits': 5, 'ref_length': 8.0}
```

--------------------------------

TITLE: Compute BLEURT Score (Custom Checkpoint)
DESCRIPTION: Calculates the BLEURT score using a specified BLEURT checkpoint, allowing for different model sizes and performance characteristics. It requires the 'evaluate' library and takes lists of strings as input.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleurt/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
from evaluate import load

predictions = ["hello there", "general kenobi"]
references = ["hello there", "general kenobi"]
bleurt = load("bleurt", module_type="metric", checkpoint="bleurt-base-128")
results = bleurt.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Citation for Scikit-learn (BibTeX)
DESCRIPTION: BibTeX entry for the paper 'Scikit-learn: Machine Learning in Python' by Pedregosa et al., published in the Journal of Machine Learning Research.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_9

LANGUAGE: bibtex
CODE:
```
@article{scikit-learn,
title={Scikit-learn: Machine Learning in {P}ython},
author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
journal={Journal of Machine Learning Research},
volume={12},
pages={2825--2830},
year={2011}
}
```

--------------------------------

TITLE: Load and Compute Regard Measurement
DESCRIPTION: Loads the 'regard' measurement from the Hugging Face evaluate library and computes the sentiment scores for two groups of sentences. This demonstrates the basic usage for comparing language polarity.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> regard = evaluate.load("regard", module_type="measurement")
>>> group1 = ['xyz are described as mean', 'xyz are thought of as being too ambitious']
>>> group2 = ['xyz are known for making too much noise', 'xyz are described as often violent']
>>> regard.compute(data = group1, references = group2)
```

--------------------------------

TITLE: Compute METEOR with Multiple References per Prediction
DESCRIPTION: This snippet demonstrates how to compute the METEOR score when multiple reference translations are available for each prediction. It loads the 'meteor' metric, defines predictions and a list of reference lists, computes the score, and prints the rounded result.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/meteor/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> meteor = evaluate.load('meteor')
>>> predictions = ["It is a guide to action which ensures that the military always obeys the commands of the party"]
>>> references = [['It is a guide to action that ensures that the military will forever heed Party commands', 'It is the guiding principle which guarantees the military forces always being under the command of the Party', 'It is the practical guide for the army always to heed the directions of the party']]
>>> results = meteor.compute(predictions=predictions, references=references)
>>> print(round(results['meteor'], 2))
1.0
```

--------------------------------

TITLE: Compute Brier Score with String Labels and Positive Label
DESCRIPTION: Shows how to compute the Brier Score when the ground truth references contain string labels. It highlights the need to specify the 'pos_label' argument in such cases.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/brier_score/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> brier_score_metric = evaluate.load("brier_score")
>>> predictions =  np.array([0, 0, 1, 1])
>>> references =  np.array([0.1, 0.9, 0.8, 0.3])
>>> results = brier_score.compute(predictions, references, pos_label="ham")
>>> print(results) 
{'brier_score': 0.3375}
```

--------------------------------

TITLE: COMET-22 Citation (BibTeX)
DESCRIPTION: BibTeX entry for the COMET-22 paper, detailing the authors, publication venue, and other relevant metadata for citation purposes.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/comet/README.md#_snippet_4

LANGUAGE: bibtex
CODE:
```
@inproceedings{rei-etal-2022-comet,
    title = "{COMET}-22: Unbabel-{IST} 2022 Submission for the Metrics Shared Task",
    author = "Rei, Ricardo  and
      C. de Souza, Jos{'e} G.  and
      Alves, Duarte  and
      Zerva, Chrysoula  and
      Farinha, Ana C  and
      Glushkova, Taisiya  and
      Lavie, Alon  and
      Coheur, Luisa  and
      Martins, Andr{'e} F. T.",
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.52",
    pages = "578--585",
}

```

--------------------------------

TITLE: Train Keras Model with Custom Callback
DESCRIPTION: Compiles and trains the Keras model using binary cross-entropy loss and the Adam optimizer. It integrates the custom `MetricsCallback` to monitor accuracy during training.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/keras_integrations.md#_snippet_2

LANGUAGE: python
CODE:
```
batch_size = 128
epochs = 2

model.compile(loss="binary_crossentropy", optimizer="adam")

model_history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, 
callbacks = [MetricsCallback(x_data = x_train, y_data = y_train, metric_name = "accuracy")])

```

--------------------------------

TITLE: Compute Exact Match Score
DESCRIPTION: Loads the 'exact_match' comparison metric and computes the score by comparing two lists of predictions. The result is a dictionary containing the 'exact_match' score, which is a float between 0.0 and 1.0.

SOURCE: https://github.com/huggingface/evaluate/blob/main/comparisons/exact_match/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> exact_match = evaluate.load("exact_match", module_type="comparison")
>>> results = exact_match.compute(predictions1=[0, 1, 1], predictions2=[1, 1, 1])
>>> print(results)
{'exact_match': 0.66}
```

LANGUAGE: python
CODE:
```
>>> exact_match = evaluate.load("exact_match", module_type="comparison")
>>> results = exact_match.compute(predictions1=[0, 0, 0], predictions2=[1, 1, 1])
>>> print(results)
{'exact_match': 1.0}
```

--------------------------------

TITLE: Compute Text Duplicates
DESCRIPTION: Calculates the fraction of duplicate strings in a given list. It takes a list of strings as input and returns a dictionary containing the duplicate fraction.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/text_duplicates/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> data = ["hello sun","hello moon", "hello sun"]
>>> duplicates = evaluate.load("text_duplicates")
>>> results = duplicates.compute(data=data)
```

--------------------------------

TITLE: Compute MASE with Raw Values for Multi-output
DESCRIPTION: Computes the MASE metric for multi-dimensional lists with the 'raw_values' configuration. This returns the MASE score for each output dimension separately.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mase/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> mase_metric = evaluate.load("mase", "multilist")
>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]
>>> references = [[0.1, 2], [-1, 2], [8, -5]]
>>> training = [[0.5, 1], [-1, 1], [7, -6]]
>>> results = mase_metric.compute(predictions=predictions, references=references, training=training)
>>> print(results)
{'mase': 0.1818...}
>>> results = mase_metric.compute(predictions=predictions, references=references, training=training, multioutput='raw_values')
>>> print(results)
{'mase': array([0.1052..., 0.2857...])}
```

--------------------------------

TITLE: Compute MASE with Uniform Average
DESCRIPTION: Computes the MASE metric using the 'uniform_average' strategy for multi-output predictions. This method averages the errors across all outputs with equal weight.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mase/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> mase_metric = evaluate.load("mase")
>>> predictions = [2.5, 0.0, 2, 8]
>>> references = [3, -0.5, 2, 7]
>>> training = [5, 0.5, 4, 6, 3, 5, 2]
>>> results = mase_metric.compute(predictions=predictions, references=references, training=training)
>>> print(results)
{'mase': 0.1833...}
```

--------------------------------

TITLE: Compute Exact Match Score
DESCRIPTION: Computes the exact match score between a list of predictions and references. This is the core functionality of the metric.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/exact_match/README.md#_snippet_1

LANGUAGE: python
CODE:
```
results = exact_match_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Compute Toxicity with Custom Model and Toxic Label
DESCRIPTION: Computes toxicity scores using a specified custom model and a particular toxic label. The 'toxic_label' argument should correspond to a label defined in the model's configuration (e.g., 'offensive').

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/toxicity/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
toxicity = evaluate.load("toxicity", 'DaNLP/da-electra-hatespeech-detection')
input_texts = ["she went to the library", "he is a douchebag"]
results = toxicity.compute(predictions=input_texts, toxic_label='offensive')
print([round(s, 4) for s in results["toxicity"]])
```

--------------------------------

TITLE: Citation for Generalizing ROC Curve for Multiple Classes (BibTeX)
DESCRIPTION: BibTeX entry for the paper 'A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems' by David J. Hand and Robert J. Till, published in Machine Learning.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_8

LANGUAGE: bibtex
CODE:
```
@article{10.1023/A:1010920819831,
author = {Hand, David J. and Till, Robert J.},
title = {A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems},
year = {2001},
issue_date = {November 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {2},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010920819831},
doi = {10.1023/A:1010920819831},
journal = {Mach. Learn.},
month = {oct},
pages = {171â€“186},
numpages = {16},
keywords = {Gini index, AUC, error rate, ROC curve, receiver operating characteristic}
}
```

--------------------------------

TITLE: Evaluate: Text2TextGenerationEvaluator
DESCRIPTION: This snippet represents the Text2TextGenerationEvaluator from the Hugging Face evaluate library, used for evaluating text-to-text generation models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_7

LANGUAGE: python
CODE:
```
from evaluate import Text2TextGenerationEvaluator

# Example usage (conceptual)
text2text_evaluator = Text2TextGenerationEvaluator()
# results = text2text_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Citation: Woodard (1982)
DESCRIPTION: BibTeX entry for the paper 'An information theoretic measure of speech recognition performance' by J.P. Woodard and J.T. Nelson, presented at the Workshop on standardisation for speech I/O technology in 1982.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/wer/README.md#_snippet_5

LANGUAGE: bibtex
CODE:
```
@inproceedings{woodard1982,
author = {Woodard, J.P. and Nelson, J.T.},
year = {1982},
journal = {Workshop on standardisation for speech I/O technology, Naval Air Development Center, Warminster, PA},
title = {An information theoretic measure of speech recognition performance}
}
```

--------------------------------

TITLE: Hugging Face Evaluate: TER Ignoring Punctuation and Case
DESCRIPTION: Calculates TER while ignoring punctuation and case sensitivity. This provides a more lenient comparison. Requires the 'evaluate' library. Outputs score, edits, and reference length.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/ter/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> predictions = ["does this sentence match??",
...                     "what about this sentence?"]
>>> references = [["does this sentence match", "does this sentence match!?!"],
...             ["wHaT aBoUt ThIs SeNtEnCe?", "wHaT aBoUt ThIs SeNtEnCe?"]]
>>> ter = evaluate.load("ter")
>>> results = ter.compute(predictions=predictions, 
...                         references=references, 
...                         ignore_punct=True,
...                         case_sensitive=False)
>>> print(results)
{'score': 0.0, 'num_edits': 0, 'ref_length': 8.0}
```

--------------------------------

TITLE: Regard Scores with Single Input
DESCRIPTION: Illustrates the output format of the 'regard' measurement when provided with a single list of sentences. It shows the polarity scores for 'neutral', 'positive', 'negative', and 'other' categories.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_1

LANGUAGE: python
CODE:
```
{'neutral': 0.95, 'positive': 0.02, 'negative': 0.02, 'other': 0.01}
{'negative': 0.97, 'other': 0.02, 'neutral': 0.01, 'positive': 0.0}
```

--------------------------------

TITLE: Evaluate: TextGenerationEvaluator
DESCRIPTION: This snippet represents the TextGenerationEvaluator from the Hugging Face evaluate library, used for evaluating text generation models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_6

LANGUAGE: python
CODE:
```
from evaluate import TextGenerationEvaluator

# Example usage (conceptual)
textgen_evaluator = TextGenerationEvaluator()
# results = textgen_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Average Regard Scores with Two Inputs
DESCRIPTION: Demonstrates the output when using `aggregation='average'` to compare two lists of sentences. It provides the average regard score for each category across both groups.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_6

LANGUAGE: python
CODE:
```
{'neutral': 0.37, 'negative': 0.57, 'other': 0.05, 'positive': 0.01}
```

--------------------------------

TITLE: Mean Absolute Percentage Error Citation
DESCRIPTION: BibTeX entry for the paper 'Mean Absolute Percentage Error for regression models' by de Myttenaere et al., providing a reference for the MAPE metric's application in regression.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/mape/README.md#_snippet_4

LANGUAGE: bibtex
CODE:
```
@article{DEMYTTENAERE201638,
    title = {Mean Absolute Percentage Error for regression models},
    journal = {Neurocomputing},
    volume = {192},
    pages = {38--48},
    year = {2016},
    note = {Advances in artificial neural networks, machine learning and computational intelligence},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2015.12.114},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231216003325},
    author = {Arnaud {de Myttenaere} and Boris Golden and BÃ©nÃ©dicte {Le Grand} and Fabrice Rossi},
}
```

--------------------------------

TITLE: BLEU Citation
DESCRIPTION: Provides the BibTeX citation for the original BLEU paper by Papineni et al. (2002), which introduced the BLEU metric for automatic evaluation of machine translation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleu/README.md#_snippet_4

LANGUAGE: BibTeX
CODE:
```
@INPROCEEDINGS{Papineni02bleu:
    author = {Kishore Papineni and Salim Roukos and Todd Ward and Wei-jing Zhu},
    title = {BLEU: a Method for Automatic Evaluation of Machine Translation},
    booktitle = {},
    year = {2002},
    pages = {311--318}
}
@inproceedings{lin-och-2004-orange,
    title = {"ORANGE": a Method for Evaluating Automatic Evaluation Metrics for Machine Translation},
    author = "Lin, Chin-Yew  and
      Och, Franz Josef",
    booktitle = {COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics},
    month = "aug 23--aug 27",
    year = "2004",
    address = {Geneva, Switzerland},
    publisher = {COLING},
    url = {https://www.acl.org/anthology/C04-1072},
    pages = {501--507},
}
```

--------------------------------

TITLE: Hugging Face Evaluate: TER with Incorrect Sample (Ignoring Punctuation/Case)
DESCRIPTION: Demonstrates TER calculation with an extra, incorrect sample, ignoring punctuation and case. Useful for testing robustness. Requires the 'evaluate' library. Outputs score, edits, and reference length.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/ter/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> predictions = ["does this sentence match??",
...                    "what about this sentence?",
...                    "What did the TER metric user say to the developer?"]
>>> references = [["does this sentence match", "does this sentence match!?!"],
...             ["wHaT aBoUt ThIs SeNtEnCe?", "wHaT aBoUt ThIs SeNtEnCe?"],
...             ["Your jokes are...", "...TERrible"]]
>>> ter = evaluate.load("ter")
>>> results = ter.compute(predictions=predictions, 
...                         references=references,
...                         ignore_punct=True,
...                         case_sensitive=False)
>>> print(results)
{'score': 100.0, 'num_edits': 10, 'ref_length': 10.0}
```

--------------------------------

TITLE: Evaluate: AutomaticSpeechRecognitionEvaluator
DESCRIPTION: This snippet represents the AutomaticSpeechRecognitionEvaluator from the Hugging Face evaluate library, used for evaluating ASR models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_10

LANGUAGE: python
CODE:
```
from evaluate import AutomaticSpeechRecognitionEvaluator

# Example usage (conceptual)
asr_evaluator = AutomaticSpeechRecognitionEvaluator()
# results = asr_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Evaluate: QuestionAnsweringEvaluator
DESCRIPTION: This snippet represents the QuestionAnsweringEvaluator from the Hugging Face evaluate library, used for evaluating question answering models. It includes the 'compute' method for performing evaluations.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_3

LANGUAGE: python
CODE:
```
from evaluate import QuestionAnsweringEvaluator

# Example usage (conceptual)
qa_evaluator = QuestionAnsweringEvaluator()
# results = qa_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: XNLI Citation
DESCRIPTION: The BibTeX entry for citing the original XNLI paper. This is essential for academic use and proper attribution.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xnli/README.md#_snippet_4

LANGUAGE: BibTeX
CODE:
```
@InProceedings{conneau2018xnli,
  author = "Conneau, Alexis
                 and Rinott, Ruty
                 and Lample, Guillaume
                 and Williams, Adina
                 and Bowman, Samuel R.
                 and Schwenk, Holger
                 and Stoyanov, Veselin",
  title = "XNLI: Evaluating Cross-lingual Sentence Representations",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods
               in Natural Language Processing",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  location = "Brussels, Belgium",
}
```

--------------------------------

TITLE: Python: Compute Precision (Binary with sample_weight)
DESCRIPTION: Calculates precision for a binary classification task, incorporating sample weights. This allows certain samples to have a greater influence on the metric calculation. The `sample_weight` parameter is used for this purpose.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/precision/README.md#_snippet_3

LANGUAGE: python
CODE:
```
>>> precision_metric = evaluate.load("precision")
>>> results = precision_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])
>>> print(results)
{'precision': 0.23529411764705882}
```

--------------------------------

TITLE: Scikit-Learn: Compute Metrics with Evaluator
DESCRIPTION: Uses the Hugging Face evaluator to compute the accuracy metric for a Scikit-Learn pipeline on the IMDb test set.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
from evaluate import evaluator

task_evaluator = evaluator("text-classification")
task_evaluator.compute(pipe, ds["test"], "accuracy")

>>> {'accuracy': 0.82956}
```

--------------------------------

TITLE: Spacy: Create Evaluator-Compatible Wrapper
DESCRIPTION: Wraps a Spacy NLP pipeline to be compatible with the Hugging Face evaluator. The wrapper classifies sentiment as positive (1) if polarity is non-negative, and negative (0) otherwise.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/custom_evaluator.mdx#_snippet_7

LANGUAGE: Python
CODE:
```
class SpacyEvalPipeline:
    def __init__(self, nlp):
        self.nlp = nlp
        self.task = "text-classification"

    def __call__(self, input_texts, **kwargs):
        results =[]
        for p in self.nlp.pipe(input_texts):
            if p._.blob.polarity>=0:
                results.append({"label": 1})
            else:
                results.append({"label": 0})
        return results

pipe = SpacyEvalPipeline(nlp)
```

--------------------------------

TITLE: Create Custom Keras Callback for Metrics
DESCRIPTION: Implements a custom Keras callback to compute specified metrics during model training. The callback calculates the metric after each epoch using predictions and references from the provided data.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/keras_integrations.md#_snippet_1

LANGUAGE: python
CODE:
```
class MetricsCallback(keras.callbacks.Callback):

    def __init__(self, metric_name, x_data, y_data) -> None:
        super(MetricsCallback, self).__init__()

        self.x_data = x_data
        self.y_data = y_data
        self.metric_name = metric_name
        self.metric = evaluate.load(metric_name)

    def on_epoch_end(self, epoch, logs=dict()):
        m = self.model 
        # Ensure we get labels of "1" or "0"
        training_preds = np.round(m.predict(self.x_data))
        training_labels = self.y_data

        # Compute score and save
        score = self.metric.compute(predictions = training_preds, references = training_labels)
        
        logs.update(score)

```

--------------------------------

TITLE: Calculate CUAD Metric (Partial Match)
DESCRIPTION: This Python code snippet demonstrates a partial match scenario when using the CUAD metric. It shows how the metric calculates scores when predictions partially overlap with references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/cuad/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from evaluate import load
cuad_metric = load("cuad")
predictions = [{'prediction_text': ['The seller:', 'The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd.'], 'id': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement__Parties'}]
predictions = [{'prediction_text': ['The Company appoints the Distributor as an exclusive distributor of Products in the Market, subject to the terms and conditions of this Agreement.', 'The buyer/End-User: Shenzhen LOHAS Supply Chain Management Co., Ltd.'], 'id': 'LohaCompanyltd_20191209_F-1_EX-10.16_11917878_EX-10.16_Supply Agreement__Parties'}]
results = cuad_metric.compute(predictions=predictions, references=references)
print(results)
```

--------------------------------

TITLE: Evaluate: ImageClassificationEvaluator
DESCRIPTION: This snippet represents the ImageClassificationEvaluator from the Hugging Face evaluate library, designed for evaluating image classification models.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import ImageClassificationEvaluator

# Example usage (conceptual)
image_evaluator = ImageClassificationEvaluator()
```

--------------------------------

TITLE: Compute NIST MT Score
DESCRIPTION: This Python code snippet demonstrates how to load and use the NIST MT metric from the Hugging Face evaluate library. It takes a hypothesis and a list of references as input and returns the NIST score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/nist_mt/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
import evaluate
nist_mt = evaluate.load("nist_mt")
hypothesis1 = "It is a guide to action which ensures that the military always obeys the commands of the party"
reference1 = "It is a guide to action that ensures that the military will forever heed Party commands"
reference2 = "It is the guiding principle which guarantees the military forces always being under the command of the Party"
nist_mt.compute(hypothesis1, [reference1, reference2])
# {'nist_mt': 3.3709935957649324}
```

--------------------------------

TITLE: Compute METEOR with One Reference per Prediction
DESCRIPTION: This snippet shows how to compute the METEOR score when there is a single reference translation for each prediction. It loads the 'meteor' metric, defines predictions and a single reference, computes the score, and prints the rounded result.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/meteor/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> meteor = evaluate.load('meteor')
>>> predictions = ["It is a guide to action which ensures that the military always obeys the commands of the party"]
>>> reference = ["It is a guide to action which ensures that the military always obeys the commands of the party"]
>>> results = meteor.compute(predictions=predictions, references=reference)
>>> print(round(results['meteor'], 2))
1.0
```

--------------------------------

TITLE: Compute Maximum Toxicity Score
DESCRIPTION: Determines the maximum toxicity score among a list of input texts. This is achieved by setting the 'aggregation' parameter to 'maximum'.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/toxicity/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
toxicity = evaluate.load("toxicity", module_type="measurement")
input_texts = ["she went to the library", "he is a douchebag"]
results = toxicity.compute(predictions=input_texts, aggregation="maximum")
print(round(results['max_toxicity'], 4))
```

--------------------------------

TITLE: Evaluate: Base Evaluator Class
DESCRIPTION: This snippet represents the base class for all evaluator classes in the Hugging Face evaluate library. It provides a foundational structure and common methods for different evaluation tasks.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import Evaluator

# Example usage (conceptual, inheriting from the base class)
class CustomEvaluator(Evaluator):
    pass
```

--------------------------------

TITLE: Evaluate Keras Model Accuracy on Test Set
DESCRIPTION: Loads the 'accuracy' metric from Hugging Face Evaluate and computes the model's accuracy on the preprocessed test dataset after training. Predictions are rounded to binary labels.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/keras_integrations.md#_snippet_3

LANGUAGE: python
CODE:
```
acc = evaluate.load("accuracy")
# Round the predictions to turn them into "0" or "1" labels
test_preds = np.round(model.predict(x_test))
test_labels = y_test

```

LANGUAGE: python
CODE:
```
print("Test accuracy is : ", acc.compute(predictions = test_preds, references = test_labels))
# Test accuracy is : 0.9855

```

--------------------------------

TITLE: Evaluate: TokenClassificationEvaluator
DESCRIPTION: This snippet represents the TokenClassificationEvaluator from the Hugging Face evaluate library, used for evaluating token classification models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_5

LANGUAGE: python
CODE:
```
from evaluate import TokenClassificationEvaluator

# Example usage (conceptual)
token_evaluator = TokenClassificationEvaluator()
# results = token_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Compute ROC AUC Score (Binary)
DESCRIPTION: Computes the ROC AUC score for binary classification. Requires reference labels and prediction scores. The default implementation is binary.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> roc_auc_score = evaluate.load("roc_auc")
>>> refs = [1, 0, 1, 1, 0, 0]
>>> pred_scores = [0.5, 0.2, 0.99, 0.3, 0.1, 0.7]
>>> results = roc_auc_score.compute(references=refs, prediction_scores=pred_scores)
>>> print(round(results['roc_auc'], 2))
0.78
```

--------------------------------

TITLE: METEOR Citation
DESCRIPTION: This entry provides the BibTeX citation for the METEOR paper, which is useful for academic referencing. It includes the title, authors, publication details, and URL.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/meteor/README.md#_snippet_4

LANGUAGE: bibtex
CODE:
```
@inproceedings{banarjee2005,
  title     = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},
  author    = {Banerjee, Satanjeev  and Lavie, Alon},
  booktitle = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
  month     = jun,
  year      = {2005},
  address   = {Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W05-0909},
  pages     = {65--72},
}
```

--------------------------------

TITLE: BERTScore Citation
DESCRIPTION: BibTeX entry for the BERTScore paper, providing citation details for academic use.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bertscore/README.md#_snippet_3

LANGUAGE: bibtex
CODE:
```
@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}
```

--------------------------------

TITLE: Evaluate: SummarizationEvaluator
DESCRIPTION: This snippet represents the SummarizationEvaluator from the Hugging Face evaluate library, used for evaluating summarization models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_8

LANGUAGE: python
CODE:
```
from evaluate import SummarizationEvaluator

# Example usage (conceptual)
summary_evaluator = SummarizationEvaluator()
# results = summary_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Calculate chrF++ Score with Lowercase Normalization
DESCRIPTION: Illustrates calculating chrF++ scores with case normalization enabled by setting 'lowercase=True'. This ensures that case differences do not affect the score.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/chrf/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> prediction = ["The relationship between cats and dogs is not exactly friendly.", "a good bookshop is just a genteel black hole that knows how to read."]
>>> reference = [["The relationship between dogs and cats is not exactly friendly.", ], ["A good bookshop is just a genteel Black Hole that knows how to read."]]
>>> chrf = evaluate.load("chrf")
>>> results = chrf.compute(predictions=prediction,
...                         references=reference,
...                         word_order=2,
...                         lowercase=True)
>>> print(results)
{'score': 92.12853119829202, 'char_order': 6, 'word_order': 2, 'beta': 2}
```

--------------------------------

TITLE: Citation for Analyzing a Portion of the ROC Curve (BibTeX)
DESCRIPTION: BibTeX entry for the paper 'Analyzing a Portion of the ROC Curve' by Donna Katzman McClish, published in Medical Decision Making.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_7

LANGUAGE: bibtex
CODE:
```
@article{doi:10.1177/0272989X8900900307,
author = {Donna Katzman McClish},
title ={Analyzing a Portion of the ROC Curve},
journal = {Medical Decision Making},
volume = {9},
number = {3},
pages = {190-195},
year = {1989},
doi = {10.1177/0272989X8900900307},
    note ={PMID: 2668680},
URL = {https://doi.org/10.1177/0272989X8900900307},
eprint = {https://doi.org/10.1177/0272989X8900900307}
}
```

--------------------------------

TITLE: Regard Scores Difference with Two Inputs
DESCRIPTION: Illustrates the default output when comparing two lists of sentences. The scores represent the difference in regard between the two groups for each category.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_4

LANGUAGE: python
CODE:
```
{'neutral': 0.35, 'negative': -0.36, 'other': 0.01, 'positive': 0.01}
```

--------------------------------

TITLE: Google BLEU Score Symmetry
DESCRIPTION: Illustrates the symmetrical nature of the Google BLEU score, showing that swapping the predicted and reference sentences yields the same score. This is demonstrated by comparing two computation results where the inputs are reversed.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/google_bleu/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
import evaluate

predictions = "the cat sat on the mat"
references = "the cat ate the mat"
google_bleu = evaluate.load("google_bleu")
result_a = google_bleu.compute(predictions=[predictions], references=[[references]])
result_b = google_bleu.compute(predictions=[references], references=[[predictions]])
print(result_a == result_b)
>>> True
```

--------------------------------

TITLE: Evaluate: TextClassificationEvaluator
DESCRIPTION: This snippet represents the TextClassificationEvaluator from the Hugging Face evaluate library, designed for evaluating text classification models.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_4

LANGUAGE: python
CODE:
```
from evaluate import TextClassificationEvaluator

# Example usage (conceptual)
text_evaluator = TextClassificationEvaluator()
```

--------------------------------

TITLE: Compute Matthews Correlation Coefficient with Negative Correlation
DESCRIPTION: Illustrates a scenario where sample weights lead to a negative Matthews Correlation Coefficient. This highlights the metric's ability to capture inverse predictions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/matthews_correlation/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> matthews_metric = evaluate.load("matthews_correlation")
>>> results = matthews_metric.compute(references=[1, 3, 2, 0, 3, 2],
...                                     predictions=[1, 2, 2, 0, 3, 3],
...                                     sample_weight=[0.5, 1, 0, 0, 0, 1])
>>> print(results)
{'matthews_correlation': -0.25}
```

--------------------------------

TITLE: BLEU Calculation with Single Reference
DESCRIPTION: Calculates the BLEU score for predictions against a single reference translation. It takes lists of predictions and references as input and returns the BLEU score along with other metrics like precisions and brevity penalty.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bleu/README.md#_snippet_1

LANGUAGE: Python
CODE:
```
>>> predictions = ["hello there general kenobi","foo bar foobar"]
>>> references = [
...     ["hello there general kenobi"],
...     ["foo bar foobar"]
... ]
>>> bleu = evaluate.load("bleu")
>>> results = bleu.compute(predictions=predictions, references=references)
>>> print(results)
{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 7, 'reference_length': 7}
```

--------------------------------

TITLE: Evaluate: TranslationEvaluator
DESCRIPTION: This snippet represents the TranslationEvaluator from the Hugging Face evaluate library, used for evaluating translation models. It includes the 'compute' method.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/package_reference/evaluator_classes.mdx#_snippet_9

LANGUAGE: python
CODE:
```
from evaluate import TranslationEvaluator

# Example usage (conceptual)
translation_evaluator = TranslationEvaluator()
# results = translation_evaluator.compute(predictions=..., references=...)
```

--------------------------------

TITLE: Compute XTREME-S Metric
DESCRIPTION: Calculates the XTREME-S metric using predictions and references. The metric's output varies based on the chosen subset, potentially including accuracy, F1, WER, CER, or BLEU scores.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_1

LANGUAGE: python
CODE:
```
>>> references = ["it is sunny here", "paper and pen are essentials"]
>>> predictions = ["it's sunny", "paper pen are essential"]
>>> results = xtreme_s_metric.compute(predictions=predictions, references=references)
```

--------------------------------

TITLE: Compute HONEST Score (With Groups)
DESCRIPTION: Computes the HONEST score for different identity groups. This allows for analyzing score variations across specified groups, such as male and female.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/honest/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> honest = evaluate.load('honest', 'en')
>>> completions = [['CEO', 'businessman', 'father'], ['good', 'excellent', 'smart'], ['secretary', 'prostitute', 'maid'], ['beautiful', 'smart', 'tall']]
>>> groups = ["male", "male", "female", "female"]
>>> result = honest.compute(predictions=completions, groups = groups)
>>> {g:round(s,3) for g,s in result['honest_score_per_group'].items()}
{'male': 0.0, 'female': 0.167}
```

--------------------------------

TITLE: Maximum Regard Scores with Two Inputs
DESCRIPTION: Shows the output when using `aggregation='maximum'` to compare two lists of sentences. It provides the maximum regard score for each demographic category within each group.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_5

LANGUAGE: python
CODE:
```
{'negative': 0.98, 'other': 0.04, 'neutral': 0.03, 'positive': 0.0}
```

--------------------------------

TITLE: Python: Compute Precision (Binary with pos_label=0)
DESCRIPTION: Computes precision for a binary classification task, specifically setting the positive label to '0'. This is useful when the minority class is labeled as '0'. The result is rounded to two decimal places.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/precision/README.md#_snippet_2

LANGUAGE: python
CODE:
```
>>> precision_metric = evaluate.load("precision")
>>> results = precision_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)
>>> print(round(results['precision'], 2))
0.67
```

--------------------------------

TITLE: Maximum Regard Scores with Single Input
DESCRIPTION: Shows the output when the 'maximum' aggregation option is used with a single input list for the 'regard' measurement. It provides the highest regard score across all input strings for each category.

SOURCE: https://github.com/huggingface/evaluate/blob/main/measurements/regard/README.md#_snippet_3

LANGUAGE: python
CODE:
```
{'neutral': 0.95, 'positive': 0.024, 'negative': 0.972, 'other': 0.019}
```

--------------------------------

TITLE: XTREME-S MLS Subset: WER and CER Calculation
DESCRIPTION: Calculates Word Error Rate (WER) and Character Error Rate (CER) using the 'mls' subset of the XTREME-S metric. Requires predictions and references as lists of strings.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
>>> xtreme_s_metric = evaluate.load('xtreme_s', 'mls')
>>> references = ["it is sunny here", "paper and pen are essentials"]
>>> predictions = ["it's sunny", "paper pen are essential"]
>>> results = xtreme_s_metric.compute(predictions=predictions, references=references)
>>> print({k: round(v, 2) for k, v in results.items()})
{'wer': 0.56, 'cer': 0.27}
```

--------------------------------

TITLE: Calculate BERTScore with Partial Match
DESCRIPTION: This snippet shows how to calculate BERTScore with partial matches between predictions and references using the 'bertscore' metric and the 'distilbert-base-uncased' model.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bertscore/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from evaluate import load
bertscore = load("bertscore")
predictions = ["hello world", "general kenobi"]
references = ["goodnight moon", "the sun is shining"]
results = bertscore.compute(predictions=predictions, references=references, model_type="distilbert-base-uncased")
print(results)
```

--------------------------------

TITLE: Compute TER Score
DESCRIPTION: Calculates the TER score, number of edits, and average reference length for given predictions and references. It supports various normalization and case sensitivity options. The input format requires predictions and references to be lists of strings, with references being a list of lists to accommodate multiple references per prediction.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/ter/README.md#_snippet_0

LANGUAGE: python
CODE:
```
>>> predictions = ["does this sentence match??",
...                     "what about this sentence?",
...                     "What did the TER metric user say to the developer?"]
>>> references = [["does this sentence match", "does this sentence match!?!"],
...             ["wHaT aBoUt ThIs SeNtEnCe?", "wHaT aBoUt ThIs SeNtEnCe?"],
...             ["Your jokes are...", "...TERrible"]]
>>> ter = evaluate.load("ter")
>>> results = ter.compute(predictions=predictions,
...                         references=references,
...                         case_sensitive=True)
>>> print(results)
{'score': 150.0, 'num_edits': 15, 'ref_length': 10.0}
```

--------------------------------

TITLE: Calculate BERTScore with Maximal Values
DESCRIPTION: This snippet demonstrates how to compute BERTScore using the 'bertscore' metric loaded from the evaluate library. It uses the 'distilbert-base-uncased' model for exact matching between predictions and references.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/bertscore/README.md#_snippet_1

LANGUAGE: python
CODE:
```
from evaluate import load
bertscore = load("bertscore")
predictions = ["hello world", "general kenobi"]
references = ["hello world", "general kenobi"]
results = bertscore.compute(predictions=predictions, references=references, model_type="distilbert-base-uncased")
print(results)
```

--------------------------------

TITLE: XTREME-S Minds14 Subset: F1 and Accuracy Calculation
DESCRIPTION: Calculates F1 score and accuracy using the 'minds14' subset of the XTREME-S metric. This is suitable for classification tasks. Requires predictions and references as lists of integers.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_5

LANGUAGE: Python
CODE:
```
>>> xtreme_s_metric = evaluate.load('xtreme_s', 'minds14')
>>> references = [0, 1, 0, 0, 1]
>>> predictions = [0, 1, 1, 0, 0]
>>> results = xtreme_s_metric.compute(predictions=predictions, references=references)
>>> print({k: round(v, 2) for k, v in results.items()})
{'f1': 0.58, 'accuracy': 0.6}
```

--------------------------------

TITLE: XTREME-S Covost2 Subset: BLEU Score Calculation
DESCRIPTION: Calculates the BLEU score using the 'covost2' subset of the XTREME-S metric. This is suitable for evaluating machine translation or speech recognition tasks. Requires predictions and references as lists of strings.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
>>> xtreme_s_metric = evaluate.load('xtreme_s', 'covost2')
>>> references = ["bonjour paris", "il est necessaire de faire du sport de temps en temp"]
>>> predictions = ["bonjour paris", "il est important de faire du sport souvent"]
>>> results = xtreme_s_metric.compute(predictions=predictions, references=references)
>>> print({k: round(v, 2) for k, v in results.items()})
{'bleu': 31.65}
```

--------------------------------

TITLE: XTREME-S Fleurs-Lang ID Subset: Accuracy Calculation
DESCRIPTION: Calculates accuracy using the 'fleurs-lang_id' subset of the XTREME-S metric. This is used for language identification tasks. Requires predictions and references as lists of integers.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/xtreme_s/README.md#_snippet_4

LANGUAGE: Python
CODE:
```
>>> xtreme_s_metric = evaluate.load('xtreme_s', 'fleurs-lang_id')
>>> references = [0, 1, 0, 0, 1]
>>> predictions = [0, 1, 1, 0, 0]
>>> results = xtreme_s_metric.compute(predictions=predictions, references=references)
>>> print({k: round(v, 2) for k, v in results.items()})
{'accuracy': 0.6}
```

--------------------------------

TITLE: Exact Match Metric Calculation
DESCRIPTION: Computes the Exact Match score, which is the rate at which predicted strings precisely match their corresponding references. This is commonly used in tasks like question answering or text generation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/types_of_evaluations.mdx#_snippet_1

LANGUAGE: Python
CODE:
```
from evaluate import load

exact_match = load("exact_match")
results = exact_match.compute(predictions=["the cat sat on the mat", "hello world"], references=["the cat sat on the mat", "hello world"])
print(results)
```

--------------------------------

TITLE: McNemar Test for Model Comparison
DESCRIPTION: Performs McNemar's test to compare the predictions of two models. This statistical test helps determine if the models' predictions significantly diverge, indicated by a p-value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/types_of_evaluations.mdx#_snippet_3

LANGUAGE: Python
CODE:
```
from evaluate import load

mcnemar = load("mcnemar")
results = mcnemar.compute(references=[0, 1, 0, 1, 0], predictions=[[0, 1], [1, 0], [0, 0], [1, 1], [0, 0]])
print(results)
```

--------------------------------

TITLE: Compute ROC AUC Score for Binary Classification (Python)
DESCRIPTION: Calculates the ROC AUC score for a binary classification problem. It takes references (true labels) and prediction scores as input. The result is a single ROC AUC value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_4

LANGUAGE: python
CODE:
```
>>> roc_auc_score = evaluate.load("roc_auc")
>>> refs = [1, 0, 1, 1, 0, 0]
>>> pred_scores = [0.5, 0.2, 0.99, 0.3, 0.1, 0.7]
>>> results = roc_auc_score.compute(references=refs, prediction_scores=pred_scores)
>>> print(round(results['roc_auc'], 2))
0.78
```

--------------------------------

TITLE: ROC AUC Score Calculation (Python)
DESCRIPTION: Calculates the Receiver Operating Characteristic Area Under the Curve (ROC AUC) score. This metric supports binary, multiclass, and multilabel classification tasks. It accepts ground truth labels and prediction scores, with options for different averaging methods and partial AUC computation.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_3

LANGUAGE: Python
CODE:
```
from evaluate import load

# Load the ROC AUC metric
roc_auc = load("roc_auc")

# Example for binary classification
references_binary = [0, 1, 0, 1, 0]
prediction_scores_binary = [0.1, 0.8, 0.3, 0.9, 0.2]
results_binary = roc_auc.compute(references=references_binary, prediction_scores=prediction_scores_binary)
print(f"Binary ROC AUC: {results_binary}")

# Example for multiclass classification (using 'ovr' strategy)
references_multiclass = [0, 1, 2, 0, 1]
# Probabilities for each class (must sum to 1)
prediction_scores_multiclass = [[0.1, 0.8, 0.1], [0.2, 0.3, 0.5], [0.9, 0.05, 0.05], [0.7, 0.2, 0.1], [0.1, 0.7, 0.2]]
results_multiclass = roc_auc.compute(references=references_multiclass, prediction_scores=prediction_scores_multiclass, multi_class="ovr", average="macro")
print(f"Multiclass ROC AUC (ovr, macro): {results_multiclass}")

# Example for multilabel classification with no averaging
references_multilabel = [[0, 1, 1], [1, 0, 0], [0, 0, 1]]
prediction_scores_multilabel = [[0.1, 0.8, 0.7], [0.9, 0.2, 0.1], [0.3, 0.4, 0.6]]
results_multilabel = roc_auc.compute(references=references_multilabel, prediction_scores=prediction_scores_multilabel, average=None)
print(f"Multilabel ROC AUC (no average): {results_multilabel}")
```

--------------------------------

TITLE: Compute ROC AUC Score for Multiclass Classification (Python)
DESCRIPTION: Computes the ROC AUC score for a multiclass classification problem using the 'one-vs-rest' (ovr) strategy. It requires references and prediction scores for each class. The output is a single ROC AUC value.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_5

LANGUAGE: python
CODE:
```
>>> roc_auc_score = evaluate.load("roc_auc", "multiclass")
>>> refs = [1, 0, 1, 2, 2, 0]
>>> pred_scores = [[0.3, 0.5, 0.2],
...                 [0.7, 0.2, 0.1],
...                 [0.005, 0.99, 0.005],
...                 [0.2, 0.3, 0.5],
...                 [0.1, 0.1, 0.8],
...                 [0.1, 0.7, 0.2]]
>>> results = roc_auc_score.compute(references=refs,
...                                 prediction_scores=pred_scores,
...                                 multi_class='ovr')
>>> print(round(results['roc_auc'], 2))
0.85
```

--------------------------------

TITLE: Perplexity Measurement for Text Generation
DESCRIPTION: Measures the perplexity of generated text, often used with models like GPT-2 or BERT when no ground truth references are available. Lower perplexity generally indicates higher quality generated text.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/types_of_evaluations.mdx#_snippet_4

LANGUAGE: Python
CODE:
```
from evaluate import load

perplexity = load("perplexity", module_type="metric", model_id="gpt2-small")
results = perplexity.compute(model_id="gpt2-small", predictions=["hello world", "the cat sat on the mat"])
print(results)
```

--------------------------------

TITLE: Accuracy Metric Calculation
DESCRIPTION: Calculates the accuracy of a model's predictions by comparing them to ground truth references. This metric is useful for classification tasks and measures the proportion of correct predictions.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/types_of_evaluations.mdx#_snippet_0

LANGUAGE: Python
CODE:
```
from evaluate import load

accuracy = load("accuracy")
results = accuracy.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 2, 1, 0, 0, 1])
print(results)
```

--------------------------------

TITLE: Compute ROC AUC Score for Multilabel Classification (Python)
DESCRIPTION: Calculates the ROC AUC score for a multilabel classification problem. It computes the score for each label independently. The function takes references and prediction scores for each label and returns an array of ROC AUC values, one for each label.

SOURCE: https://github.com/huggingface/evaluate/blob/main/metrics/roc_auc/README.md#_snippet_6

LANGUAGE: python
CODE:
```
>>> roc_auc_score = evaluate.load("roc_auc", "multilabel")
>>> refs = [[1, 1, 0],
...         [1, 1, 0],
...         [0, 1, 0],
...         [0, 0, 1],
...         [0, 1, 1],
...         [1, 0, 1]]
>>> pred_scores = [[0.3, 0.5, 0.2],
...                 [0.7, 0.2, 0.1],
...                 [0.005, 0.99, 0.005],
...                 [0.2, 0.3, 0.5],
...                 [0.1, 0.1, 0.8],
...                 [0.1, 0.7, 0.2]]
>>> results = roc_auc_score.compute(references=refs,
...                                 prediction_scores=pred_scores,
...                                 average=None)
>>> print([round(res, 2) for res in results['roc_auc']])
[0.83, 0.38, 0.94]
```

--------------------------------

TITLE: Mean Intersection over Union (IoU) Metric Calculation
DESCRIPTION: Calculates the Mean Intersection over Union (IoU) for image segmentation tasks. It measures the overlap between predicted and ground truth segmentations, divided by their union.

SOURCE: https://github.com/huggingface/evaluate/blob/main/docs/source/types_of_evaluations.mdx#_snippet_2

LANGUAGE: Python
CODE:
```
from evaluate import load

mean_iou = load("mean_iou")
results = mean_iou.compute(references=[[1, 1], [2, 2]], predictions=[[1, 1], [1, 1]])
print(results)
```