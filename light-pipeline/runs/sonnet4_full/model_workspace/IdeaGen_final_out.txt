Token-Budget Auctions for Multi-Agent Question Answering
Hypothesis: Auctions improve accuracy per token cost.

MVP workflow
1. Select 500 open-domain questions with a strict per-experiment token budget.
2. Inputs: questions, simple web search, three diverse LLM agents.
3. Method: second-price auctions allocate tokens by self-estimated marginal value.
4. Output: answers with calibrated confidence, token cost, and source citations.
5. Baseline: strongest single LLM with equal tokens; compare accuracy-per-1k-tokens.
6. Deliverable: publish bids, allocations, prompts, outputs, scores, and scoring code.

Terms
Second-price auction: highest bid wins, pays second-highest bid.
Marginal value: expected accuracy gain from extra tokens.
Accuracy-per-1k-tokens: correct answers per 1,000 tokens spent.
Calibrated confidence: probabilities matching observed correctness rates.


Cite-and-Challenge Peer Protocol for Factual Claims
Hypothesis: Peer challenges reduce hallucinations and increase citation precision.

MVP workflow
1. Select 300 claims needing sources across science, health, history, finance.
2. Inputs: claims, web search API, two answerers, one challenger.
3. Method: answerers cite spans; challenger flags unsupported spans; single revision allowed.
4. Output: final claims, citations, flagged changes, and explanation traces.
5. Baseline: single agent with citations; measure hallucination rate and citation precision/recall.
6. Deliverable: dataset, interaction logs, verification script, and result summaries.

Terms
Hallucination: confident statement without supporting evidence.
Precision/recall: correctness of cited sources, and coverage of needed sources.
Challenger: agent that finds unsupported or weakly supported parts.


Slot-Memory Board Coordination for Multi-Agent Tool Use
Hypothesis: Fixed slots reduce chatter, latency, and errors.

MVP workflow
1. Select 200 multi-step tool tasks with automatic pass/fail checks.
2. Inputs: tasks, three agents, six-slot shared memory board, simple tools.
3. Method: agents write to fixed slots; referee prunes conflicts using rules.
4. Output: solutions, slot histories, token counts, latency, and success rates.
5. Baseline: free-form chat memory; compare tokens, latency, and task success.
6. Deliverable: open-source board code, logs, and evaluation harness.

Terms
Slot-memory board: shared page with a few named fields anyone can edit.
Referee: simple process that resolves conflicts and enforces brief slot rules.